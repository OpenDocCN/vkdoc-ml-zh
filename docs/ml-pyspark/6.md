# 六、随机森林

这一章主要讲述用 PySpark 构建随机森林(RF)进行分类。我们将了解它们的各个方面以及预测是如何发生的；但在了解更多关于随机森林的知识之前，我们必须了解 RF 的构建模块，即决策树(DT)。决策树也用于分类/回归。但是就准确性而言，由于各种原因，随机森林胜过 DT 分类器，我们将在本章后面讨论这些原因。让我们了解更多关于决策树的知识。

## 决策图表

决策树属于机器学习的监督类别，使用频率表进行预测。决策树的一个优点是它可以处理分类变量和数值变量。顾名思义，它以一种树状结构运行，并根据各种分裂形成这些规则，最终做出预测。决策树中使用的算法是 J. R. Quinlan 开发的 ID3。

我们可以将决策树分解成不同的组件，如图 [6-1](#Fig1) 所示。

![img/469852_1_En_6_Chapter/469852_1_En_6_Fig1_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fig1_HTML.jpg)

图 6-1

决策图表

树从中分支的最顶端的分裂节点被称为根节点；在上面的例子中，年龄是根节点。圆圈中表示的值称为叶节点或预测。让我们用一个样本数据集来理解决策树实际上是如何工作的。

表 [6-1](#Tab1) 中显示的数据包含了一些不同年龄组和属性的人的样本数据。基于这些属性要做出的最终决定是保险费是否应该偏高。这是一个典型的分类案例，我们将使用决策树对其进行分类。我们有四个输入栏(年龄组、吸烟者、医疗状况、工资水平)。

表 6-1

示例数据集

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"></colgroup> 
| 

年龄层

 | 

吸烟者

 | 

医疗条件

 | 

薪资水平

 | 

保险费

 |
| --- | --- | --- | --- | --- |
| 老的 | 是 | 是 | 高的 | 高的 |
| 十几岁的青少年 | 是 | 是 | 中等 | 高的 |
| 年纪轻的 | 是 | 是 | 中等 | 低的 |
| 老的 | 不 | 是 | 高的 | 高的 |
| 年纪轻的 | 是 | 是 | 高的 | 低的 |
| 十几岁的青少年 | 不 | 是 | 低的 | 高的 |
| 十几岁的青少年 | 不 | 不 | 低的 | 低的 |
| 老的 | 不 | 不 | 低的 | 高的 |
| 十几岁的青少年 | 不 | 是 | 中等 | 高的 |
| 年纪轻的 | 不 | 是 | 低的 | 高的 |
| 年纪轻的 | 是 | 不 | 高的 | 低的 |
| 十几岁的青少年 | 是 | 不 | 中等 | 低的 |
| 年纪轻的 | 不 | 不 | 中等 | 高的 |
| 老的 | 是 | 不 | 中等 | 高的 |

### 熵

决策树生成这些数据的子集，每个子集包含相同的类值(同质)；为了计算同质性，我们使用熵。这也可以使用基尼指数和分类误差等其他指标来计算，但我们将使用熵来理解决策树是如何工作的。计算熵的公式是

-***p log***<sub>**2**</sub>***p***-***q log***<sub>**2**</sub>***q***

图 [6-2](#Fig2) 表明如果子集完全纯，熵等于零；这意味着它只属于一个类，如果子集被平均分成两个类，则它等于 1

![img/469852_1_En_6_Chapter/469852_1_En_6_Fig2_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fig2_HTML.jpg)

图 6-2

熵

如果我们想计算我们的目标变量(保险费)的熵，我们首先要计算每一类的概率，然后用上面的公式计算熵。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

保险费

 |
| --- |
| 高(9) | 低(5) |

高类别的概率等于 9/14 =0.64

低类别的概率等于 5/14 =0.36

熵=*p*(*高*)*log*<sub>2</sub>(*p*(*高*)*p*(*低*)*log*<sub>2</sub>(*p*(*低*))

=——0.64′t0【日志】<sub><sub>(0.64))——0.36′T4【日志】<sub>(0.36))</sub></sub></sub>

= 0.94

为了构建决策树，我们需要计算两种熵:

1.  目标熵(保险费)

2.  具有属性的目标的熵(例如。保险费-吸烟者)

我们已经看到了目标的熵，所以让我们计算具有输入特征的目标的第二个熵。例如，让我们考虑吸烟者的特征。

![$$ Entrop{y}_{\left( Target, Feature\right)}= Probabilit{y}_{Feature}\ast Entrop{y}_{Categories} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equa.png)

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-center"> <col class="tcol3 align-left"> <col class="tcol4 align-center"></colgroup> 
| 

熵计算

目标-保险费

特征-吸烟者

 | 

保险费(目标)

 |
| --- | --- |
| 

高(9)

 | 

低(5)

 |
| --- | --- |
| 吸烟者(特写) | 是(7) | three | four |
| 第七项 | six | one |

![$$ Entrop{y}_{\left( Target, Smoker\right)}={P}_{yes}\ast Entrop{y}_{\left(3,4\right)}+{P}_{no}\ast Entrop{y}_{\left(6,1\right)} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equb.png)

*P*<sub>T3】是 T5】=![$ \frac{7}{14} $](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_IEq1.png)= 0.5</sub>

*P*<sub>T3】noT5】=![$ \frac{7}{14} $](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_IEq2.png)= 0.5</sub>

![$$ Entrop{y}_{\left(3,4\right)}=-\frac{3}{7}\ast lo{g}_2\left(\frac{3}{7}\right)-\left(\frac{4}{7}\right)\ast lo{g}_2\left(\frac{4}{7}\right) $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equc.png)

= 0.99

![$$ Entrop{y}_{\left(6,1\right)}=-\frac{6}{7}\ast lo{g}_2\left(\frac{6}{7}\right)-\left(\frac{1}{7}\right)\ast lo{g}_2\left(\frac{1}{7}\right) $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equd.png)

= 0.59

![$$ Entrop{y}_{\left( Target, Smoker\right)}=0.55\ast 0.99+0.5\ast 0.59 $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Eque.png)

=0.79

类似地，我们计算所有其他属性的熵:

*熵* <sub>( *目标*，*年龄组* )</sub> = 0.69

*熵* <sub>( *目标*，*医疗状况* )</sub> = 0.89

*熵* <sub>( *目标*，*薪资水平* )</sub> = 0.91

### 信息增益

信息增益(IG)用于在决策树中进行分裂。提供最大信息增益的属性用于分割子集。信息增益告诉我们，就预测而言，哪一个特征是最重要的。从熵的角度来说，IG 是目标在分割前和分割后的熵的变化。

![$$ Information\ Gain= Entrop{y}_{(Target)}- Entrop{y}_{\left( Target, Feature\right)} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equf.png)

![$$ I{G}_{Smoker}= Entrop{y}_{(Target)}- Entrop{y}_{\left( Target, Smoker\right)} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equg.png)

= 0.94 − 0.79

= 0.15

![$$ I{G}_{Age\ Group}= Entrop{y}_{(Target)}- Entrop{y}_{\left( Target, Age\ Group\right)} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equh.png)

= 0.94 – 0.69

=0.25

![$$ I{G}_{Medical\ Condition}= Entrop{y}_{(Target)}- Entrop{y}_{\left( Target, Medical\ Condition\right)} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equi.png)

= 0.94 – 0.89

=0.05

![$$ I{G}_{Salary\ Level}= Entrop{y}_{(Target)}- Entrop{y}_{\left( Target, Salary\ Level\right)} $$](img/469852_1_En_6_Chapter/469852_1_En_6_Chapter_TeX_Equj.png)

= 0.94 – 0.91

=0.03

我们可以观察到，年龄组属性给出了最大的信息增益；因此，决策树的根节点应该是年龄组，第一次拆分发生在该属性上，如图 [6-3](#Fig3) 所示。

![img/469852_1_En_6_Chapter/469852_1_En_6_Fig3_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fig3_HTML.jpg)

图 6-3

决策树分裂

寻找提供最大信息增益的下一个属性的过程递归地继续，并且在决策树中进行进一步的分裂。最后，决策树可能看起来如图 [6-4](#Fig4) 所示。

![img/469852_1_En_6_Chapter/469852_1_En_6_Fig4_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fig4_HTML.jpg)

图 6-4

决策树拆分

决策树提供的优点是，它可以通过跟踪根节点到任何叶节点而容易地转换成一组规则，因此可以容易地用于分类。存在与决策树相关联的超参数集，这些超参数集提供了以不同方式构建树的更多选项。其中之一是最大深度，它允许我们决定决策树的深度；树越深，树的裂缝就越多，有可能过度拟合。

## 随机森林

现在我们知道了决策树是如何工作的，我们可以继续学习随机森林。顾名思义，随机森林是由很多树组成的:很多决策树。它们非常受欢迎，有时是监督机器学习的首选方法。随机森林也可以用于分类和回归。他们将来自许多单独决策树的投票组合起来，然后用多数票预测该类，或者在回归的情况下取平均值。这真的很有效，因为弱学习者最终会聚集在一起做出强预测。重要性在于这些决策树的形成方式。“随机”这个名称在 RF 中是有原因的，因为这些树是由一组随机的特征和一组随机的训练样本组成的。现在，每个决策树都使用一组略有不同的数据点进行训练，试图了解输入和输出之间的关系，最终与使用其他数据集进行训练的其他决策树的预测相结合，从而形成随机森林。如果我们举一个与上面相似的例子，创建一个有五棵决策树的随机森林，它可能看起来如图 [6-5](#Fig5) 所示。

![img/469852_1_En_6_Chapter/469852_1_En_6_Fig5_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fig5_HTML.jpg)

图 6-5

个体决策树

现在，这些决策树中的每一个都使用了数据子集和特征子集来进行训练。这也称为“打包”技术，即引导聚合。每棵树都对预测进行投票排序，投票数最多的类是随机森林分类器的最终预测，如图 [6-6](#Fig6) 所示。

![img/469852_1_En_6_Chapter/469852_1_En_6_Fig6_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fig6_HTML.jpg)

图 6-6

随机森林

随机森林提供的一些优势如下:

*   特征重要性:随机森林可以根据预测能力给出用于训练的每个特征的重要性。这提供了一个很好的机会来选择相关的特性，去掉较弱的特性。所有特征重要性的总和总是等于 1。

*   更高的准确性:由于它从单个决策树收集投票，随机森林的预测能力比单个决策树相对更高。

*   较少过拟合:单个分类器的结果被平均或最大化投票，因此减少了过拟合的机会。

随机森林的一个缺点是，与决策树相比，它很难可视化，并且在计算方面涉及更多，因为它构建了多个单独的分类器，

## 密码

本章的这一节重点介绍如何使用 PySpark 和 Jupyter Notebook 从头构建一个随机森林分类器。

### 注意

这本书的 GitHub repo 上提供了完整的数据集和代码，在 Spark 2.0 和更高版本上执行得最好。

让我们使用 Spark 的 MLlib 库构建一个随机森林模型，并使用输入特性预测目标变量。

### 数据信息

我们将在这个例子中使用的数据集是一个有几千行六列的开源数据集。我们必须使用五个输入变量来预测使用随机森林模型的目标变量。

### 步骤 1:创建 SparkSession 对象

我们启动 Jupyter 笔记本并导入 SparkSession，然后创建一个新的 SparkSession 对象来使用 Spark。

```py
[In]: from pyspark.sql import SparkSession
[In]: spark=SparkSession.builder.appName('random_forest').getOrCreate()

```

### 步骤 2:读取数据集

然后，我们使用 Dataframe 在 Spark 中加载和读取数据集。我们必须确保我们已经从数据集可用的同一个目录文件夹中打开了 PySpark，否则我们必须提到数据文件夹的目录路径。

```py
[In]: df=spark.read.csv('affairs.csv',inferSchema=True,header=True)

```

### 步骤 3:探索性数据分析

在本节中，我们将通过查看数据集并验证数据集的形状和变量的各种统计度量来更深入地研究数据集。我们从检查数据集的形状开始。

```py
[In]: print((df.count(), len(df.columns)))
[Out]: (6366, 6)

```

因此，上面的输出确认了数据集的大小，然后我们可以验证输入值的数据类型，以检查我们是否需要更改/转换任何列的数据类型。

```py
[In]: df.printSchema()

[Out]: root
 |-- rate_marriage: integer (nullable = true)
 |-- age: double (nullable = true)
 |-- yrs_married: double (nullable = true)
 |-- children: double (nullable = true)
 |-- religious: integer (nullable = true)
 |-- affairs: integer (nullable = true)

```

正如我们所看到的，没有需要转换成数字形式的分类列。让我们用 Spark 中的 show 函数来看看数据集:

```py
[In]: df.show(5)

[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figa_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figa_HTML.jpg)

我们现在可以使用 describe 函数来检查数据集的统计度量。

```py
[In]: df.describe().select('summary','rate_marriage','age','yrs_married','children','religious').show()
[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figb_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figb_HTML.jpg)

我们可以观察到，人的平均年龄接近 29 岁，结婚 9 年。

让我们研究各个列，以便更深入地了解数据的细节。与 counts 一起使用的 groupBy 函数返回数据中每个类别的出现频率。

```py
[In]: df.groupBy('affairs').count().show()
[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figc_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figc_HTML.jpg)

所以，在总人口中，有超过 33%的人有婚外情。

```py
[In]: df.groupBy('rate_marriage').count().show()
[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figd_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figd_HTML.jpg)

大多数人对他们的婚姻评价很高(4 或 5)，其余的人评价较低。让我们再深入一点，了解婚姻评级是否与婚外情变量有关。

```py
[In]: df.groupBy('rate_marriage','affairs').count().orderBy('rate_marriage','affairs','count',ascending=True).show()
[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Fige_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Fige_HTML.jpg)

显然，这些数据表明，对婚姻评价低的人有外遇的比例很高。这可能被证明是预测的一个有用特征。我们将以类似的方式探索其他变量。

```py
[In]: df.groupBy('religious','affairs').count().orderBy('religious','affairs','count',ascending=True).show()
[Out]: 

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figf_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figf_HTML.jpg)

从宗教角度的评分以及对宗教特征评分较低和对婚外情评分较高的人数来看，我们也有类似的情况。

```py
[In]: df.groupBy('children','affairs').count().orderBy('children','affairs','count',ascending=True).show()
[Out]: 

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figg_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figg_HTML.jpg)

上表没有清楚地表明子女数量和卷入婚外情机会之间关系的任何趋势。让我们使用 groupBy 函数和 mean 来了解数据集的更多信息。

```py
[In]: df.groupBy('affairs').mean().show()
[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figh_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figh_HTML.jpg)

所以，从年龄的角度来看，有婚外情的人对他们的婚姻评价很低，甚至有点偏高。他们结婚的时间也更长，也更不信教。

### 步骤 4:特征工程

这是我们使用 Spark 的 VectorAssembler 创建一个组合所有输入特征的单一向量的部分。

```py
[In]: from pyspark.ml.feature import VectorAssembler

```

我们需要将所有的输入列组合成一个向量，作为模型的输入特征。因此，我们选择需要用来创建单个特征向量的输入列，并将输出向量命名为 features。

```py
[In]: df_assembler = VectorAssembler(inputCols=['rate_marriage', 'age', 'yrs_married', 'children', 'religious'], outputCol="features")

[In}:df = df_assembler.transform(df)

[In]: df.printSchema()
[Out]:
root
 |-- rate_marriage: integer (nullable = true)
 |-- age: double (nullable = true)
 |-- yrs_married: double (nullable = true)
 |-- children: double (nullable = true)
 |-- religious: integer (nullable = true)
 |-- affairs: integer (nullable = true)
 |-- features: vector (nullable = true)

```

正如我们所看到的，现在我们有了一个名为 features 的额外列，它是所有输入要素的组合，表示为一个密集矢量。

```py
[In]: df.select(['features','affairs']).show(10,False)
[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figi_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figi_HTML.jpg)

让我们只选择 features 列作为输入，事务列作为输出来训练随机森林模型。

```py
[In]: model_df=df.select(['features','affairs'])

```

### 步骤 5:拆分数据集

为了训练和评估随机森林模型的性能，我们必须将数据集分为训练和测试数据集。我们将其分成 75/25 的比例，并在数据集的 75%上训练我们的模型。我们可以打印火车的形状和测试数据来验证大小。

```py
[In]: train_df,test_df=model_df.randomSplit([0.75,0.25])
[In]: print(train_df.count())
[Out]: 4775

[In]: train_df.groupBy('affairs').count().show()
[Out]:
+-------+-----+
|affairs|count|
+-------+-----+
|      1| 1560|
|      0| 3215|
+-------+-----+

```

这确保我们将目标类(“事务”)的集合值平衡到训练集和测试集中。

```py
[In]: test_df.groupBy('affairs').count().show()

[Out]:
+-------+-----+
|affairs|count|
+-------+-----+
|      1|  493|
|      0| 1098|
+-------+-----+

```

### 步骤 6:建立和训练随机森林模型

在这一部分中，我们使用输入和状态等特征作为输出列来构建和训练随机森林模型。

```py
[In]: from pyspark.ml.classification import RandomForestClassifier

[In]: rf_classifier=RandomForestClassifier(labelCol='affairs',numTrees=50).fit(train_df)

```

有许多超参数可以设置来调整模型的性能，但我们在这里选择的是默认参数，除了我们想要构建的决策树的数量。

### 步骤 7:测试数据的评估

一旦我们在训练数据集上训练了我们的模型，我们就可以在测试集上评估它的性能。

```py
[In]: rf_predictions=rf_classifier.transform(test_df)

[In]: rf_predictions.show()

[Out]:

```

![img/469852_1_En_6_Chapter/469852_1_En_6_Figj_HTML.jpg](img/469852_1_En_6_Chapter/469852_1_En_6_Figj_HTML.jpg)

预测表中的第一列是测试数据的输入要素。第二列是测试数据的实际标签或输出。第三列(rawPrediction)表示两种可能输出的置信度。第四列是每个类别标签的条件概率，最后一列是随机森林分类器的预测。我们可以对预测列应用一个 groupBy 函数，以找出对正类和负类的预测数量。

```py
[In]: rf_predictions.groupBy('prediction').count().show()
[Out]:
+----------+-----+
|prediction|count|
+----------+-----+
|       0.0| 1257|
|       1.0|  334|
+----------+-----+

```

为了评估这些预测，我们将导入分类评估器。

```py
[In]: from pyspark.ml.evaluation import MulticlassClassificationEvaluator
[In]: from pyspark.ml.evaluation import BinaryClassificationEvaluator

```

### 准确

```py
[In]: rf_accuracy=MulticlassClassificationEvaluator(labelCol='affairs',metricName='accuracy').evaluate(rf_predictions)
[In]: print('The accuracy of RF on test data is {0:.0%}'.format(rf_accuracy))
[Out]: The accuracy of RF on test data is 73%

```

### 精确

```py
[In]: rf_precision=MulticlassClassificationEvaluator(labelCol='affairs',metricName='weightedPrecision').evaluate(rf_predictions)
[In]: print('The precision rate on test data is {0:.0%}'.format(rf_precision))

[Out]: The precision rate on test data is 71%

```

### 罗马纪元

```py
[In]: rf_auc=BinaryClassificationEvaluator(labelCol='affairs').evaluate(rf_predictions)
[In]: print( rf_auc)
[Out]: 0.738

```

如前所述，RF 给出了每个特征在预测能力方面的重要性，这对于找出对预测贡献最大的关键变量非常有用。

```py
[In]: rf_classifier.featureImportances
[Out]: (5,[0,1,2,3,4],[0.563965247822,0.0367408623003,0.243756511958,0.0657893200779,0.0897480578415])

```

我们使用了五个特征，并且可以使用特征重要性函数来找出重要性。要了解哪个输入要素映射到哪个索引值，我们可以使用元数据信息。

```py
[In]: df.schema["features"].metadata["ml_attr"]["attrs"]
[Out]:
  {'idx': 0, 'name': 'rate_marriage'},
  {'idx': 1, 'name': 'age'},
  {'idx': 2, 'name': 'yrs_married'},
  {'idx': 3, 'name': 'children'},
  {'idx': 4, 'name': 'religious'}}

```

因此，从预测的角度来看，结婚率是最重要的特征，其次是结婚年龄。最不重要的变量似乎是年龄。

### 步骤 8:保存模型

有时，在训练完模型后，我们只需要调用模型进行预测，因此保留模型对象并重用它进行预测是非常有意义的。这包括两个部分。

1.  保存 ML 模型

2.  加载 ML 模型

```py
[In]: from pyspark.ml.classification import RandomForestClassificationModel
[In]: rf_classifier.save("/home/jovyan/work/RF_model")
      This way we saved the model as object locally.The next step is to load the model again for predictions
[In]: rf=RandomForestClassificationModel.load("/home/jovyan/work/RF_model")
 [In]: new_preditions=rf.transform(new_df)

```

新的预测表将包含具有模型预测的列

## 结论

在本章中，我们回顾了理解随机森林的构建块并在 PySpark 中创建 ML 模型以进行分类以及评估指标(如准确度、精密度和 auc)的过程。我们还讲述了如何在本地保存 ML 模型对象并在预测中重用它。