# 4.线性回归

正如我们在前一章中谈到的机器学习，这是一个非常广阔的领域，有多种算法属于不同的类别，但线性回归是最基本的机器学习算法之一。本章着重于用 PySpark 构建一个线性回归模型，并深入研究 LR 模型的工作原理。它将涵盖在使用 LR 以及不同评估指标之前需要考虑的各种假设。但是在试图理解线性回归之前，我们必须了解变量的类型。

## 变量

变量以不同的形式捕获数据信息。主要有两类广泛使用的变量，如图 [4-1](#Fig1) 所示。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig1_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig1_HTML.jpg)

图 4-1

变量的类型

我们甚至可以将这些变量进一步细分为子类别，但在本书中我们将坚持这两种类型。

数字变量是那些本质上是定量的值，比如数字(整数/浮点数)。例如，工资记录、考试分数、一个人的年龄或身高以及股票价格都属于数值变量的范畴。

另一方面，分类变量本质上是定性的，主要代表被测量数据的类别。例如，颜色、结果(是/否)、评级(好/差/平均)。

为了建立任何类型的机器学习模型，我们需要输入和输出变量。输入变量是用于建立和训练机器学习模型以预测输出或目标变量的那些值。我们举个简单的例子。假设我们想预测一个人的工资，给定这个人的年龄，使用机器学习。在这种情况下，工资是我们的输出/目标/因变量，因为它取决于年龄，这被称为输入或自变量。现在，输出变量本质上可以是分类的或数字的，并且根据其类型，选择机器学习模型。

现在回到线性回归，它主要用于我们试图预测数字输出变量的情况。线性回归用于预测符合输入数据的直线，指出可能的最佳方式，并有助于预测看不见的数据，但这里要注意的一点是，模型如何仅从“年龄”中学习并预测给定人员的工资金额？当然，这两个变量(工资和年龄)之间需要有某种关系。变量关系有两种主要类型:

*   线性的

*   非线性的

任何两个变量之间的线性关系的概念表明两者在某些方面是成比例的。任何两个变量之间的相关性给了我们一个指示，表明它们之间的线性关系有多强或多弱。相关系数的范围从-1 到+ 1。负相关意味着通过增加一个变量，另一个变量减少。例如，车辆的功率和里程可能是负相关的，因为当我们增加功率时，车辆的里程会下降。另一方面，工资和工作年限是正相关变量的一个例子。非线性关系本质上比较复杂，因此需要额外的细节来预测目标变量。比如自动驾驶汽车，地形、信号系统、行人等输入变量与汽车速度的关系是非线性的。

### 注意

下一节包括线性回归背后的理论，对许多读者来说可能是多余的。如果是这种情况，请随意跳过这一部分。

## 理论

既然我们已经了解了变量的基本知识和它们之间的关系，让我们以年龄和工资为例来深入理解线性回归。

线性回归的总体目标是通过数据预测一条直线，使得这些点中的每一个与该直线的垂直距离最小。因此，在这种情况下，我们将预测给定年龄的人的工资。假设我们有四个人的记录，记录了他们的年龄和各自的工资，如表 [4-1](#Tab1) 所示。

表 4-1

示例数据集

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

-你好。不，不

 | 

年龄

 | 

薪金(' 0000 美元)

 |
| --- | --- | --- |
| one | Twenty | five |
| Two | Thirty | Ten |
| three | Forty | Fifteen |
| four | Fifty | Twenty-two |

我们有一个可以利用的输入变量(年龄)来预测工资(我们将在本书的后面部分介绍)，但是让我们后退一步。让我们假设开始时我们所拥有的只是这四个人的工资值。在图 [4-2](#Fig2) 中绘制了每个人的工资。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig2_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig2_HTML.jpg)

图 4-2

工资散点图

现在，如果我们要根据前面几个人的工资来预测第五个人(新人)的工资，最好的预测方法是取现有工资值的平均值。根据这些信息，这将是最好的预测。这就像建立一个机器学习模型，但没有任何输入数据(因为我们使用输出数据作为输入数据)。

让我们继续计算这些给定工资值的平均工资。

平均值。薪水= ![$$ \frac{\left(5+10+15+22\right)}{4} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq1.png) = 13

所以，下一个人的工资值的最佳预测是 13。图 [4-3](#Fig3) 展示了每个人的工资值以及平均值(仅使用一个变量时的最佳拟合线)。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig3_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig3_HTML.jpg)

图 4-3

最佳拟合线图

图 [4-3](#Fig3) 所示的平均值线可能是这些数据点在这种情况下的最佳拟合线，因为除了工资本身，我们没有使用任何其他变量。如果我们仔细观察，没有一个早期的工资值位于这条最佳拟合线上；如图 [4-4](#Fig4) 所示，这似乎与平均工资值有一定的差距。这些也被称为错误。如果我们继续计算这个距离的总和并将它们相加，它变成 0，这是有意义的，因为它是所有数据点的平均值。因此，我们不是简单地将它们相加，而是将每个误差平方，然后将它们相加。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig4_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig4_HTML.jpg)

图 4-4

残差图

误差平方和= 64 + 9 + 4 + 81 = 158。

因此，将残差平方相加，得出的总值为 158，即误差平方和(SSE)。

### 注意

到目前为止，我们还没有使用任何输入变量来计算 SSE。

让我们暂时搁置这个分数，并加入输入变量(人的年龄)来预测这个人的工资。让我们从图 [4-5](#Fig5) 所示的人的年龄和工资之间的关系开始。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig5_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig5_HTML.jpg)

图 4-5

工资与年龄的相关图

正如我们所观察到的，工作经验年限和工资值之间似乎存在明显的正相关关系，这对我们来说是一件好事，因为它表明，由于投入(年龄)和产出(工资)之间存在很强的线性关系，该模型将能够以很高的准确度预测目标值(工资)。如前所述，线性回归的总体目标是得出一条符合数据点的直线，使实际目标值和预测值之间的平方差最小。因为它是一条直线，我们知道在线性代数中直线的方程是

y= mx + c，同样如图 [4-6](#Fig6) 所示。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig6_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig6_HTML.jpg)

图 4-6

直线图

在哪里，

m =直线的斜率(![$$ \frac{x_2-{x}_1}{y_2-{y}_1} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq2.png))

x =轴上的值

y =轴上的值

c =截距(x = 0 时 y 的值)

由于线性回归也是找出直线，线性回归方程变成

![$$ y={B}_{\mathbf{0}}+{B}_{\mathbf{1}}\ast x $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_Equa.png)

(因为我们仅使用 1 个输入变量，即年龄)

其中:

y=工资(预测)

B0 =截距(年龄为 0 时的工资值)

B1=工资的斜率或系数

x=年龄

现在，你可能会问，是否可以通过数据点绘制多条线(如图 [4-7](#Fig7) )以及如何计算出哪条线是最佳拟合线。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig7_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig7_HTML.jpg)

图 4-7

穿过数据的可能直线

找出最佳拟合线的第一个标准是它应该通过数据点的质心，如图 [4-8](#Fig8) 所示。在我们的例子中，质心值为

平均年龄= ![$$ \frac{\left(20+30+40+50\right)}{4} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq3.png) = 35 岁

平均值(工资)= ![$$ \frac{\left(5+10+15+22\right)}{4} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq4.png) = 13

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig8_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig8_HTML.jpg)

图 4-8

数据的质心

第二个标准是它应该能够最小化误差平方和。我们知道我们的回归线方程等于

![$$ y={B}_{\mathbf{0}}+{B}_{\mathbf{1}}\ast x $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_Equb.png)

现在，使用线性回归的目的是得出截距( *B* <sub>0</sub> )和系数( *B* <sub>1</sub> )的最佳值，使得残差/误差最小化到最大程度。

通过使用下面的公式，我们可以很容易地找出数据集的值*B*<sub>0</sub>&*B*<sub>1</sub>。

*B*T2】1=![$$ \frac{\sum \left({x}_i-{x}_{mean}\right)\ast \left({y}_i-{y}_{mean}\right)}{\sum {\left({x}_i-{x}_{mean}\right)}^2} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq5.png)

*b*<sub>【0】</sub>=*和* <sub>*意思是*</sub>-*【b】*

表 [4-2](#Tab2) 展示了使用输入数据计算线性回归的斜率和截距。

表 4-2

斜率和截距的计算

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"> <col class="tcol4 align-center"> <col class="tcol5 align-center"> <col class="tcol6 align-center"></colgroup> 
| 

年龄

 | 

薪水

 | 

年龄差异(不同于平均值)

 | 

薪资差异(差异。来自平均值)

 | 

协方差(乘积)

 | 

年龄差异(平方)

 |
| --- | --- | --- | --- | --- | --- |
| Twenty | five | -15 | -8 | One hundred and twenty | Two hundred and twenty-five |
| Thirty | Ten | -5 | -3 | Fifteen | Twenty-five |
| Forty | Fifteen | five | Two | Ten | Twenty-five |
| Fifty | Twenty-two | Fifteen | nine | One hundred and thirty-five | Two hundred and twenty-five |

平均年龄= 35 岁

平均工资=13

任何两个变量(年龄和工资)之间的协方差被定义为每个变量(年龄和工资)与其平均值之间的距离的乘积。简而言之，年龄和工资方差的乘积称为协方差。现在我们有了协方差积和年龄方差的平方值，我们可以继续计算线性回归线的斜率和截距值:

*B*T2】1=![$$ \frac{\sum (Covariance)}{\sum \left( Age\ Variance\ Squared\right)} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq6.png)

= ![$$ \frac{280}{500} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq7.png)

=0.56

*B*<sub>0</sub>= 13-(0.56 * 35)

= -6.6

我们最终的线性回归方程变成

![$$ y={B}_{\mathbf{0}}+{B}_{\mathbf{1}}\ast x $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_Equc.png)

**工资= -6.6 + (0.56 *年龄)**

我们现在可以用这个等式预测任何给定年龄的工资值。例如，该模型会预测第一个人的工资如下:

薪金(第一人)= -6.6 + (0.56*20)

= 4.6 ($ ‘0000)

## 解释

Slope ( *B* <sub>1</sub> = 0.56)这里的意思是，人的年龄每增加 1 岁，工资也会增加 5600 美元。

截距并不总是从其值中推导出意义。就像在这个例子中，负 6.6 的值表明如果这个人还没有出生(年龄=0)，那么这个人的工资将是负 66，000 美元。

图 [4-9](#Fig9) 显示了我们数据集的最终回归线。

![img/469852_1_En_4_Chapter/469852_1_En_4_Fig9_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fig9_HTML.jpg)

图 4-9

回归线

让我们使用回归方程预测数据中所有四条记录的工资，并比较与实际工资的差异，如表 [4-3](#Tab3) 所示。

表 4-3

预测值和实际值之间的差异

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

年龄

 | 

薪水

 | 

预计工资

 | 

差异/误差

 |
| --- | --- | --- | --- |
| Twenty | five | Four point six | -0.4 |
| Thirty | Ten | Ten point two | Zero point two |
| Forty | Fifteen | Fifteen point eight | Zero point eight |
| Fifty | Twenty-two | Twenty-one point four | -0.6 |

简而言之，线性回归得出截距( *B* <sub>0</sub> )和系数( *B* <sub>1</sub> ， *B* <sub>2</sub> )的最优值，使得预测值和目标变量之间的差异(误差)最小。

但问题仍然是:这是一个很好的适合吗？

## 估价

评价回归线的拟合优度有多种方法，但其中一种方法是利用决定系数( *r* <sup>*平方*</sup> )值。请记住，当我们仅使用输出变量本身并且其值为 158 时，我们已经计算了误差平方和。现在让我们重新计算这个模型的 SSE，它是我们使用一个输入变量构建的。表 [4-4](#Tab4) 显示了使用线性回归后新 SSE 的计算。

表 4-4

使用线性回归后上证指数的下降

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

年龄

 | 

薪水

 | 

预计工资

 | 

差异/误差

 | 

平方误差

 | 

旧上证所

 |
| --- | --- | --- | --- | --- | --- |
| Twenty | five | Four point six | -0.4 | Zero point one six | Sixty-four |
| Thirty | Ten | Ten point two | Zero point two | Zero point zero four | nine |
| Forty | Fifteen | Fifteen point eight | Zero point eight | Zero point six four | four |
| Fifty | Twenty-two | Twenty-one point four | -0.6 | Zero point three six | Eighty-one |

正如我们所观察到的，平方差的总和从 158 显著减少到只有 1.2，这是因为使用了线性回归。目标变量(工资)的变化可以借助回归来解释(由于使用了输入变量——年龄)。因此，OLS 致力于减少误差平方和。误差平方和是两种类型的组合:

TSS(总误差平方和)= SSE(误差平方和)+ SSR(剩余误差平方和)

总平方和是实际值和平均值之间的平方差之和，并且总是固定的。在我们的示例中，这等于 158。

SSE 是目标变量的实际值与预测值的平方差，在使用线性回归后，该值减少到 1.2。

SSR 是回归解释的平方和，可以通过(TSS–SSE)计算。

SSR = 158–1.2 = 156.8

*r* <sup>*平方*</sup> (决定系数)=![$$ \frac{SSR}{TSS} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq8.png)=![$$ \frac{156.8}{158} $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_IEq9.png)= 0.99

这一百分比表明，我们的线性回归模型能够以 99 %的准确度预测给定人员年龄的工资金额。另外 1%可以归因于模型无法解释的误差。我们的线性回归线非常适合模型，但它也可能是过度拟合的情况。当您的模型在训练数据上预测精度很高，但在未知/测试数据上性能下降时，就会发生过度拟合。解决过拟合问题的技术被称为正则化，并且有不同类型的正则化技术。就线性回归而言，可以使用脊、套索或弹性网正则化技术来处理过度拟合。

岭回归也称为 L2 正则化，其重点是将输入要素的系数值限制为接近于零，而拉索回归(L1)则使某些系数为零，以提高模型的概化能力。Elasticnet 是这两种技术的结合。

说到底，回归仍然是一种参数驱动的方法，并且假设关于输入数据点分布的基本模式很少。如果输入数据不符合这些假设，则线性回归模型表现不佳。因此，在使用线性回归模型之前，快速浏览这些假设以了解它们是很重要的。

假设:

*   输入变量和输出变量之间必须有线性关系。

*   独立变量(输入要素)不应相互关联(也称为多重共线性)。

*   残差/误差值之间必须没有相关性。

*   残差和输出变量之间必须有线性关系。

*   残差/误差值必须呈正态分布。

## 密码

本章的这一节着重于使用 PySpark 和 Jupyter Notebook 从头构建一个线性回归模型。

虽然我们看到了一个简单的例子，只有一个输入变量来理解线性回归，这是很少的情况。大多数情况下，数据集会包含多个变量，因此在这种情况下构建多变量回归模型更有意义。线性回归方程看起来像这样:

![$$ y={B}_{\mathbf{0}}+{B}_{\mathbf{1}}\ast {X}_{\mathbf{1}}+{B}_{\mathbf{2}}\ast {X}_{\mathbf{2}}+{B}_{\mathbf{3}}\ast {X}_{\mathbf{3}}+\dots $$](img/469852_1_En_4_Chapter/469852_1_En_4_Chapter_TeX_Equd.png)

### 注意

完整的数据集和代码可以在本书的 GitHub repo 上参考，在 Spark 2.3 和更高版本上执行得最好。

让我们使用 Spark 的 MLlib 库构建一个线性回归模型，并使用输入特性预测目标变量。

### 数据信息

我们将在本例中使用的数据集是一个虚拟数据集，总共包含 1，232 行和 6 列。我们必须使用 5 个输入变量，通过线性回归模型来预测目标变量。

### 步骤 1:创建 SparkSession 对象

我们启动 Jupyter 笔记本并导入 SparkSession，然后创建一个新的 SparkSession 对象来使用 Spark:

```py
[In]: from pyspark.sql import SparkSession
[In]: spark=SparkSession.builder.appName('lin_reg').getOrCreate()

```

### 步骤 2:读取数据集

然后，我们使用 Dataframe 在 Spark 中加载和读取数据集。我们必须确保我们已经从数据集可用的同一个目录文件夹中打开了 PySpark，否则我们必须提到数据文件夹的目录路径:

```py
[In]: df=spark.read.csv('Linear_regression_dataset.csv',inferSchema=True,header=True)

```

### 步骤 3:探索性数据分析

在本节中，我们将通过查看数据集、验证数据集的形状、各种统计测量以及输入和输出变量之间的相关性来更深入地研究数据集。我们从检查数据集的形状开始。

```py
[In]:print((df.count(), len(df.columns)))
[Out]: (1232, 6)

```

上面的输出确认了数据集的大小，我们可以验证输入值的数据类型，以检查我们是否需要更改/转换任何列的数据类型。在此示例中，所有列都包含整数值或双精度值。

```py
[In]: df.printSchema()
[Out]:

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Figa_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Figa_HTML.jpg)

总共有六列，其中五列是输入列(`var_1`到`var_5`)和目标列(输出)。我们现在可以使用`describe`函数来检查数据集的统计度量。

```py
[In]: df.describe().show(3,False)
[Out]:

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Figb_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Figb_HTML.jpg)

这使我们能够对数据集列的分布、中心测量和分布有所了解。然后，我们使用 head 函数查看数据集，并传递我们想要查看的行数。

```py
[In]: df.head(3)
[Out]:

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Figc_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Figc_HTML.jpg)

我们可以使用 corr 函数检查输入变量和输出变量之间的相关性:

```py
[In]: from pyspark.sql.functions import corr
[In]: df.select(corr('var_1','output')).show()
[Out] :

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Figd_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Figd_HTML.jpg)

`var_1`似乎与输出列的相关性最强。

### 步骤 4:特征工程

这是我们使用 Spark 的 VectorAssembler 创建一个组合所有输入特征的单一向量的部分。它仅创建一个要素来捕获该行的输入值。因此，它不是五个输入列，而是将所有输入列合并成一个特征向量列。

```py
[In]: from pyspark.ml.linalg import Vector
[In]: from pyspark.ml.feature import VectorAssembler

```

用户可以选择用作输入特征的列数，并且只能通过 VectorAssembler 传递这些列。在我们的例子中，我们将传递所有五个输入列来创建一个单独的特征向量列。

```py
[In]: df.columns
[Out]: ['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'output']

[In]: vec_assmebler=VectorAssembler(inputCols=['var_1', 'var_2', 'var_3', 'var_4', 'var_5'],outputCol='features')
[In]: features_df=vec_assmebler.transform(df)

[In]: features_df.printSchema()
[Out]:

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Fige_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Fige_HTML.jpg)

正如我们可以看到的，我们有一个额外的列(“features”)，其中包含所有输入的单个密集向量。

```py
[In]: features_df.select('features').show(5,False)
[Out]:

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Figf_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Figf_HTML.jpg)

我们获取数据帧的子集，并仅选择 features 列和 output 列来构建线性回归模型。

```py
[In]: model_df=features_df.select('features','output')

[In]: model_df.show(5,False)
[Out]:

```

![img/469852_1_En_4_Chapter/469852_1_En_4_Figg_HTML.jpg](img/469852_1_En_4_Chapter/469852_1_En_4_Figg_HTML.jpg)

```py
[In]: print((model_df.count(), len(model_df.columns)))
[Out]: (1232, 2) 

```

### 步骤 5:拆分数据集

我们必须将数据集分成训练和测试数据集，以便训练和评估所建立的线性回归模型的性能。我们将其分成 70/30 的比例，并在 70%的数据集上训练我们的模型。我们可以打印火车的形状和测试数据来验证尺寸。

```py
[In]: train_df,test_df=model_df.randomSplit([0.7,0.3])

[In]: print((train_df.count(), len(train_df.columns)))
[Out]: (882, 2)

[In]: print((test_df.count(), len(test_df.columns)))
[Out]: (350, 2)

```

### 步骤 6:建立和训练线性回归模型

在这一部分中，我们使用输入和输出列的功能来构建和定型线性回归模型。我们还可以获取模型的系数(B1，B2，B3，B4，B5)和截距(B0)值。我们还可以使用 r2 评估模型在训练数据上的性能。该模型在训练数据集上给出了非常好的准确度(86%)。

```py
[In]: from pyspark.ml.regression import LinearRegression

[In]: lin_Reg=LinearRegression(labelCol='output')

[In]: lr_model=lin_Reg.fit(train_df)

[In]: print(lr_model.coefficients)
[Out]: [0.000345569740987,6.07805293067e-05,0.000269273376209,-0.713663600176,0.432967466411]
[In]: print(lr_model.intercept)
[Out]: 0.20596014754214345

[In]: training_predictions=lr_model.evaluate(train_df)

[In]: print(training_predictions.r2)
[Out]: 0.8656062610679494

```

### 步骤 7:评估测试数据的线性回归模型

整个练习的最后一部分是检查模型在未知或测试数据上的性能。我们使用 evaluate 函数对测试数据进行预测，并可以使用 r2 来检查模型对测试数据的准确性。表现好像和训练差不多。

```py
[In]: test_predictions=lr_model.evaluate(test_df)

[In]: print(test_results.r2)
[Out]: 0.8716898064262081

[In]: print(test_results.meanSquaredError)
[Out]: 0.00014705472365990883

```

## 结论

在本章中，我们回顾了使用 PySpark 构建线性回归模型的过程，并解释了寻找最佳系数和截距值的过程。