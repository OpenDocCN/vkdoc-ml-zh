# 三、数据处理

本章试图涵盖使用 PySpark 处理和分析数据的所有主要步骤。尽管我们在本节中考虑的数据规模相对较小，但是使用 PySpark 处理大型数据集的步骤完全相同。数据处理是执行机器学习所需的关键步骤，因为我们需要清理、过滤、合并和转换我们的数据，使其成为所需的形式，以便我们能够训练机器学习模型。我们将利用多个 PySpark 函数来执行数据处理。

## 加载和读取数据

假设我们已经安装了 Spark 2.3 版，为了使用 Spark，我们首先从导入和创建`SparkSession`对象开始。

```py
[In]: from pyspark.sql import SparkSession
[In]: spark=SparkSession.builder.appName('data_processing').getOrCreate()
[In]: df=spark.read.csv('sample_data.csv',inferSchema=True,header=True)

```

我们需要确保数据文件位于我们打开 PySpark 的同一个文件夹中，或者我们可以指定数据所在文件夹的路径以及数据文件名。我们可以用 PySpark 读取多种数据文件格式。我们只需要根据文件格式(csv、JSON、parquet、table、text)更新读取格式参数。对于制表符分隔的文件，我们需要在读取文件时传递一个额外的参数来指定分隔符(`sep='\t'`)。将参数`inferSchema`设置为 true 表示 Spark 将在后台自己推断数据集中值的数据类型。

上面的命令使用示例数据文件中的值创建了一个 spark 数据帧。我们可以认为这是一个带有列和标题的表格格式的 Excel 电子表格。我们现在可以在这个 Spark 数据帧上执行多个操作。

```py
[In]: df.columns
[Out]: ['ratings', 'age', 'experience', 'family', 'mobile']

```

我们可以使用“columns”方法打印数据帧中的列名列表。如我们所见，我们的数据框架中有五列。为了验证列数，我们可以简单地使用 Python 的`length`函数。

```py
[In]: len(df.columns)
[Out]: 5

```

我们可以使用`count`方法来获得数据帧中的记录总数:

```py
[In]: df.count
[Out] : 33

```

我们的数据框架中共有 33 条记录。在进行预处理之前，最好打印出数据帧的形状，因为它给出了行和列的总数。Spark 中没有任何检查数据形状的直接函数；相反，我们需要结合列的数量和长度来打印形状。

```py
[In]: print((df.count),(len(df.columns))
[Out]: ( 33,5)

```

查看数据框中列的另一种方法是 spark 的`printSchema`方法。它显示了列的数据类型以及列名。

```py
[In]:df.printSchema()
[Out]: root
 |-- ratings: integer (nullable = true)
 |-- age: integer (nullable = true)
 |-- experience: double (nullable = true)
 |-- family: double (nullable = true)
 |-- Mobile: string (nullable = true)

```

`nullable`属性指示对应的列是否可以采用空值(true)或不采用空值(false)。我们还可以根据需要改变列的数据类型。

下一步是先睹为快，查看数据帧的内容。我们可以使用 Spark `show`方法来查看数据帧的顶行。

```py
[In]: df.show(3)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figa_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figa_HTML.jpg)

自从我们在`show`方法中传递了值 5 之后，我们只能看到五条记录和所有的五列。为了只查看某些列，我们必须使用`select`方法。让我们只查看两列(年龄和手机):

```py
[In]: df.select('age','mobile').show(5)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figb_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figb_HTML.jpg)

`Select`函数仅从数据帧中返回两列和五条记录。在本章中，我们将继续使用`select`函数。下一个要使用的函数是用于分析数据帧的`describe`。它返回数据帧中每一列的统计度量。我们将再次使用 show 和 describe，因为 describe 将结果作为另一个数据帧返回。

```py
[In]: df.describe().show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figc_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figc_HTML.jpg)

对于数字列，它返回中心的度量值，并随计数一起传播。对于非数字列，它显示计数以及最小值和最大值，这些值基于这些字段的字母顺序，并不表示任何实际意义。

## 添加新列

我们可以使用 spark 的`withColumn`函数在 dataframe 中添加一个新列。让我们通过使用`age`列向我们的数据框架添加一个新列(10 年后的年龄)。我们简单地给`age`列中的每个值加上 10 年。

```py
[In]: df.withColumn("age_after_10_yrs",(df["age"]+10)).show(10,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figd_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figd_HTML.jpg)

正如我们所观察到的，我们在数据帧中有了一个新列。`show`函数帮助我们查看新的列值，但是为了将新的列添加到数据帧中，我们需要将其分配给一个现有的或新的数据帧。

```py
[In]: df= df.withColumn("age_after_10_yrs",(df["age"]+10))

```

这一行代码确保更改发生，并且 dataframe 现在包含新列(10 年后的年龄)。

要将`age`列的数据类型从 integer 改为 double，我们可以使用 Spark 中的`cast`方法。我们需要从`pyspark.types:`导入`DoubleType`

```py
[In]: from pyspark.sql.types import StringType,DoubleType
[In]: df.withColumn('age_double',df['age'].cast(DoubleType())).show(10,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Fige_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Fige_HTML.jpg)

因此，上面的命令创建了一个新列(`age_double`)，它将年龄值从整数转换为双精度类型。

## 过滤数据

根据条件筛选记录是处理数据时的常见要求。这有助于清理数据并仅保留相关记录。PySpark 中的过滤非常简单，可以使用`filter`函数来完成。

### 条件 1

这是仅基于数据帧的一列的最基本的过滤类型。假设我们只想获取“Vivo”手机的记录:

```py
[In]: df.filter(df['mobile']=='Vivo').show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figf_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figf_HTML.jpg)

我们有所有`Mobile`列有‘Vivo’值的记录。在筛选记录后，我们可以进一步只选择几列。例如，如果我们想查看使用“Vivo”手机的人的年龄和评级，我们可以在过滤记录后使用`select`功能来完成。

```py
[In]: df.filter(df['mobile']=='Vivo').select('age','ratings','mobile').show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figg_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figg_HTML.jpg)

### 条件 2

这涉及基于多列的筛选，并且仅当满足所有条件时才返回记录。这可以通过多种方式实现。比方说，我们只想过滤“Vivo”用户和那些拥有 10 年以上经验的用户。

```py
[In]: df.filter(df['mobile']=='Vivo').filter(df['experience'] >10).show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figh_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figh_HTML.jpg)

为了将这些条件应用于各个列，我们使用了多个筛选函数。还有另一种方法可以达到同样的效果，如下所述。

```py
[In]: df.filter((df['mobile']=='Vivo')&(df['experience'] >10)).show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figi_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figi_HTML.jpg)

## 列中的不同值

如果我们想要查看任何 dataframe 列的不同值，我们可以使用`distinct`方法。让我们查看数据帧中 m `obile`列的不同值。

```py
[In]: df.select('mobile').distinct().show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figj_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figj_HTML.jpg)

为了获得列中不同值的计数，我们可以简单地使用`count`和`distinct`函数。

```py
[In]: df.select('mobile').distinct().count()
[Out]: 5

```

## 分组数据

`Grouping` `is a`非常有用的理解数据集各个方面的方法。它有助于根据列值对数据进行分组，并提取洞察力。它还可以与其他多种功能一起使用。让我们看一个使用数据帧的`groupBy`方法的例子。

```py
[In]: df.groupBy('mobile').count().show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figk_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figk_HTML.jpg)

这里，我们根据 m `obile`列中的分类值对所有记录进行分组，并使用`count`方法计算每个类别的记录数。我们可以通过使用`orderBy`方法按照定义的顺序对这些结果进行排序，从而进一步细化这些结果。

```py
[In]: df.groupBy('mobile').count().orderBy('count',ascending=False).show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figl_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figl_HTML.jpg)

现在，`mobiles`的计数根据每个类别按降序排序。

我们还可以应用`groupBy`方法来计算统计度量，例如每个类别的平均值、总和、最小值或最大值。让我们看看其余列的平均值是多少。

```py
[In]: df.groupBy('mobile').mean().show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figm_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figm_HTML.jpg)

`mean`方法给出了每个手机品牌的平均年龄、评级、体验和家庭规模栏。我们也可以通过使用`sum`方法和`groupBy`来获得每个移动品牌的总和。

```py
[In]: df.groupBy('mobile').sum().show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Fign_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Fign_HTML.jpg)

现在让我们来看看每个手机品牌的用户数据的最小值和最大值。

```py
[In]: df.groupBy('mobile').max().show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figo_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figo_HTML.jpg)

```py
[In]:df.groupBy('mobile').min().show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figp_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figp_HTML.jpg)

## 聚集

我们也可以使用`agg`函数来获得与上面类似的结果。让我们使用 PySpark 中的`agg`函数来简单地计算每个手机品牌的总体验。

```py
[In]: df.groupBy('mobile').agg({'experience':'sum'}).show(5,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figq_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figq_HTML.jpg)

因此，这里我们只需使用`agg`函数，并传递我们希望进行聚合的列名(experience)。

## 用户定义函数(UDF)

UDF 广泛用于数据处理中，对数据帧进行某些变换。PySpark 有两种 UDF:传统的 UDF 和熊猫 UDF。熊猫 UDF 在速度和处理时间方面更强大。我们将看到如何在 PySpark 中使用这两种类型的 UDF。首先，我们必须从 PySpark 函数中导入`udf`。

```py
[In]: from pyspark.sql.functions import udf

```

现在，我们可以通过使用 lambda 或典型的 Python 函数来应用基本的 UDF。

### 传统 Python 函数

我们创建了一个简单的 Python 函数，它根据移动品牌返回价格范围的类别:

```py
[In]:
def price_range(brand):
    if brand in ['Samsung','Apple']:
        return 'High Price'
    elif brand =='MI':
        return 'Mid Price'
    else:
        return 'Low Price'

```

在下一步中，我们创建一个 UDF ( `brand_udf`)，它使用这个函数并捕获它的数据类型，以便将这个转换应用到 dataframe 的移动列上。

```py
[In]: brand_udf=udf(price_range,StringType())

```

在最后一步，我们将`udf(brand_udf)`应用到 dataframe 的 m `obile`列，并创建一个具有新值的新列(`price_range`)。

```py
[In]: df.withColumn('price_range',brand_udf(df['mobile'])).show(10,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figr_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figr_HTML.jpg)

### 使用 Lambda 函数

不用定义传统的 Python 函数，我们可以利用 lambda 函数，用一行代码创建一个 UDF，如下所示。我们根据用户的年龄将年龄列分为两组(`young`、`senior`)。

```py
[In]: age_udf = udf(lambda age: "young" if age <= 30 else "senior", StringType())
[In]: df.withColumn("age_group", age_udf(df.age)).show(10,False) 

[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figs_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figs_HTML.jpg)

### 熊猫 UDF(矢量化 UDF)

如前所述，熊猫 UDF 比它们的同类更快更高效。有两种类型的熊猫 UDF:

*   数量

*   分组地图

使用熊猫 UDF 非常类似于使用基本的 UDF。我们必须首先从 PySpark 函数导入`pandas_udf`,并将其应用于任何要转换的特定列。

```py
[In]: from pyspark.sql.functions import pandas_udf

```

在本例中，我们定义了一个 Python 函数，用于计算假设预期寿命为 100 岁的用户的剩余寿命。这是一个非常简单的计算:我们使用 Python 函数从 100 中减去用户的年龄。

```py
[In]:
def remaining_yrs(age):
    yrs_left=(100-age)
   return yrs_left

[In]: length_udf = pandas_udf(remaining_yrs, IntegerType())

```

一旦我们使用 Python 函数(remaining_yrs)创建了熊猫 UDF (length `_udf`)，我们就可以将其应用到`age`列并创建一个新列 yrs_left。

```py
[In]:df.withColumn("yrs_left", length_udf(df['age'])).show(10,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figt_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figt_HTML.jpg)

### 熊猫 UDF(多列)

我们可能会遇到这样的情况，我们需要多个列作为输入来创建一个新列。因此，下面的例子展示了在数据帧的多列上应用熊猫 UDF 的方法。在这里，我们将创建一个新列，它只是 ratings 和 experience 列的乘积。像往常一样，我们定义一个 Python 函数，并计算两列的乘积。

```py
[In]:
def prod(rating,exp):
    x=rating*exp
    return x
[In]: prod_udf = pandas_udf(prod, DoubleType())

```

创建熊猫 UDF 后，我们可以将它应用于两个列(`ratings`、`experience`)以形成新列(`product`)。

```py
[In]:df.withColumn("product",prod_udf(df['ratings'],df['experience'])).show(10,False)
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figu_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figu_HTML.jpg)

## 删除重复值

我们可以使用`dropDuplicates`方法从数据帧中删除重复的记录。该数据帧中的记录总数为 33，但它还包含 7 个重复记录，这可以通过删除这些重复记录来轻松确认，因为我们只剩下 26 行。

```py
[In]: df.count()
[Out]: 33
[In]: df=df.dropDuplicates()
[In]: df.count()
[Out]: 26

```

## 删除列

我们可以利用`drop`函数从数据帧中删除任何列。如果我们想从 dataframe 中删除 m `obile`列，我们可以将它作为一个参数传递给`drop`函数。

```py
[In]: df_new=df.drop('mobile')
[In]: df_new.show()
[Out]:

```

![img/469852_1_En_3_Chapter/469852_1_En_3_Figv_HTML.jpg](img/469852_1_En_3_Chapter/469852_1_En_3_Figv_HTML.jpg)

## 写入数据

一旦我们完成了处理步骤，我们就可以以所需的格式将干净的数据帧写入所需的位置(本地/云)。

### 战斗支援车

如果我们想把它保存回原来的 csv 格式作为单个文件，我们可以使用 spark 中的`coalesce`函数。

```py
[In]: pwd
[Out]: ' /home/jovyan/work '
[In]: write_uri= '  /home/jovyan/work/df_csv '
[In]: df.coalesce(1).write.format("csv").option("header","true").save(write_uri)

```

### 镶木地板

如果数据集很大，并且包含很多列，我们可以选择压缩它，并将其转换为 parquet 文件格式。它减少了数据的总体大小，并在处理数据时优化了性能，因为它处理所需列的子集，而不是整个数据。我们可以很容易地将数据帧转换并保存为拼花格式，只需将格式命名为如下所示的`parquet`。

```py
[In]: parquet_uri='/home/jovyan/work/df_parquet'
[In]: df.write.format('parquet').save(parquet_uri)

```

### 注意

完整的数据集和代码可以在本书的 GitHub repo 上参考，在 Spark 2.3 和更高版本上执行得最好。

## 结论

在本章中，我们熟悉了一些使用 PySpark 处理和转换数据的函数和技术。使用 PySpark 对数据进行预处理的方法还有很多，但是本章已经介绍了为机器学习清理和准备数据的基本步骤。