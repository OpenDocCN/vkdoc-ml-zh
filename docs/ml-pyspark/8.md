# 8.使聚集

到目前为止，在前面的章节中，我们已经看到了有监督的机器学习，其中目标变量或标签是我们已知的，我们试图根据输入特征来预测输出。无监督学习在某种意义上是不同的，因为没有标记的数据，我们不试图预测任何输出；相反，我们试图找到有趣的模式，并在数据中找出组。相似的值被分组在一起。

当我们加入一所新的学校或大学，我们会遇到许多新面孔，每个人看起来都不一样。我们几乎不认识研究所里的任何人，最初也没有成立小组。慢慢地，我们开始花时间和其他人在一起，团体开始发展。我们与许多不同的人打交道，并弄清楚他们与我们有多相似和不相似。几个月后，我们几乎在自己的朋友群中安顿下来。群体中的朋友/成员具有相似的属性/爱好/品味，因此会呆在一起。聚类有点类似于这种基于定义组的属性集来形成组的方法。

## 从集群开始

我们可以对任何类型的数据进行聚类，形成相似的观察结果组，并使用它来做出更好的决策。在早期，客户细分通常是通过基于规则的方法来完成的，这需要大量的人工工作，并且只能使用有限数量的变量。例如，如果企业想要进行客户细分，他们会考虑多达 10 个变量，如年龄、性别、工资、地点等。，并创建仍能提供合理性能的基于规则的细分市场；但在今天的情况下，这将变得非常无效。一个原因是数据可用性丰富，另一个原因是动态的客户行为。有成千上万的其他变量可以被认为是产生这些机器学习驱动的片段，这些片段更加丰富和有意义。

当我们开始聚类时，每个观察都是不同的，不属于任何组，而是基于每个观察的属性有多相似。我们以这样的方式对它们进行分组，即每组包含最相似的记录，并且任意两组之间存在尽可能多的差异。那么，我们如何衡量两个观察值是相似还是不同呢？

有多种方法可以计算任意两个观测值之间的距离。首先，我们认为任何观测值都是一种向量形式，包含如下所示的观测值(A)。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

年龄

 | 

工资(万美元)

 | 

重量(千克)

 | 

高度(英尺。)

 |
| --- | --- | --- | --- |
| Thirty-two | eight | Sixty-five | six |

现在，假设我们想要计算这个观察/记录与任何其他观察(B)的距离，其他观察(B)也包含类似的属性，如下所示。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

年龄

 | 

工资(万美元)

 | 

重量(千克)

 | 

高度(英尺。)

 |
| --- | --- | --- | --- |
| Forty | Fifteen | Ninety | five |

我们可以用欧几里得方法来测量距离，这很简单。

它也被称为笛卡尔距离。我们试图计算任意两点间直线的距离；如果这些点之间的距离很小，它们更可能是相似的，而如果距离很大，它们彼此不相似，如图 [8-1](#Fig1) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig1_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig1_HTML.jpg)

图 8-1

基于欧氏距离的相似性

任意两点之间的欧几里德距离可以使用下面的公式计算:

![$$ Dis{t}_{\left(A,B\right)}=\sqrt{{\left(A1-B1\right)}^2+{\left(A2-B2\right)}^2+{\left(A3-B3\right)}^2+{\left(A4-B4\right)}^2} $$](../images/469852_1_En_8_Chapter/469852_1_En_8_Chapter_TeX_Equa.png)

![$$ Dis{t}_{\left(A,B\right)}=\sqrt{{\left( Age\kern0.5em diff\right)}^2+{\left( Salary\ diff\right)}^2+{\left( Weight\ diff\right)}^2+{\left( Height\ diff\right)}^2} $$](../images/469852_1_En_8_Chapter/469852_1_En_8_Chapter_TeX_Equb.png)

![$$ Dis{t}_{\left(A,B\right)}=\sqrt{{\left(32-40\right)}^2+{\left(8-15\right)}^2+{\left(65-90\right)}^2+{\left(6-5\right)}^2} $$](../images/469852_1_En_8_Chapter/469852_1_En_8_Chapter_TeX_Equc.png)

![$$ Dis{t}_{\left(A,B\right)}=\sqrt{\left(64+49+625+1\right)} $$](../images/469852_1_En_8_Chapter/469852_1_En_8_Chapter_TeX_Equd.png)

*Dist* <sub>( *A* ， *B* )</sub> = 27.18

因此，观察值 A 和 B 之间的欧几里德距离是 27.18。计算观测值之间距离的其他技术如下:

1.  曼哈顿距离

2.  马哈拉诺比斯距离

3.  闵可夫斯基距离

4.  切比雪夫距离

5.  余弦距离

聚类的目标是具有最小的簇内距离和最大的簇间差异。基于我们用来进行聚类的距离方法，我们可能会得到不同的组，因此，确保选择与业务问题相一致的正确距离度量是至关重要的。在研究不同的集群技术之前，让我们快速回顾一下集群的一些应用。

## 应用程序

如今，聚类被用于从客户细分到异常检测的各种用例中。企业广泛使用机器学习驱动的聚类来分析客户和细分，以围绕这些结果创建市场战略。聚类通过在一个聚类中找到相似的对象和彼此远离的不相似的对象来驱动大量搜索引擎的结果。它基于搜索查询推荐最接近的相似结果

基于数据类型和业务需求，可以通过多种方式进行集群。最常用的是 K-means 和层次聚类。

### k 均值

k’代表我们想要在给定数据集中形成的聚类或组的数量。这种类型的聚类包括预先决定聚类的数量。在研究 K-means 聚类如何工作之前，让我们先熟悉几个术语。

1.  图心

2.  变化

质心是指在一个簇或组的中心的中心数据点。它也是聚类中最具代表性的点，因为它是与聚类中其他点距离最远的数据点。图 [8-2](#Fig2) 显示了三个随机集群的质心(用十字表示)。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig2_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig2_HTML.jpg)

图 8-2

星团的质心

每个聚类或组包含不同数量的最接近聚类质心的数据点。一旦单个数据点改变聚类，聚类的质心值也会改变。改变组内的中心位置，产生新的质心，如图 [8-3](#Fig3) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig3_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig3_HTML.jpg)

图 8-3

新群的新质心

聚类的整体思想是最小化类内距离，即数据点与类的质心的内部距离，并最大化类间距离，即两个不同类的质心之间的距离。

方差是该聚类内质心和数据点之间的聚类内距离的总和，如图 [8-4](#Fig4) 所示。方差随着聚类数的增加而不断减小。聚类越多，每个聚类中的数据点数量就越少，因此可变性就越小。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig4_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig4_HTML.jpg)

图 8-4

在调整距离中

K-means 聚类总共由四个步骤组成，以形成数据集中的内部组。我们将考虑一个样本数据集来理解 K 均值聚类算法是如何工作的。数据集包含一些用户，他们的年龄和体重值如表 [8-1](#Tab1) 所示。现在我们将使用 K-means 聚类来得出有意义的聚类并理解算法。

表 8-1

K 均值的样本数据集

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 |
| --- | --- | --- |
| one | Eighteen | Eighty |
| Two | Forty | Sixty |
| three | Thirty-five | One hundred |
| four | Twenty | Forty-five |
| five | Forty-five | One hundred and twenty |
| six | Thirty-two | Sixty-five |
| seven | Seventeen | Fifty |
| eight | Fifty-five | Fifty-five |
| nine | Sixty | Ninety |
| Ten | Ninety | Fifty |

如果我们在二维空间中绘制这些用户，我们可以看到最初没有点属于任何组，我们的目的是在这组用户中找到聚类(我们可以尝试两个或三个)，使得每个组包含相似的用户。每个用户由年龄和体重表示，如图 [8-5](#Fig5) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig5_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig5_HTML.jpg)

图 8-5

聚类前的用户

#### 第一步:决定 K

它从决定簇的数量(K)开始。大多数情况下，我们在开始时不能确定正确的组数，但是我们可以使用一种基于可变性的称为肘方法的方法来找到最佳的组数。对于这个例子，为了简单起见，让我们从 K=2 开始。因此，我们在这个样本数据中寻找两个集群。

#### 步骤 2:随机初始化质心

下一步是随机将任意两个点视为新聚类的质心。这些可以随机选择，所以我们选择用户号 5 和用户号 10 作为新群集中的两个质心，如表 [8-2](#Tab2) 所示。

表 8-2

K 均值的样本数据集

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 |
| --- | --- | --- |
| one | Eighteen | Eighty |
| Two | Forty | Sixty |
| three | Thirty-five | One hundred |
| four | Twenty | Forty-five |
| 5(质心 1) | Forty-five | One hundred and twenty |
| six | Thirty-two | Sixty-five |
| seven | Seventeen | Fifty |
| eight | Fifty-five | Fifty-five |
| nine | Sixty | Ninety |
| 10(质心 2) | Ninety | Fifty |

质心可以用重量和年龄值来表示，如图 [8-6](#Fig6) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig6_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig6_HTML.jpg)

图 8-6

两个簇的随机质心

#### 步骤 3:为每个值分配分类号

在这一步，我们计算每个点到质心的距离。在这个例子中，我们计算每个用户到两个质心点的欧几里德平方距离。根据距离值，我们继续决定用户属于哪个特定的集群(1 或 2)。无论用户靠近哪个质心(距离更小)，都将成为该聚类的一部分。为每个用户计算欧几里得平方距离，如表 [8-3](#Tab3) 所示。用户 5 和用户 10 到各自质心的距离为零，因为它们是与质心相同的点。

表 8-3

基于距质心距离的聚类分配

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 | 

质心 1 的 ED*

 | 

质心 2 的 ED*

 | 

串

 |
| --- | --- | --- | --- | --- | --- |
| one | Eighteen | Eighty | Forty-eight | seventy-eight | one |
| Two | Forty | Sixty | Sixty | Fifty-one | Two |
| three | Thirty-five | One hundred | Twenty-two | Seventy-four | one |
| four | Twenty | Forty-five | Seventy-nine | Seventy | Two |
| five | Forty-five | One hundred and twenty | Zero | Eighty-three | one |
| six | Thirty-two | Sixty-five | Fifty-seven | Sixty | one |
| seven | Seventeen | Fifty | Seventy-five | Seventy-three | Two |
| eight | Fifty-five | Fifty-five | Sixty-six | Thirty-five | Two |
| nine | Sixty | Ninety | Thirty-four | Fifty | one |
| Ten | Ninety | Fifty | Eighty-three | Zero | Two |
| (*欧几里德距离) |

因此，根据距质心的距离，我们将每个用户分配到簇 1 或簇 2。集群 1 包含五个用户，集群 2 也包含五个用户。初始集群如图 [8-7](#Fig7) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig7_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig7_HTML.jpg)

图 8-7

初始聚类和质心

如前所述，在聚类中包含或排除新的数据点之后，聚类的质心必然会改变。由于早期的质心(C1，C2)不再位于簇的中心，我们在下一步计算新的质心。

#### 步骤 4:计算新的质心和重新分配集群

K-means 聚类的最后一步是计算聚类的新质心，并根据与新质心的距离将聚类重新分配给每个值。让我们计算群集 1 和群集 2 的新质心。为了计算聚类 1 的质心，我们只需取那些属于聚类 1 的值的年龄和体重的平均值，如表 [8-4](#Tab4) 所示。

表 8-4

聚类 1 的新质心计算

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 |
| --- | --- | --- |
| one | Eighteen | Eighty |
| three | Thirty-five | One hundred |
| five | Forty-five | One hundred and twenty |
| six | Thirty-two | Sixty-five |
| nine | Sixty | Ninety |
| **平均值** | **38** | **91** |

群集 2 的质心计算也以类似的方式完成，如表 [8-5](#Tab5) 所示。

表 8-5

群集 2 的新质心计算

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 |
| --- | --- | --- |
| Two | Forty | Sixty |
| four | Twenty | Forty-five |
| seven | Seventeen | Fifty |
| eight | Fifty-five | Fifty-five |
| Ten | Ninety | Fifty |
| **平均值** | **44.4** | **52** |

现在我们有了每个聚类的新质心值，用十字表示，如图 [8-8](#Fig8) 所示。箭头表示质心在群集内的移动。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig8_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig8_HTML.jpg)

图 8-8

两个星团的新质心

对于每个簇的质心，我们重复步骤 3，计算每个用户与新质心的欧几里德平方距离，并找出最近的质心。然后，我们根据离质心的距离将用户重新分配到集群 1 或集群 2。在这种情况下，只有一个值(用户 6)将其集群从 1 更改为 2，如表 [8-6](#Tab6) 所示。

表 8-6

集群的重新分配

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 | 

质心 1 的 ED*

 | 

质心 2 的 ED*

 | 

串

 |
| --- | --- | --- | --- | --- | --- |
| one | Eighteen | Eighty | Twenty-three | Thirty-eight | one |
| Two | Forty | Sixty | Thirty-one | nine | Two |
| three | Thirty-five | One hundred | nine | forty-nine | one |
| four | Twenty | Forty-five | forty-nine | Twenty-five | Two |
| five | Forty-five | One hundred and twenty | Thirty | sixty-eight | one |
| six | Thirty-two | Sixty-five | Twenty-seven | Eighteen | Two |
| seven | Seventeen | Fifty | Forty-six | Twenty-seven | Two |
| eight | Fifty-five | Fifty-five | Forty | Eleven | Two |
| nine | Sixty | Ninety | Twenty-two | Forty-one | one |
| Ten | Ninety | Fifty | Sixty-six | Forty-six | Two |

现在，根据到每个集群质心的距离，集群 1 只剩下四个用户，集群 2 包含六个用户，如图 [8-9](#Fig9) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig9_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig9_HTML.jpg)

图 8-9

集群的重新分配

我们不断重复上述步骤，直到集群分配不再有变化。新星团的质心如表 [8-7](#Tab7) 所示。

表 8-7

质心的计算

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 |
| --- | --- | --- |
| one | Eighteen | Eighty |
| three | Thirty-five | One hundred |
| five | Forty-five | One hundred and twenty |
| nine | Sixty | Ninety |
| 平均值 | Thirty-nine point five | Ninety-seven point five |

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

用户标识

 | 

年龄

 | 

重量

 |
| --- | --- | --- |
| Two | Forty | Sixty |
| four | Twenty | Forty-five |
| six | Thirty-two | Sixty-five |
| seven | Seventeen | Fifty |
| eight | Fifty-five | Fifty-five |
| Ten | Ninety | Fifty |
| 平均值 | Forty-two point three three | Fifty-four point one seven |

当我们执行这些步骤时，质心的移动变得越来越小，并且这些值几乎成为特定聚类的一部分，如图 [8-10](#Fig10) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig10_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig10_HTML.jpg)

图 8-10

集群的重新分配

正如我们所观察到的，即使在质心发生变化之后，点也不再发生变化，这就完成了 K-means 聚类。结果可能会有所不同，因为它是基于第一组随机质心。为了重现结果，我们也可以自己设置起点。具有值的最终聚类如图 [8-11](#Fig11) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig11_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig11_HTML.jpg)

图 8-11

最终聚类

聚类 1 包含在身高属性上处于平均水平但在体重变量上似乎非常高的用户，而聚类 2 似乎将那些高于平均水平但非常在意自己体重的用户分组在一起，如图 [8-12](#Fig12) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig12_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig12_HTML.jpg)

图 8-12

最终聚类的属性

#### 决定聚类数(K)

大多数时候，选择最佳数量的集群是相当棘手的，因为我们需要对数据集和业务问题的背景有深刻的理解。此外，当谈到无监督学习时，没有正确或错误的答案。与另一种方法相比，一种方法可能会产生不同数量的簇。我们必须试着找出哪种方法效果最好，以及创建的集群是否与决策足够相关。每个聚类可以用几个重要的属性来表示，这些属性表示或给出关于该特定聚类的信息。但是，有一种方法可以选择数据集的最佳聚类数。这种方法被称为肘法。

肘方法有助于我们测量大量聚类数据的总方差。聚类数量越多，方差就越小。如果我们的聚类数与数据集中的记录数相等，那么可变性将为零，因为每个点与其自身的距离为零。可变性或 SSE(误差平方和)以及“K”值如图 [8-13](#Fig13) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig13_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig13_HTML.jpg)

图 8-13

肘法

正如我们所观察到的，在 K 值为 3 和 4 之间有一种肘形结构。总方差(组内差异)会突然减少，之后方差会缓慢下降。事实上，它在 K=9 值之后变平。因此，如果我们使用肘方法，K =3 的值是最有意义的，因为它可以用较少的聚类数量捕获最多的可变性。

### 分层聚类

这是另一种类型的无监督机器学习技术，它不同于 K-means，因为我们不需要事先知道聚类的数量。有两种类型的分层聚类。

*   聚集聚类(自下而上的方法)

*   分裂聚类(自上而下的方法)

我们将讨论凝聚聚类，因为它是主要类型。首先假设每个数据点都是一个独立的聚类，然后逐渐将最近的值组合到相同的聚类中，直到所有的值都成为一个聚类的一部分。这是一种自底向上的方法，它计算每个聚类之间的距离，并将两个最接近的聚类合并为一个。让我们借助可视化来理解凝聚聚类。假设我们最初有七个数据点(A1-A7)，需要使用聚集聚类将它们分组到包含相似值的簇中，如图 [8-14](#Fig14) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig14_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig14_HTML.jpg)

图 8-14

每个值作为一个单独的集群

在初始阶段(步骤 1)，每个点被视为一个单独的聚类。在下一步中，计算每个点之间的距离，并将最近的点组合成单个聚类。在本例中，A1 和 A2、A5 和 A6 彼此距离最近，因此形成如图 [8-15](#Fig15) 所示的单个集群。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig15_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig15_HTML.jpg)

图 8-15

最近的聚类合并在一起

在使用层次聚类时，可以通过多种方式来确定最佳聚类数。一种方法是使用 elbow 方法本身，另一种方法是使用一种叫做树状图的东西。它用于可视化聚类之间的可变性(欧几里德距离)。在树状图中，垂直线的高度代表点或簇与底部列出的数据点之间的距离。每个点绘制在 X 轴上，距离表示在 Y 轴上(长度)。它是数据点的分层表示。在本例中，第 2 步的树状图如图 [8-16](#Fig16) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig16_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig16_HTML.jpg)

图 8-16

系统树图

在步骤 3 中，重复计算聚类之间的距离的练习，并将最近的聚类组合成单个聚类。这次 A3 与(A1，A2)合并，A4 与(A5，A6)合并，如图 [8-17](#Fig17) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig17_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig17_HTML.jpg)

图 8-17

最近的聚类合并在一起

第三步后的树状图如图 [8-18](#Fig18) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig18_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig18_HTML.jpg)

图 8-18

步骤 3 后的树状图

在步骤 4 中，计算唯一剩余的点 A7 之间的距离，并发现其更靠近群集(A4，A5，A6)。它与图 [8-19](#Fig19) 所示的同一个集群合并。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig19_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig19_HTML.jpg)

图 8-19

簇状构造

在最后一个阶段(步骤 5)，所有点被组合成一个单独的簇(A1，A2，A3，A4，A5，A6，A7)，如图 [8-20](#Fig20) 所示。

![../images/469852_1_En_8_Chapter/469852_1_En_8_Fig20_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Fig20_HTML.jpg)

图 8-20

凝聚聚类

有时很难通过树状图确定正确的聚类数，因为它可能变得非常复杂，并且很难根据用于聚类的数据集进行解释。与 K-means 相比，层次聚类在大型数据集上效果不佳。聚类对数据点的规模也非常敏感，因此总是建议在聚类之前进行数据缩放。还有其他类型的聚类可用于将相似的数据点分组在一起，如下所示:

1.  高斯混合模型聚类

2.  模糊 C 均值聚类

但以上方法不在本书讨论范围之内。我们现在开始使用 PySpark 中的 K-means 数据集来构建集群。

## 密码

本章的这一节将介绍使用 PySpark 和 Jupyter Notebook 进行 K-Means 聚类。

### 注意

完整的数据集和代码可以在本书的 GitHub repo 上参考，在 Spark 2.0 和更高版本上执行得最好。

在本练习中，我们考虑最标准化的开源数据集 IRIS 数据集，用于捕获聚类数并比较监督和非监督性能。

### 数据信息

我们将在本章使用的数据集是著名的开源 IRIS 数据集，包含总共 150 条记录，共 5 列(萼片长度、萼片宽度、花瓣长度、花瓣宽度、物种)。每种类型有 50 个记录。我们将尝试在不使用物种标签信息的情况下，将这些物种分组。

### 步骤 1:创建 SparkSession 对象

我们启动 Jupyter Notebook 并导入 SparkSession，然后创建一个新的 SparkSession 对象来使用 Spark:

```py
[In]: from pyspark.sql import SparkSession
[In]: spark=SparkSession.builder.appName('K_means').getOrCreate()

```

### 步骤 2:读取数据集

然后，我们使用 dataframe 在 Spark 中加载和读取数据集。我们必须确保我们已经从数据集可用的同一个目录文件夹中打开了 PySpark，否则我们必须提到数据文件夹的目录路径。

```py
[In]:
df=spark.read.csv('iris_dataset.csv',inferSchema=True,header=True)

```

### 步骤 3:探索性数据分析

在本节中，我们通过查看数据集并验证其形状来探索数据集。

```py
[In]:print((df.count(), len(df.columns)))
[Out]: (150,3)

```

因此，上面的输出确认了数据集的大小，然后我们可以验证输入值的数据类型，以检查我们是否需要更改/转换任何列的数据类型。

```py
[In]: df.printSchema()
[Out]: root
 |-- sepal_length: double (nullable = true)
 |-- sepal_width: double (nullable = true)
 |-- petal_length: double (nullable = true)
 |-- petal_width: double (nullable = true)
 |-- species: string (nullable = true)

```

总共有五列，其中四列是数字列，标签列是分类列。

```py
[In]: from pyspark.sql.functions import rand

[In]: df.orderBy(rand()).show(10,False)
[Out]:
+------------+-----------+------------+-----------+----------+
|sepal_length|sepal_width|petal_length|petal_width|species   |
+------------+-----------+------------+-----------+----------+
|5.5         |2.6        |4.4         |1.2        |versicolor|
|4.5         |2.3        |1.3         |0.3        |setosa    |
|5.1         |3.7        |1.5         |0.4        |setosa    |
|7.7         |3.0        |6.1         |2.3        |virginica |
|5.5         |2.5        |4.0         |1.3        |versicolor|
|6.3         |2.3        |4.4         |1.3        |versicolor|
|6.2         |2.9        |4.3         |1.3        |versicolor|
|6.3         |2.5        |4.9         |1.5        |versicolor|
|4.7         |3.2        |1.3         |0.2        |setosa    |
|6.1         |2.8        |4.0         |1.3        |versicolor|
+------------+-----------+------------+-----------+----------+

[In]: df.groupBy('species').count().orderBy('count').show(10,False)
[Out]:
+----------+-----+
|species   |count|
+----------+-----+
|virginica |50   |
|setosa    |50   |
|versicolor|50   |
+----------+-----+

```

因此，它确认了数据集中每个物种都有相同数量的记录

### 步骤 4:特征工程

这是我们使用 Spark 的 VectorAssembler 创建一个组合所有输入特征的单一向量的部分。它仅创建一个要素来捕获该特定行的输入值。因此，不是四个输入列(我们不考虑标签列，因为它是一种无监督的机器学习技术)，它本质上是以列表的形式将它转换为具有四个输入值的单个列。

```py
[In]: from pyspark.ml.linalg import Vector
[In]: from pyspark.ml.feature import VectorAssembler

[In]: input_cols=['sepal_length', 'sepal_width', 'petal_length', 'petal_width']

[In]: vec_assembler = VectorAssembler(inputCols = input_cols, outputCol="features")

[In]: final_data = vec_assembler.transform(df)

```

### 步骤 5:构建 K-Means 聚类模型

最终数据包含可用于运行 K 均值聚类的输入向量。由于我们需要在使用 K-means 之前预先声明' K '的值，所以我们可以使用 elbow 方法来计算出' K '的正确值。为了使用肘方法，我们对不同的“K”值运行 K 均值聚类。首先，我们从 PySpark 库中导入 K-means，并创建一个空列表来捕获 K 的每个值的可变性或 SSE(在聚类距离内)。

```py
[In]:from pyspark.ml.clustering import KMeans
[In]:errors=[]
[In]:
for k in range(2,10):
    kmeans = KMeans(featuresCol='features',k=k)
    model = kmeans.fit(final_data)
    intra_distance = model.computeCost(final_data)
    errors.append(intra_distance)

```

### 注意

“K”的最小值应该为 2，以便能够构建聚类。

现在，我们可以使用 numpy 和 matplotlib 绘制星团内距离与星团数量的关系。

```py
[In]: import pandas as pd
[In]: import numpy as np
[In]: import matplotlib.pyplot as plt
[In]: cluster_number = range(2,10)
[In]: plt.xlabel('Number of Clusters (K)')
[In]: plt.ylabel('SSE')
[In]: plt.scatter(cluster_number,errors)
[In]: plt.show()

[Out]:

```

![../images/469852_1_En_8_Chapter/469852_1_En_8_Figa_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Figa_HTML.jpg)

在这种情况下，k=3 似乎是最佳的聚类数，因为我们可以看到 3 到 4 个值之间的肘形结构。我们使用 k=3 构建最终的集群。

```py
[In]: kmeans = KMeans(featuresCol='features',k=3)
[In]: model = kmeans.fit(final_data)
[In]: model.transform(final_data).groupBy('prediction').count().show()
[Out]:
+----------+-----+
|prediction|count|
+----------+-----+
|         1|   50|
|         2|   38|
|         0|   62|
+----------+-----+

```

基于 IRIS 数据集，K-Means 聚类为我们提供了三个不同的聚类。我们肯定是做了一些错误的分配，因为只有一个类别有 50 条记录，其余的类别都混在一起了。我们可以使用 transform 函数将分类编号分配给原始数据集，并使用 groupBy 函数来验证分组。

```py
[In]: predictions=model.transform(final_data)

[In]: predictions.groupBy('species','prediction').count().show()

[Out]:
+----------+----------+-----+
|   species|prediction|count|
+----------+----------+-----+
| virginica|         2|   14|
|    setosa|         0|   50|
| virginica|         1|   36|
|versicolor|         1|    3|
|versicolor|         2|   47|
+----------+----------+-----+

```

正如可以观察到的，setosa 物种与 versicolor 完美地分组在一起，几乎被捕获在同一个簇中，但 verginica 似乎属于两个不同的组。K-means 每次都会产生不同的结果，因为它每次都随机选择起始点(质心)。因此，在 K-means 聚类中得到的结果可能与这些结果完全不同，除非我们使用种子来重现这些结果。种子确保分割和初始质心值在整个分析过程中保持一致。

### 步骤 6:集群的可视化

在最后一步，我们可以借助 Python 的 matplotlib 库来可视化新的集群。为此，我们首先将 Spark 数据帧转换成 Pandas 数据帧。

```py
[In]: pandas_df = predictions.toPandas()
[In]: pandas_df.head()

```

![../images/469852_1_En_8_Chapter/469852_1_En_8_Figb_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Figb_HTML.jpg)

我们导入所需的库来绘制第三个可视化并观察集群。

```py
[In]: from mpl_toolkits.mplot3d import Axes3D

[In]: cluster_vis = plt.figure(figsize=(12,10)).gca(projection='3d')

[In]: cluster_vis.scatter(pandas_df.sepal_length, pandas_df.sepal_width, pandas_df.petal_length, c=pandas_df.prediction,depthshade=False)

[In]: plt.show()

```

![../images/469852_1_En_8_Chapter/469852_1_En_8_Figc_HTML.jpg](../images/469852_1_En_8_Chapter/469852_1_En_8_Figc_HTML.jpg)

## 结论

在这一章中，我们讨论了不同类型的无监督机器学习技术，并使用 PySpark 中的 K-means 算法构建了聚类。K-Means 使用随机质心初始化对数据点进行分组，而分层聚类侧重于将整个数据点合并到单个聚类中。我们还介绍了各种确定最佳聚类数的技术，如肘方法和树状图，它们在对数据点进行分组时使用方差优化。