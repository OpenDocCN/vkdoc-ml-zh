# 五、逻辑回归

本章着重于用 PySpark 构建一个逻辑回归模型，并理解逻辑回归背后的思想。逻辑回归用于分类问题。我们已经在前面的章节中看到了分类的细节。虽然用于分类，但还是叫逻辑回归。这是因为在幕后，线性回归方程仍然可以找到输入变量和目标变量之间的关系。线性回归和逻辑回归之间的主要区别是，我们使用某种非线性函数将后者的输出转换为概率，以将其限制在 0 和 1 之间。例如，我们可以使用逻辑回归来预测用户是否会购买该产品。在这种情况下，模型将返回每个用户的购买概率。逻辑回归在许多商业应用中被广泛使用。

## 可能性

为了理解逻辑回归，我们必须先复习概率的概念。它被定义为在所有可能的结果中，期望的事件或感兴趣的结果发生的几率。举个例子，如果我们掷硬币，得到正面或反面的机会是相等的(50%)，如图 [5-1](#Fig1) 所示。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig1_HTML.png](img/469852_1_En_5_Chapter/469852_1_En_5_Fig1_HTML.png)

图 5-1

事件的概率

如果我们掷一个公平的骰子，那么得到(1 到 6)之间任何一个数字的概率等于 16.7%。

如果我们从一个包含四个绿色球和一个蓝色球的袋子中挑选一个球，挑选一个绿色球的概率是 80%。

逻辑回归用于预测每个目标类的概率。在二进制分类(只有两个类)的情况下，它返回与每个记录的每个类相关联的概率。如前所述，它在幕后使用线性回归来捕捉输入和输出变量之间的关系，但我们另外使用一个元素(非线性函数)来将输出从连续形式转换为概率。让我们借助一个例子来理解这一点。让我们考虑一下，我们必须使用模型来预测某个特定用户是否会购买该产品，我们只使用了一个输入变量，即用户在网站上花费的时间。相同的数据在表 [5-1](#Tab1) 中给出。

表 5-1。

转换数据集

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"></colgroup> 
| 

-你好。不，不

 | 

花费的时间(分钟)

 | 

修改的

 |
| --- | --- | --- |
| one | one | 不 |
| Two | Two | 不 |
| three | five | 不 |
| four | Fifteen | 是 |
| five | Seventeen | 是 |
| six | Eighteen | 是 |

让我们将这些数据形象化，以便看出转换用户和非转换用户之间的区别，如图 [5-2](#Fig2) 所示。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig2_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fig2_HTML.jpg)

图 5-2

转换状态与花费的时间

### 使用线性回归

让我们尝试使用线性回归而不是逻辑回归来理解逻辑回归在分类场景中更有意义的原因。为了使用线性回归，我们必须将目标变量从分类形式转换成数字形式。因此，让我们为转换后的列重新分配值:

是= 1

否= 0

现在，我们的数据看起来就像表 [5-2](#Tab2) 中给出的那样。

表 5-2。

抽样资料

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"></colgroup> 
| 

-你好。不，不

 | 

花费的时间(分钟)

 | 

修改的

 |
| --- | --- | --- |
| one | one | Zero |
| Two | Two | Zero |
| three | five | Zero |
| four | Fifteen | one |
| five | Seventeen | one |
| six | Eighteen | one |

将分类变量转换成数字变量的过程也很关键，我们将在本章的后半部分详细讨论这一点。现在，让我们绘制这些数据点，以便更好地可视化和理解它(图 [5-3](#Fig3) )。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig3_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fig3_HTML.jpg)

图 5-3

转换状态(1 和 0)与花费时间的关系

正如我们所观察到的，在我们的目标列中只有两个值(1 和 0)，并且每个点都位于这两个值中的任意一个上。现在，假设我们对这些数据点进行线性回归，得出一条“最佳拟合线”，如图 [5-4](#Fig4) 所示。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig4_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fig4_HTML.jpg)

图 5-4

用户回归线

这条线的回归方程是

![$$ y={B}_0+{B}_1\ast x $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equa.png)

![$$ {y}_{\left(1,0\right)}={B}_0+{B}_1\ast Time\ Spent $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equb.png)

就用一条直线来区分 1 和 0 值而言，到目前为止一切看起来都不错。看起来线性回归在区分转换用户和非转换用户方面做得很好，但是这种方法有一个小问题。

举个例子，一个新用户在网站上花了 20 秒，我们必须使用线性回归线来预测这个用户是否会转化。我们使用上面的回归方程，并尝试预测 20 秒所用时间的 y 值。

我们可以简单地通过计算来计算 y 的值

![$$ y={B}_0+{B}_1\ast (20) $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equc.png)

或者，我们也可以简单地从耗时轴到最佳拟合线上画一条垂直线来预测 y 的值。显然，y 的预测值 1.7 似乎远远大于 1，如图 [5-5](#Fig5) 所示。这种方法没有任何意义，因为我们只想预测 0 到 1 之间的值。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig5_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fig5_HTML.jpg)

图 5-5

使用回归线的预测

因此，如果我们对分类案例使用线性回归，就会产生预测输出值范围从–无穷大到+无穷大的情况。因此，我们需要另一种方法来将这些值限制在 0 和 1 之间。0 和 1 之间的值的概念不再陌生，因为我们已经看到了概率。因此，逻辑回归本质上提出了与概率值相关联的正类和负类之间的决策边界。

### 使用 Logit

为了实现将输出值转换成概率的目标，我们使用了一种叫做 Logit 的东西。Logit 是一个非线性函数，它对线性方程进行非线性变换，将输出在 0 和 1 之间转换。在逻辑回归中，非线性函数是 Sigmoid 函数，如下所示:

![$$ \frac{1}{1+{e}^{-x}} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equd.png)

它总是产生 0 到 1 之间的值，与 x 的值无关。

所以，回到我们之前的线性回归方程

![$$ y={B}_0+{B}_1\ast Time\ Spent $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Eque.png)

我们将输出(y)传递给这个非线性函数(sigmoid ),使其值在 0 和 1 之间变化。

概率= ![$$ \frac{1}{1+{e}^{-y}} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_IEq1.png)

概率= ![$$ \frac{1}{1+{e^{-}}^{\left({B}_0+{B}_1\ast Time\ Spent\right)}} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_IEq2.png)

使用上述等式，预测值被限制在 0 和 1 之间，输出现在如图 [5-6](#Fig6) 所示。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig6_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fig6_HTML.jpg)

图 5-6

逻辑曲线

使用非线性函数的优点是，无论输入值(花费的时间)如何，输出总是转换的概率。这条曲线也被称为逻辑曲线。逻辑回归还假设输入和目标变量之间存在线性关系，因此找出截距和系数的最佳值来捕捉这种关系。

## 解释(系数)

使用被称为梯度下降的技术找到输入变量的系数，该技术寻找以总误差最小化的方式优化损失函数。我们可以看看 logistic 回归方程，了解系数的解释。

![$$ y=\frac{1}{1+{e^{-\Big(}}^{B_0+{B}_1\ast x\Big)}} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equf.png)

比方说，在计算了我们示例中的数据点之后，我们得到花费时间的系数值为 0.75。

为了理解这个 0.75 意味着什么，我们必须取这个系数的指数值。

*e*T2 0.75= 2.12

这个 2.12 被认为是一个奇怪的比率，它表明在网站上花费的每单位时间的增加会增加 112%的客户转化几率。

## 虚拟变量

到目前为止，我们只处理了连续/数值变量，但数据集中出现分类变量是不可避免的。因此，让我们来理解使用分类值进行建模的方法。由于机器学习模型只消耗数字格式的数据，我们必须采用某种技术将分类数据转换成数字形式。我们已经在上面看到了一个例子，其中我们将目标类(Yes/No)转换为数值(1 或 0)。这就是所谓的标签编码，我们为特定列中的每个类别分配唯一的数值。还有一种方法非常有效，被称为实体模型化或热编码。让我们借助一个例子来理解这一点。让我们在现有的示例数据中再添加一列。假设我们有一个包含用户使用的搜索引擎的附加列。因此，我们的数据看起来像这样，如表 [5-3](#Tab3) 所示。

表 5-3。

分类数据集

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"> <col class="tcol4 align-center"></colgroup> 
| 

-你好。不，不

 | 

花费的时间(分钟)

 | 

搜索引擎

 | 

修改的

 |
| --- | --- | --- | --- |
| one | five | 谷歌 | Zero |
| Two | Two | 堆 | Zero |
| three | Ten | 美国 Yahoo 公司(提供互联网的信息检索服务) | one |
| four | Fifteen | 堆 | one |
| five | one | 美国 Yahoo 公司(提供互联网的信息检索服务) | Zero |
| six | Twelve | 谷歌 | one |

因此，要使用搜索引擎专栏中提供的额外信息，我们必须使用实体模型化将其转换为数字格式。因此，我们将获得额外数量的虚拟变量(列)，这将等于搜索引擎列中不同类别的数量。以下步骤解释了将分类特征转换为数字特征的整个过程。

表 5-5。

木乃伊化

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"> <col class="tcol4 align-center"> <col class="tcol5 align-center"> <col class="tcol6 align-center"></colgroup> 
| 

-你好。不，不

 | 

花费的时间(分钟)

 | 

如果 _Google

 | 

色冰

 | 

SE_Yahoo

 | 

修改的

 |
| --- | --- | --- | --- | --- | --- |
| one | one | one | Zero | Zero | Zero |
| Two | Two | Zero | one | Zero | Zero |
| three | five | Zero | Zero | one | Zero |
| four | Fifteen | Zero | one | Zero | one |
| five | Seventeen | Zero | one | Zero | one |
| six | Eighteen | one | Zero | Zero | one |

1.  找出类别列中不同类别的数量。到目前为止，我们只有三个不同的类别(谷歌、必应、雅虎)。

2.  Create new columns for each of the distinct categories and add value 1 in the category column for which it is present or else 0 as shown in Table [5-4](#Tab4).

    表 5-4。

    一个热编码

    <colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"> <col class="tcol4 align-center"> <col class="tcol5 align-center"> <col class="tcol6 align-center"> <col class="tcol7 align-center"></colgroup> 
    | 

    -你好。不，不

     | 

    花费的时间(分钟)

     | 

    搜索引擎

     | 

    如果 _Google

     | 

    色冰

     | 

    SE_Yahoo

     | 

    修改的

     |
    | --- | --- | --- | --- | --- | --- | --- |
    | one | one | 谷歌 | one | Zero | Zero | Zero |
    | Two | Two | 堆 | Zero | one | Zero | Zero |
    | three | five | 美国 Yahoo 公司(提供互联网的信息检索服务) | Zero | Zero | one | Zero |
    | four | Fifteen | 堆 | Zero | one | Zero | one |
    | five | Seventeen | 美国 Yahoo 公司(提供互联网的信息检索服务) | Zero | one | Zero | one |
    | six | Eighteen | 谷歌 | one | Zero | Zero | one |

3.  删除原始类别列。因此，数据集现在总共包含五列(不包括索引)，因为我们有三个额外的虚拟变量，如表 [5-5](#Tab5) 所示。

整个想法是以不同的方式表示相同的信息，以便机器学习模型也可以从分类值中学习。

## 模型评估

为了衡量逻辑回归模型的性能，我们可以使用多个指标。最明显的是精度参数。准确性是模型做出的正确预测的百分比。然而，准确性并不总是首选的方法。为了理解逻辑模型的性能，我们应该使用混淆矩阵。它由预测值计数和实际值组成。二进制类的混淆矩阵如表 [5-6](#Tab6) 所示。

表 5-6。

混淆矩阵

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

实际/预测

 | 

预测类别(是)

 | 

预测类别(否)

 |
| --- | --- | --- |
| **实际类别(是)** | 真正数(TP) | 假阴性(FN) |
| **实际类别(无)** | 假阳性(FP) | 真阴性(TN) |

让我们理解混淆矩阵中的单个值。

### 真阳性

这些是实际上属于正类的值，并且模型也正确地预测了它们属于正类。

*   **实际类:**正(1)

*   **ML 模型预测类**:正(1)

### 真正的否定

这些值实际上属于负类，并且模型也正确地预测了它们属于负类。

*   **实际类别:**负(0)

*   **ML 模型预测类:**阴性(1)

### 假阳性

这些值实际上属于负类，但模型错误地预测它们属于正类。

*   **实际类别:**负(0)

*   **ML 模型预测类:**正(1)

### 假阴性

这些值实际上属于正类，但模型错误地预测它们属于负类。

*   **实际类:**正(1)

*   **ML 模型预测类:**阴性(1)

### 准确

准确度是真阳性和真阴性的总和除以记录总数:

![$$ \frac{\left( TP+ TN\right)}{Total\ number\ of\ Records} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equg.png)

但是如前所述，由于目标阶层的不平衡，它并不总是首选指标。大多数时候，目标类频率是偏斜的(与 TP 示例相比，TN 示例的数量更大)。例如，欺诈检测数据集包含 99 %的真实交易和仅 1%的欺诈交易。现在，如果我们的逻辑回归模型预测所有真实交易，没有欺诈案件，它仍然有 99%的准确率。全部的重点是找出关于积极类的表现；因此，我们可以使用几个其他评估指标。

### 回忆

召回率有助于从正面类的角度评估模型的性能。它表示模型能够正确预测的实际阳性案例占阳性案例总数的百分比。

![$$ \frac{(TP)}{\left( TP+ FN\kern0.28em \right)} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equh.png)

它谈到了机器学习模型在预测积极类时的质量。那么，在所有积极的类别中，该模型能够正确预测多少呢？该指标被广泛用作分类模型的评估标准。

### 精确

精度是指模型预测的所有阳性案例中实际阳性案例的数量:

![$$ \frac{(TP)}{\left( TP+ FP\right)} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equi.png)

这些也可以作为评价标准。

### F1 分数

F1 得分= ![$$ 2\ast \frac{\left( Precision\ast Recall\right)}{\left( Precision+ Recall\kern0.28em \right)} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_IEq3.png)

### 截止/阈值概率

因为我们知道逻辑回归模型的输出是概率得分，所以决定预测概率的截止值或阈值是非常重要的。默认情况下，概率阈值设置为 50%。这意味着，如果模型的概率输出低于 50%，模型将预测它属于负类(0)，如果它等于并高于 50%，它将被分配正类(1)。

如果阈值限制非常低，那么该模型将预测许多肯定的类别，并且将具有高召回率。相反，如果阈值概率非常高，则模型可能会错过正例，召回率会很低，但精度会更高。在这种情况下，模型将预测很少的阳性病例。决定一个好的阈值通常是具有挑战性的。受试者操作者特征曲线，或 ROC 曲线，可以帮助决定哪个阈值是最好的。

### 受试者工作特征曲线

ROC 用于决定模型的阈值。如图 [5-7](#Fig7) 所示，是召回率(也称为灵敏度)和精确度(特异性)之间的关系图。

![img/469852_1_En_5_Chapter/469852_1_En_5_Fig7_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fig7_HTML.jpg)

图 5-7

受试者工作特征曲线

人们希望选择一个在召回率和精确度之间提供平衡的阈值。因此，现在我们已经了解了与逻辑回归相关的各种组件，我们可以继续使用 PySpark 构建逻辑回归模型。

## 逻辑回归代码

本章的这一节重点介绍如何使用 PySpark 和 Jupyter Notebook 从头构建一个逻辑回归模型。

### 注意

这本书的 GitHub repo 上提供了完整的数据集和代码，在 Spark 2.3 和更高版本上执行得最好。

让我们使用 Spark 的 MLlib 库建立一个逻辑回归模型，并预测目标类标签。

### 数据信息

我们将在本例中使用的数据集是一个虚拟数据集，总共包含 20，000 行和 6 列。我们必须使用 5 个输入变量，通过逻辑回归模型来预测目标类别。该数据集包含关于零售体育商品网站的在线用户的信息。这些数据包括用户的国家、使用的平台、年龄、回头客或首次访客，以及在网站上浏览的网页数量。它还包含客户最终是否购买了产品的信息(转换状态)。

### 步骤 1:创建 Spark 会话对象

我们启动 Jupyter 笔记本并导入 SparkSession，然后创建一个新的 SparkSession 对象来使用 Spark。

```py
[In]: from pyspark.sql import SparkSession
[In]: spark=SparkSession.builder.appName('log_reg').getOrCreate()

```

### 步骤 2:读取数据集

然后，我们使用 Dataframe 在 Spark 中加载和读取数据集。我们必须确保我们已经从数据集可用的同一个目录文件夹中打开了 PySpark，否则我们必须提到数据文件夹的目录路径。

```py
[In]: df=spark.read.csv('Log_Reg_dataset.csv',inferSchema=True,header=True)

```

### 步骤 3:探索性数据分析

在本节中，我们将通过查看数据集并验证它的形状和变量的各种统计测量值来更深入地研究数据集。我们从检查数据集的形状开始:

```py
[In]:print((df.count(), len(df.columns)))
[Out]: (20000, 6)

```

因此，上面的输出确认了数据集的大小，然后我们可以验证输入值的数据类型，以检查我们是否需要更改/转换任何列的数据类型。

```py
[In]: df.printSchema()
[Out]: root
 |-- Country: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- Repeat_Visitor: integer (nullable = true)
 |-- Search_Engine: string (nullable = true)
 |-- Web_pages_viewed: integer (nullable = true)
 |-- Status: integer (nullable = true)

```

正如我们所看到的，有两个这样的列(Country，Search_Engine)，它们本质上是分类的，因此需要转换成数字形式。让我们用 Spark 中的 show 函数来看看数据集。

```py
[In]: df.show(5)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figa_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figa_HTML.jpg)

我们现在可以使用 describe 函数来检查数据集的统计度量。

```py
[In]: df.describe().show()
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figb_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figb_HTML.jpg)

我们可以观察到，访问者的平均年龄接近 28 岁，他们在网站访问期间查看了大约 9 个网页。

让我们研究各个列，以便更深入地了解数据的细节。与 counts 一起使用的 groupBy 函数返回数据中每个类别的出现频率。

```py
[In]: df.groupBy('Country').count().show()
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figc_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figc_HTML.jpg)

因此，来自印度尼西亚的游客数量最多，其次是印度:

```py
[In]: df.groupBy('Search_Engine').count().show()
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figd_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figd_HTML.jpg)

雅虎搜索引擎的用户数量最多。

```py
[In]: df.groupBy('Status').count().show()
[Out]:
+------+-----+
|Status|count|
+------+-----+
|     1|10000|
|     0|10000|
+------+-----+

```

我们有同等数量的用户转化和非转化。

让我们使用`groupBy`函数和均值来了解更多关于数据集的信息。

```py
[In]: df.groupBy('Country').mean().show()
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Fige_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fige_HTML.jpg)

我们来自马来西亚的转化率最高，其次是印度。网页平均访问量在马来西亚最高，在巴西最低。

```py
[In]: df.groupBy('Search_Engine').mean().show()

[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figf_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figf_HTML.jpg)

我们从使用谷歌搜索引擎的用户访问者那里获得了最高的转化率。

```py
[In]: df.groupBy(Status).mean().show()
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figg_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figg_HTML.jpg)

我们可以清楚地看到，转换状态和页面浏览数量以及重复访问之间有着密切的联系。

### 步骤 4:特征工程

在这一部分，我们将分类变量转换成数字形式，并使用 Spark 的`VectorAssembler`创建一个组合了所有输入特征的向量。

```py
[In]: from pyspark.ml.feature import StringIndexer
[In]: from pyspark.ml.feature import VectorAssembler

```

由于我们要处理两个分类列，我们必须将国家和搜索引擎列转换成数字形式。机器学习模型无法理解分类值。

第一步是使用`StringIndexer` `into numerical form`标记列。它为列的每个类别分配唯一的值。因此，在下面的例子中，搜索引擎(Yahoo，Google，Bing)的所有三个值都被赋值(0.0，1.0，2.0)。这在名为`search_engine_num`的栏目中可见一斑。

```py
[In]: search_engine_indexer =StringIndexer(inputCol="Search_Engine", outputCol="Search_Engine_Num").fit(df)

[In]:df = search_engine_indexer.transform(df)

[In]: df.show(3,False)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figh_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figh_HTML.jpg)

```py
[In]: df.groupBy('Search_Engine').count().orderBy('count',ascending=False).show(5,False)

[Out]: 

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figi_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figi_HTML.jpg)

```py
[In]: df.groupBy(‘Search_Engine_Num').count().orderBy('count',ascending=False).show(5,False)

[Out]: 

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figj_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figj_HTML.jpg)

下一步是将这些值中的每一个表示成一个热编码向量的形式。然而，这个向量在表示方面有一点不同，因为它捕获向量中的值和位置。

```py
[In]: from pyspark.ml.feature import OneHotEncoder

[In]:search_engine_encoder=OneHotEncoder(inputCol="Search_Engine_Num", outputCol="Search_Engine_Vector")
 [In]: df = search_engine_encoder.transform(df)

[In]: df.show(3,False)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figk_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figk_HTML.jpg)

```py
[In]: df.groupBy('Search_Engine_Vector').count().orderBy('count',ascending=False).show(5,False)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figl_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figl_HTML.jpg)

我们将用于构建逻辑回归的最后一个特性是`Search_Engine_Vector`。让我们理解这些列值代表什么。

```py
 (2,[0],[1.0]) represents a vector of length 2 , with 1 value :

Size of Vector – 2
Value contained in vector – 1.0
Position of 1.0 value in vector – 0th place

```

这种表示法可以节省计算空间，从而加快计算速度。向量的长度等于元素总数减 1，因为每个值只需借助两列就可以很容易地表示出来。例如，如果我们需要使用一种热编码来表示搜索引擎，通常，我们可以这样做，如下所示。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

搜索引擎

 | 

谷歌

 | 

美国 Yahoo 公司(提供互联网的信息检索服务)

 | 

堆

 |
| --- | --- | --- | --- |
| 谷歌 | one | Zero | Zero |
| 美国 Yahoo 公司(提供互联网的信息检索服务) | Zero | one | Zero |
| 堆 | Zero | Zero | one |

以优化方式表示上述信息的另一种方式是使用两列而不是三列，如下所示。

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

搜索引擎

 | 

谷歌

 | 

美国 Yahoo 公司(提供互联网的信息检索服务)

 |
| --- | --- | --- |
| 谷歌 | one | Zero |
| 美国 Yahoo 公司(提供互联网的信息检索服务) | Zero | one |
| 堆 | Zero | Zero |

让我们对另一个分类列(`Country`)重复相同的过程。

```py
[In]:country_indexer = StringIndexer(inputCol="Country", outputCol="Country_Num").fit(df)
[In]: df = country_indexer.transform(df)
[In]: df.groupBy('Country').count().orderBy('count',ascending=False).show(5,False)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figm_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figm_HTML.jpg)

```py
[In]: df.groupBy('Country_Num').count().orderBy('count',ascending=False).show(5,False)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Fign_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Fign_HTML.jpg)

```py
[In]: country_encoder = OneHotEncoder(inputCol="Country_Num", outputCol="Country_Vector")
[In]: df = country_encoder.transform(df)

[In]: df.select(['Country','Country_Num','Country_Vector']).show(3,False)
[Out]:

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figo_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figo_HTML.jpg)

```py
[In]: df.groupBy('Country_Vector').count().orderBy('count',ascending=False).show(5,False)
[Out]: 

```

![img/469852_1_En_5_Chapter/469852_1_En_5_Figp_HTML.jpg](img/469852_1_En_5_Chapter/469852_1_En_5_Figp_HTML.jpg)

既然我们已经将两个分类列转换为数字形式，我们需要将所有输入列组合成一个向量，作为模型的输入特征。

因此，我们选择需要用来创建单个特征向量的输入列，并将输出向量命名为 features。

```py
[In]: df_assembler = VectorAssembler(inputCols=['Search_Engine_Vector','Country_Vector','Age', 'Repeat_Visitor','Web_pages_viewed'], outputCol="features")
[In}:df = df_assembler.transform(df)

[In]: df.printSchema()
[Out]:
root
 |-- Country: string (nullable = true)
 |-- Age: integer (nullable = true)
 |-- Repeat_Visitor: integer (nullable = true)
 |-- Search_Engine: string (nullable = true)
 |-- Web_pages_viewed: integer (nullable = true)
 |-- Status: integer (nullable = true)
 |-- Search_Engine_Num: double (nullable = false)
 |-- Search_Engine_Vector: vector (nullable = true)
 |-- Country_Num: double (nullable = false)
 |-- Country_Vector: vector (nullable = true)
 |-- features: vector (nullable = true) 

```

正如我们所看到的，现在我们有了一个名为 features 的额外列，它是所有输入要素的组合，表示为一个密集矢量。

```py
[In]: df.select(['features','Status']).show(10,False)
[Out]:
+-----------------------------------+------+
|features                           |Status|
+-----------------------------------+------+
|[1.0,0.0,0.0,1.0,0.0,41.0,1.0,21.0]|1     |
|[1.0,0.0,0.0,0.0,1.0,28.0,1.0,5.0] |0     |
|(8,[1,4,5,7],[1.0,1.0,40.0,3.0])   |0     |
|(8,[2,5,6,7],[1.0,31.0,1.0,15.0])  |1     |
|(8,[1,5,7],[1.0,32.0,15.0])        |1     |
|(8,[1,4,5,7],[1.0,1.0,32.0,3.0])   |0     |
|(8,[1,4,5,7],[1.0,1.0,32.0,6.0])   |0     |
|(8,[1,2,5,7],[1.0,1.0,27.0,9.0])   |0     |
|(8,[0,2,5,7],[1.0,1.0,32.0,2.0])   |0     |
|(8,[2,5,6,7],[1.0,31.0,1.0,16.0])  |1     |
+-----------------------------------+------+
only showing top 10 rows

```

让我们只选择 features 列作为输入，Status 列作为输出来训练逻辑回归模型。

```py
 [In]: model_df=df.select(['features','Status'])

```

### 步骤 5:拆分数据集

为了训练和评估逻辑回归模型的性能，我们必须将数据集分为训练和测试数据集。我们以 75/25 的比例分割它，并在数据集的 75%上训练我们的模型。拆分数据的另一个用途是，我们可以使用 75%的数据来应用交叉验证，以便得出最佳的超参数。交叉验证可以是不同的类型，其中训练数据的一部分被保留用于训练，而剩余部分用于验证目的。K-fold 交叉验证主要用于训练具有最佳超参数的模型。

我们可以打印火车的形状和测试数据来验证尺寸。

```py
[In]: training_df,test_df=model_df.randomSplit([0.75,0.25])
[In]: print(training_df.count())
[Out]: (14907)

[In]: training_df.groupBy('Status').count().show()
[Out]:
+------+-----+
|Status|count|
+------+-----+
|     1| 7417|
|     0| 7490|
+------+-----+

```

这确保我们在训练和测试集中有一个目标类(`Status`)的平衡集。

```py
[In]:print(test_df.count())
[Out]: (5093)

[In]: test_df.groupBy('Status').count().show()
[Out]:
+------+-----+
|Status|count|
+------+-----+
|     1| 2583|
|     0| 2510|
+------+-----+

```

### 步骤 6:建立和训练逻辑回归模型

在这一部分中，我们使用功能作为输入列，状态作为输出列来构建和训练逻辑回归模型。

```py
[In]: from pyspark.ml.classification import LogisticRegression

[In]: log_reg=LogisticRegression(labelCol='Status').fit(training_df)

```

### 培训结果

我们可以使用 Spark 中的 evaluate 函数访问模型做出的预测，该函数以优化的方式执行所有步骤。这给出了另一个总共包含四列的数据框架，包括预测和概率。`prediction`列表示模型已经为给定行预测的类标签，而`probability`列包含两个概率(第 0 个索引处的负类的概率和第 1 个索引处的正类的概率)。

```py
[In]: train_results=log_reg.evaluate(training_df).predictions

[In]: train_results.filter(train_results['Status']==1).filter(train_results['prediction']==1).select(['Status','prediction','probability']).show(10,False)
[Out]:
+------+----------+----------------------------------------+
|Status|prediction|probability                             |
+------+----------+----------------------------------------+
|1     |1.0       |[0.2978572628475072,0.7021427371524929] |
|1     |1.0       |[0.2978572628475072,0.7021427371524929] |
|1     |1.0       |[0.16704676975730415,0.8329532302426959]|
|1     |1.0       |[0.16704676975730415,0.8329532302426959]|
|1     |1.0       |[0.16704676975730415,0.8329532302426959]|
|1     |1.0       |[0.08659913656062515,0.9134008634393749]|
|1     |1.0       |[0.08659913656062515,0.9134008634393749]|
|1     |1.0       |[0.08659913656062515,0.9134008634393749]|
|1     |1.0       |[0.08659913656062515,0.9134008634393749]|
|1     |1.0       |[0.08659913656062515,0.9134008634393749]|
+------+----------+----------------------------------------+

```

因此，在上述结果中，第 0 个指标的概率是针对`Status = 0`的，第 1 个指标的概率是针对`Status =1`的。

### 步骤 7:评估测试数据的线性回归模型

整个练习的最后一部分是检查模型在未知或测试数据上的性能。我们再次利用 evaluate 函数对测试进行预测。

我们将预测数据帧分配给结果，结果数据帧现在包含五列。

```py
[In]:results=log_reg.evaluate(test_df).predictions
[In]: results.printSchema()
[Out]:
root
 |-- features: vector (nullable = true)
 |-- Status: integer (nullable = true)
 |-- rawPrediction: vector (nullable = true)
 |-- probability: vector (nullable = true)
 |-- prediction: double (nullable = false)

```

我们可以使用 select 关键字过滤我们想要查看的列。

```py
[In]: results.select(['Status','prediction']).show(10,False)
[Out]:
+------+----------+
|Status|prediction|

+------+----------+
|0     |0.0       |
|0     |0.0       |
|0     |0.0       |
|0     |0.0       |
|1     |0.0       |
|0     |0.0       |
|1     |1.0       |
|0     |1.0       |
|1     |1.0       |
|1     |1.0       |
+------+----------+
only showing top 10 rows

```

由于这是一个分类问题，我们将使用混淆矩阵来衡量模型的性能。

### 混淆矩阵

我们将手动为真阳性、真阴性、假阳性和假阴性创建变量，以更好地理解它们，而不是使用直接的内置函数。

```py
[In]:tp = results[(results.Status == 1) & (results.prediction == 1)].count()
[In]:tn = results[(results.Status == 0) & (results.prediction == 0)].count()
[In]:fp = results[(results.Status == 0) & (results.prediction == 1)].count()
[In]:fn = results[(results.Status == 1) & (results.prediction == 0)].count()

```

#### 准确

正如本章已经讨论过的，精确度是评估任何分类器的最基本的度量标准；然而，由于对目标类平衡的依赖性，这不是模型性能的正确指标。

![$$ \frac{\left( TP+ TN\right)}{\left( TP+ TN+ FP+ FN\right)} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equj.png)

```py
[In]: accuracy=float((true_postives+true_negatives) /(results.count()))
[In]:print(accuracy)
[Out]: 0.9374255065554231

```

我们建立的模型的准确率大约为 94%。

#### 回忆

召回率显示了在所有的正类观察结果中，我们能够正确预测的正类案例的数量。

![$$ \frac{TP}{\left( TP+ FN\right)} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equk.png)

```py
[In]: recall = float(true_postives)/(true_postives + false_negatives)
[In]:print(recall)
[Out]: 0.937524870672503

```

模型的召回率在 0.94 左右。

#### 精确

![$$ \frac{TP}{\left( TP+ FP\right)} $$](img/469852_1_En_5_Chapter/469852_1_En_5_Chapter_TeX_Equl.png)

精确率是指在所有预测的阳性观察值中，正确预测的真阳性的数量:

```py
[In]: precision = float(true_postives) / (true_postives + false_positives)
[In]: print(precision)
[Out]: 0.9371519490851233

```

因此，召回率和精确率也在相同的范围内，这是因为我们的目标类别得到了很好的平衡。

## 结论

在本章中，我们回顾了理解逻辑回归的构建模块、将分类列转换为数字特征以及使用 PySpark 从头构建逻辑回归模型的过程。