# 4.机器学习算法简介

既然我们已经讲述了如何编程的基础知识和 Python 中的一些数据结构，让我们把注意力转回到第一章中提到的机器学习(ML)的理论和概念上。这一次，我们将开始讨论这些算法的细节，即，关注它们的输入和输出是什么以及它们是如何工作的(在高层次上；比这更低的数字涉及到大量的数学知识，不值得在入门书籍中讨论。

## ML 算法基础

ML 算法通常经历一个“训练”的过程，之后是一个被称为“测试”的评估期。大多数算法的训练过程大致遵循这种粗略的模式:(1)使算法看到数据的子集。(2)对我们希望从该数据中预测的事情进行初步猜测(例如，预测图像中存在何种类型的癌症，预测个体是否有患糖尿病的风险)。(3)看看这个猜测有多错误(如果我们有“基本事实”数据)。(4)调整预测的内部结构，希望朝着减少误差的方向。(5)重复(通常直到在我们暴露算法的数据集上结果没有显著改善)。

训练过程本身可以运行多次，因为 ML 算法将在您提供给它的称为“训练数据”的数据子集上运行一旦算法被适当地训练，它就在训练过程中从未暴露给 ML 算法的一组数据(称为“测试数据”)上被评估。为什么我们关心确保算法以前从未看到过数据？嗯，我们希望确保算法实际上正在学习一些关于数据结构的知识，而不仅仅是记忆单个数据点(即“过拟合”)。通过对未用于训练数据的维持集进行测试，我们可以评估该算法，并了解它在现实世界中的表现。

*   **边注**:在 ML 算法的训练步骤中，我们可能还想测试我们选择的 ML 算法的不同配置，甚至比较多个 ML 算法。一种方法是根据我们的数据子集进行训练，然后使用剩余的数据进行评估。然而，这会导致测试数据的意外过度拟合，因为您将调整 ML 算法参数以在测试集上执行良好。最终，我们应该对测试集完全视而不见，直到我们准备好在开发的最后一步评估它的性能。

*   因此，如果我们在尝试不同的参数/ML 模型时不应该在测试集上进行评估，我们如何比较性能呢？我们可以把训练集分成训练集和验证集。在这种情况下，我们将只使用验证集来评估我们跨多个 ML 配置/模型的训练网络，挑选出最佳的一个，然后在测试集上测试它，作为最后一步。通常，您会看到 60%的训练、20%的验证和 20%的底层数据测试分割。这意味着 60%的数据用于训练 ML 算法，20%用于验证/比较配置/多个 ML 模型，最后 20%的数据用于测试验证后选择的 ML 算法。您也可以用其他方式分割您的数据(例如，80%训练，10%验证，10%测试)，但是您应该确保您有足够的测试数据来准确了解您的算法在“真实世界”条件下的表现(例如，只测试两三个数据点没有多大用处，因为完全靠运气解决问题的可能性相对较高)。

ML 算法通常分为两大类(技术上还有第三类，但我们将在本书中跳过这一类):“监督学习”算法和“非监督学习”算法。

监督学习算法要求我们在数据上有“标签”。我所说的“标签”是指数据具有某种结果，这种结果可以是分类或连续的度量(例如，心脏病状态、预测的生存概率)。我们的数据通常具有与最终标签本身相关的多个特征(预测因素)。因此，当我们评估这些监督算法的表现时，我们将 ML 算法作为给定基准点的标签输出的内容与该点的实际标签进行比较。然后我们可以报告准确性、敏感性、特异性、ROC 等。这些算法一旦在测试集上评估。

无监督学习算法在没有标签的数据上运行。相反，他们试图优化另一个指标。例如，无监督聚类算法可能专注于尝试找到彼此密切相关的数据聚类。在这种情况下，优化的结果是数据点在一个聚类内相对于在聚类之间彼此有多紧密相关(例如，数据点之间的距离)(我们希望数据在一个聚类内彼此紧密相关，而与其他聚类中的数据不同)。在这种情况下，算法将试图优化如何将数据点分配给聚类。这里，除了描述数据点的特征之外，我们不需要任何关于数据本身的信息(即，我们不需要任何与最终诊断有关的信息，等等)。).无监督学习任务的输出在探索性数据分析中非常有用。例如，在遗传学研究中，无监督学习算法可用于根据基因表达水平区分样本。样品之间的最终分离可以产生对样品之间差异的洞察，从而产生进一步研究的区域。

以下是对机器学习领域中各个算法的一组总结。注意:这个列表并不意味着详尽无遗。更确切地说，它旨在给出现有的不同类型的 ML 算法以及它们如何工作的粗略理解。除非绝对必要，否则在这些解释中将使用非常少的数学(所以这不会非常严格)。

## 回归

这类最大似然算法处理从数据点到连续(即数值)值的尝试。人们学习的第一个回归算法可能是线性回归，它专注于寻找一条“最佳拟合线”，该线穿过具有 X 和 Y 方向的二维散点图上的数据点。但是，请注意，这些回归技术中的任何一种都可以处理多维数据。就 ML 而言，我们不试图可视化这些多维数据，因为我们的数据可能经常是“宽的”(即，每个基准点有多个预测值)。相反，我们将专注于尝试找到最佳的线(对于二维数据)、平面(对于三维数据)或超平面(对于 N 维数据，其中 N 是任何数字)来拟合我们的数据点。我们将从线性回归开始探索回归技术。从那里，我们将继续进行逻辑回归，它可以帮助预测结果的概率(在 0 和 1 之间)。最后，我们将讨论用于回归的套索和弹性网(这些算法有助于确保我们不会在模型中包含太多变量)。注意，我多次提到“模型”这个词，它的定义如下:模型只是一个可以被训练或评估的 ML 算法的实例。

### 线性回归(用于分类任务)

线性回归有许多不同的风格。也许最有用的是普通最小二乘法(或最小二乘法线性回归)。该算法的工作原理是试图最小化“残差平方和”(SSR)。那是什么？嗯，这是一个衡量我们提出的线性回归方程有多“差”的指标。我们举个简单的例子。

想象一下，我们有一些服用了减肥药的病人，我们想知道他们在某段时间内服用该药后减了多少磅。在这种情况下，我们希望预测的结果是体重减轻，输入是患者的特征，例如他们的起始体重、年龄、糖尿病状况、身体质量指数，以及他们在任何给定的一周内锻炼了多少分钟。

在线性回归算法运行后，该算法为我们提供了患者体重下降量的预测值。这应该试图代表趋势，同时也尽量减少个别数据点的“错误”程度。为了做到这一点，该算法被设置为最小化“残差”，即，该算法对体重减轻的预测与该患者的实际体重减轻相比有多远。然后对残差求平方，因为一些预测值将低于实际值，一些将高于实际值(我们对这些残差求平方以确保值不会相互抵消)。形式上，对于每个病人 *p* ，我们找到

![$$ \mathrm{residual}={\left(\mathrm{predicted}\ \mathrm{weight}\ \mathrm{lost}\ \mathrm{for}\ \mathrm{p}-\mathrm{actual}\ \mathrm{weight}\ \mathrm{lost}\ \mathrm{for}\ \mathrm{p}\right)}^2 $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equa.png)

然后，它将所有这些残差值相加，得出一个称为残差平方和的值。这是普通最小二乘回归试图最小化的量。一些微积分实际上显示了使用这种方法的线性回归有一个“封闭形式”的解决方案(即，它可以在一个步骤中运行)。相应地，该算法找到适当的斜率和截距。在这一点上，模型完成了训练，我们有了一个通用方程，可以预测一个人在服用减肥药物后体重减轻了多少。然后，我们需要确定该算法在真实世界数据上的效果如何(在我们的测试集中)。

该算法的输出非常容易理解。在大多数 Python 库中(甚至在 Excel 中)，您可以获得对回归有贡献的每个变量的斜率列表以及截距，从而得到一个看起来像

![$$ \mathrm{weight}\ \mathrm{lost}=\mathrm{intercept}+{\beta}_1\left(\mathrm{starting}\ \mathrm{weight}\right)+{\beta}_2\left(\mathrm{diabetes}\right)+{\beta}_3(BMI)+{\beta}_4\left(\mathrm{exercise}\right) $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equb.png)

的方程

其中*β*<sub>T3】nT5】代表一个斜率。如果我们试图将这种线性回归的结果绘制成图形，我们可能会运气不好，因为有五个维度(四个预测维度+一个输出维度)，并且很难在计算机上描绘三维图形以上的任何东西。相反，我们可以看看*β*<sub>*n*</sub>(β)的绝对值，看看哪些是最大的。由此，我们可以做出合理的假设，即最大的贝塔系数对结果的贡献最大。在各种流行病学研究中，线性回归被广泛使用，尤其是在给定一些个体数据的情况下试图量化结果的有效性(例如，健康运动对减肥的影响)时。</sub>

### 逻辑回归

逻辑回归类似于线性回归，因为它可以接受多个可能的预测值，并找到斜率/截距，使直线、斜率或超平面最佳拟合。但是，线性回归和逻辑回归之间的主要区别在于来自这些函数的输出值的范围。线性回归输出范围从负无穷大到正无穷大的值。但是，逻辑回归只能输出从 0 到 1 的值。考虑到逻辑回归的有限输出范围，它非常适合涉及可能性/概率预测的应用，并且可以帮助预测二元结果(例如，疾病对非疾病)，因为疾病状态可以编码为 1，而非疾病状态可以编码为 0。在医学应用中，逻辑回归模型通常应用于病例对照研究，因为β一旦指数化，就可以解释为优势比(见边注)，从而产生很大程度的可解释性。

我们试图拟合的逻辑回归方程如下:

![$$ y=\frac{e^{\beta_0+{\beta}_1\ast X+{\beta}_2\ast X\dots }}{1+{e}^{\beta_0+{\beta}_1\ast X+{\beta}_2\ast X\dots }} $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equc.png)

进一步简化

![$$ \mathsf{\ln}\left(y/\left(1-y\right)\right)={\beta}_0+{\beta}_1\ast X+{\beta}_2\ast X\dots $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equd.png)

该等式的左侧相当于我们通常认为的某件事“可能性”的自然对数(即某件事发生的概率 y 除以某件事不发生的概率 1-y)。因此，我们可以插入 *X* (我们对个体的预测)的值，并找到他们发生事件 *y* 的几率。然而，在大多数情况下(如病例对照研究)，我们不能单独报告几率(因为病例对照研究有预设的疾病规模，即“病例”人群和正常人群，即“对照”人群)。相反，我们可以取两个独立个体的预测比值，并确定比值比。

例如，如果我们在给定某人先前吸烟史的情况下预测患肺癌的概率，我们可以在病例对照研究中对个体拟合逻辑回归方程。在这种情况下，我们只有两个β，截距(*β*T2】0 和*β*T6】1(用于指示先前的吸烟史)。假设 *β* <sub>0</sub> 等于 1*β*<sub>1</sub>= 3.27。如果我们想比较吸烟者和不吸烟者患肺癌的几率，我们会计算![$$ \frac{e^{1+3.27\ast 1}}{e^{1+3.27\ast 0}}=26.31 $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_IEq1.png)。我们可以说，有吸烟史的人患肺癌的几率是没有吸烟史的人的 26.31 倍。注意，我在某人有吸烟史的情况下代入 X = 1，如果没有，则代入 X = 0(这里 X 作为指示变量)。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig1_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig1_HTML.jpg)

图 4-1

接收操作特性(ROC)曲线示例

*   **附注**:逻辑回归方程的输出值也可以被认为是预测概率(即，一个从 0 到 1 的值，表示某件事情发生的可能性，其中 1 =会发生，0 =不会发生)。当逻辑回归连续输出这些值时，我们可以建立一个“阈值”值，将连续预测转化为二元结果(小于阈值或大于阈值)。这在诸如预测疾病结果的任务中是有用的。然而，阈值的值是由程序员决定的。一个容易选择的阈值可以是 0.5；然而，另一个阈值如 0.7 可能会更好(可能有助于我们消除任何“假阳性”预测，代价是做出一些假阴性预测)。假阳性(FP)预测表明，输出预测患者是阳性病例，而实际上不是(这导致更高的医疗保健支出和不必要的治疗)。另一方面，假阴性是指预测患者没有感兴趣的结果，但实际上有(这导致误诊，如果病情危急，这可能是有害的)。我们还关心当机器预测与患者的真实状态匹配时出现的真阳性和真阴性(即，它们分别实际上是病例或实际上不是病例)。我们可以使用真阳性、真阴性、假阳性和假阴性来生成假阳性率/敏感性和特异性。

*   为了查看我们的逻辑回归模型在多种情况下的表现，我们可以生成一条“ROC 曲线”ROC(受试者-操作者特征)曲线来自于在多个阈值下找到真阳性率和假阳性率。然后，我们将这些数据点(x =假阳性率或灵敏度，y =真阳性率或 1-特异性)绘制在图表上，并将这些点连接起来，生成如下所示的曲线(ROC 曲线参见图 [4-1](#Fig1) ，灵敏度和特异性的定义参见下文)

*   其中点 A、B 和 C 是从不同阈值产生的敏感/特异性对。然后，我们可以计算曲线下面积(AUC ),这可以让我们更好地了解如何比较不同的分类器(AUC 值范围从 0 到 1，其中 1 =完美的预测值，0 =比随机差，AUC 通常越高越好)。

*   灵敏度也称为“召回”，可通过以下公式计算:

    ![$$ \mathrm{Sensitivity}=\frac{\#\mathrm{of}\ \mathrm{true}\ \mathrm{positives}}{\#\mathrm{of}\ \mathrm{positives}+\mathrm{false}\ \mathrm{negatives}} $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Eque.png)

*   特异性可以如下计算:

    ![$$ \mathrm{Specificity}=\frac{\#\mathrm{of}\ \mathrm{true}\ \mathrm{negatives}}{\#\mathrm{of}\ \mathrm{true}\ \mathrm{negatives}+\#\mathrm{of}\ \mathrm{false}\ \mathrm{positives}} $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equf.png)

*   )医学研究者通常在 ROC 曲线中产生最高灵敏度和特异性的阈值处分别报告灵敏度和特异性(除了 ROC 曲线之外)。

为了实际拟合逻辑回归方程，我们可以使用一种称为最大似然估计(MLE)的方法。MLE 以与普通最小二乘回归相似的方式运行(即，它试图最小化依赖于最小化它的“错误”程度的某个函数)；然而，它没有封闭形式的解决方案。相反，它必须通过尝试多个贝塔来试图找到适合该等式的最佳贝塔集，查看哪些贝塔最小化其“错误”程度，然后相应地调整这些贝塔以进一步最小化误差。

逻辑回归可能是医疗保健领域中最有用和最易解释的回归形式。虽然它不被认为是机器学习领域的时髦词汇，但它是一种经过尝试和测试的方法，用于处理涉及特定结果概率的预测。

### 套索、脊和弹性网回归，偏差-方差权衡

有时，我们会遇到这样的情况，我们的数据集中有太多的预测因素，尝试减少或最小化预测因素的数量可能会对我们有益。这样做的主要好处是通过将 100 多个独立变量提取为更易于管理的变量，如几十个，来帮助最终模型本身的可解释性。我们可以通过两种方法做到这一点，套索或岭回归(弹性网是两者的结合)。

在我们讨论这些算法之前，我们需要讨论机器学习中的偏差-方差权衡。当我们训练模型时，我们可以优先尝试确保我们的最佳拟合线接触我们训练数据中的所有数据点。虽然这将最小化我们的训练数据集中的所有错误，但是我们可以在我们的测试数据集上看到巨大的性能差异，因为测试数据不一定等同于我们的训练数据。在这种情况下，我们说一个模型具有很高的方差，因为它的结果根据用来评估其性能的数据而变化很大。我们还可以说，该模型具有较低的“偏差”,这意味着最佳拟合线不会对数据的底层结构做出任何假设，因为它只是试图在给定所有可用参数的情况下拟合所有数据。或者，我们可以制作另一条最佳拟合线，试图直接穿过这些点(但不触及所有这些点)。这种模型被认为有很大的偏差，因为它对数据的基本结构作出了假设(即它是线性的)；然而，它可能具有较低的方差，因为最佳拟合的线性线在拟合测试集和训练集方面做得相当不错。

在这两种极端情况下，我们的数据集中都有很高的误差。当方差很高而偏差很低时，我们会有很大程度的误差，因为我们的测试集不适合我们的模型(即，我们过度拟合了训练数据)。当偏差较高且方差较低时，我们也会有较高程度的误差，因为模型可能过于简单/做出了过多的假设(即，我们对训练数据进行了欠拟合)。我们的目标是试图找到一个“最佳点”，帮助我们在这些情况下最小化整体错误。一种方法是通过正则化方法，有选择地从我们的模型中删除变量(即，帮助最小化方差，同时略微增加偏差)，并在权衡中找到一个令人满意的中间点。套索、岭回归和弹性网都是正则化算法。

LASSO 的工作原理是在我们之前讨论过的残差平方和方程中加入一项。除了计算给定数据点的预测值和实际值之间的误差，LASSO 还添加了一个项，该项等于我们的β值的绝对值乘以我们自己设置的一个名为“lambda”的参数(该参数被称为**超参数**，因为它是我们设置的，而不是让计算机设置的)。这个额外的术语被称为“L1 规范”思考这意味着什么，我们可以看到，如果我们有大量的贝塔项，我们正在增加新的残差平方和公式。由于目标始终是最小化残差平方和(SSR)，因此该算法有选择地将β设置为等于 0。这样做可以最小化额外添加的套索术语。我们还可以通过将 lambda 设置为高值(即，去除 beta 更重要)或低值(即，这样做不太重要)，来调整去除 beta 在 LASSO 中有多重要。我们还可以在训练集中的验证集上尝试一些不同的 lambdas。

岭回归的操作类似于套索回归，只是它将β的平方和乘以λ添加到常规 SSR 公式中。这个额外的术语被称为“L2 规范”然而，岭回归不同于 LASSO，因为岭回归不会将某些β完全设置为 0(即，它不会完全消除它们)。相反，它保留了所有的功能，只是减少了不重要的测试版。

弹性回归是两者的折中。它将 L1 范数和 L2 范数添加到 SSR 方程中，并使用称为“α”的独立超参数来确定哪个范数的权重较大(随着α的增加，L1 范数/拉索更重要；随着α减小，L2 范数/岭更重要)。因此，我们在套索和岭回归之间找到了一个合适的中间点，有助于防止我们过度拟合数据。

在之前的研究中，弹性网络已被证明在帮助从队列研究中移除变量方面非常有用，队列研究使用每个数据点包含超过 1000 个特征的数据(即，非常“宽”的数据，因为有许多列/特征与单行数据相关联)。因此，我们可以在最终的模型中获得变量重要性的度量(因为不重要的变量要么被消除，要么被最小化到接近零值)。为了找到这些研究的 alpha 和 lambda 的最佳值，这些研究通常采用一种称为“网格搜索”的程序，这意味着他们在训练数据上尝试 alpha 和 lambda 值的每一种可能的组合(每个组合都限制在某个范围内),然后查看哪个在验证集上产生最佳结果。用给出最佳结果的α-λ参数组合训练的模型随后将在测试集上被评估。

*   在现实世界的使用中:2015 年，Eichstaedt 等人发表了他们关于 Twitter 如何预测县级心脏病死亡率的模型。在这篇论文中，他们基本上是从 Twitter 上下载数据，清洗数据，提取常用的单词和短语。他们在回归模型中使用这些单词和短语作为自变量，因变量是该推文所在县的动脉粥样硬化心脏病发病率。由于这个问题归结为一个简单的回归，他们能够利用我们谈到的正则化算法，特别是岭回归。应用该算法，他们还可以提取“可变重要性”，在这种情况下，这些词是回归公式中对疾病发病率影响最大的词。他们最终发现，在推特上包含愤怒/沮丧词汇/语调的县，心脏病发病率更高。重要的是，这个预测因子，当与县人口统计数据相结合时，是心脏病发病率的一个非常准确的预测因子。

虽然我们一直在谈论回归，但变量和结果之间的简单关系可能不是可以用简单的方程来建模的。相反，我们可能需要知道一些关于已经用于训练模型的实际数据点的信息，以找出新点的分类。这就是为什么我们要看看实例学习算法，它允许我们直接基于先前的数据点来捕捉关系。

## 实例学习

实例学习算法尝试通过将未知数据点的输出直接与用于训练网络的值进行比较来执行分类或回归。在回归分析中，我们看到了如何首先尝试拟合一个方程(线性方程或逻辑方程)，然后根据这些方程预测值。除了帮助找到最佳方程之外，数据的潜在点实际上并不用于回归技术。实例学习算法使用单独的训练数据点来确定测试点的类别或值。我们将探索完成这项任务的两种方法:k-最近邻和支持向量机(SVMs)。

### k-最近邻(以及以 ML 为单位的缩放)

k-最近邻是一种非参数算法(即，它不假设输出函数的形式)。与回归技术(对方程的最终形式做出假设)相比，非参数方法更擅长处理没有明确 x-y 关系的数据。这些方法的问题是，您无法找到减少找到有效预测所需的参数数量的方法。

k-最近邻算法的工作方式如下:(1)对于给定的测试点，找出与该测试点最近的 k 个点(其中 k =指定的点数)。这些是最近的 k 个邻居。(2)在分类任务的情况下，测试点的预测类将是 k 个最近邻居的大多数类(例如，如果四个最近邻居是“糖尿病”、“糖尿病”、“糖尿病”和“非糖尿病”，则测试点的类将是糖尿病)。在回归任务的情况下(这里，“回归”只是指输出一个连续的值，而不是不同的类)，我们简单地取 k 个最近邻的平均值(例如，如果测试点的四个最近邻的值分别为 50、60、70 和 80 kg，则测试点的输出值将是(50+60+70+80)/4 = 65 kg)。

还有一些额外的注意事项需要考虑:我们使用什么样的“k”值，以及我们使用什么类型的距离度量。

最佳“k”可以通过在训练验证集中尝试 k 的所有可能值来确定。一旦你找到一个使你的目标误差最小化的“k”(例如，分类的准确度)，你就可以评估你的函数。对于距离，我们可以使用欧几里得距离(类似于找到三角形的斜边)、曼哈顿距离(类似于获得城市网格上各点之间的“真实世界”距离)等等。其中一些距离可能更适合某些任务(例如，在处理高维数据时，曼哈顿距离是首选)，但您应该尝试几种距离，看看哪种距离能产生最佳结果。

另一个重要注意事项是，K-最近邻算法极易受缩放比例变化的影响。例如，如果数据的一个维度测量某人的身高(通常限制在 1 到 2 米之间)，而另一个维度测量他们的体重(10 到 100 公斤)，我们可能很难找到每个维度中最接近的点，因为他们的体重相差很大。我们也容易受到数据中异常值的影响。为了帮助解决这个问题，我们可以集中和扩展我们的数据。这样做基本上意味着将我们维度的值重新分配给一个 z 分数(即原始值-该维度中值的平均值/该维度中值的标准偏差)。在这种情况下，无论小数位数如何，大多数值都将介于-2 和 2 之间(如果正态分布)。

为了有助于可解释性，我们还可以尝试并输出在不同“k”级别的 k-最近邻分类的决策边界(如图 [4-2](#Fig2) 所示)。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig2_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig2_HTML.jpg)

图 4-2

k-最近邻示例。这里，我们可以看到修改 k 如何导致白色和灰色类点之间的不同决策边界

这里，图表中的每个像素都被着色为给定特定 k 的 k-最近邻输出的类。当 k 较小时，我们可以看到图表的白色和灰色区域之间的边界非常不规则，这是有意义的，因为使用较少的点来分类对象。当 k 较大时，我们看到决策边界的形状更加规则，因为一个类需要更多的 k-最近邻才能成为多数。较大的 k 值可以更好地理解点实际上是如何相互分离的，但是也可能会对一些点进行错误分类。然而，当 k 很小时，我们可能会学习到不那么有用的决策边界，并且可能只对训练数据起作用。

### 支持向量机

我将只简单地提到这个算法，因为当深入细节时，它往往会变得非常数学化。支持向量机算法基于这样的假设运行，即可能存在将两类数据分开的线、平面或超平面。目标是找到这种分离，使得最终平面和实际数据点之间的界限尽可能地高(即，找到可以完美分离两类数据的线，也称为“硬界限”)。然而，在某些情况下，我们可以通过允许一些数据点被错误分类来获得更大的余量(这种余量被称为“软余量”)。这样做，我们可以得到一条线，除了一些异常值之外，它仍然有很大的边距来分隔数据。我们可以在图 [4-3](#Fig3) 中看到一个硬边界和软边界分类器的比较示例。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig3_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig3_HTML.jpg)

图 4-3

硬边界和软边界分类器的 SVM 边界。“支持向量”表示为空心圆或正方形。违反硬边界假设的圆形或正方形用虚线边框标记。决策边界是带有虚线边界的黑色实线

这里，左边的图像代表一个硬边界分类器，因为没有一个圆或正方形点跨越线周围的“边界”边界。右边的图像表示一个软边距分类器，因为一些圆/正方形被允许出现在线周围的边距区域中，即使它们违反了先前的边距。

有时，我们必须对数据进行变换(比如求平方)，以找到将数据点相互分离的最佳直线、平面或超平面。我们可以对数据应用多种可能的变换，支持向量机可以帮助我们找到要应用的最佳变换，从而为我们提供一个能够最好地分离手头数据的平面。例如，在下面的情况下(图 [4-4](#Fig4) ，我们可以使用 SVM 算法来找到最好地分离这些数据的平面。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig4_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig4_HTML.jpg)

图 4-4

SVM 用了一个很难在二维空间中线性分离的例子

在左侧，很难找到一条线或多项式来适当地分隔这些数据的类别(其中点的颜色代表其类别)。但是，如果我们对数据应用一个变换(在这种情况下，一个称为径向基函数核的变换)，我们可以找到一个平面来为我们分隔这些数据。SVM 算法让我们有能力找到这个平面。SVM 也可以被实现用于回归任务。

*   在现实世界中的使用:Son 等人在 2010 年发表了支持向量机的使用，用于预测心力衰竭患者是否会坚持药物治疗。他们的输入数据点预测了性别、每日用药频率、用药知识、纽约心脏协会功能分类、射血分数、简易精神状态检查分数以及他们是否有配偶。他们的输出是心力衰竭患者是否在服药。他们能够实现接近 80%的检测准确率，这是令人印象深刻的，因为他们只有 76 个人的小数据集。自那以后，支持向量机已被用于许多医学预测应用，包括预测痴呆症、患者是否需要住院等等。类似地，k-最近邻已被用于基于先前的患者数据来确定个体是否有患心脏病的风险。

## 决策树和基于树的集成算法

决策树有助于产生 ML 世界中一些最易解释的结果。决策树不太像真正的树。相反，它们的结构就像一棵倒置的树，顶部有根，叶子和树枝的数量随着你的深入而增加，如图 [4-5](#Fig5) 所示。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig5_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig5_HTML.jpg)

图 4-5

预测乙型和丁型肝炎感染/恢复/未知状态的决策树结构

在顶部，有一个根(正式称为节点),它有左右两个分支。每个分支也有一个节点(它也有自己的左右分支等等)。这些分支中的一些不会进一步分裂成其他左/右分支(这些被称为叶)。在决策树生成算法中，将为树的每个节点学习决策规则。该节点可以是类似于“如果患者乙肝表面抗原滴度测试阳性，则转到左分支；否则，去正确的分支。”这些分支也有自己的决策节点，直到它们到达一个叶节点，该叶节点通常给出患者的分类(例如，他们有疾病或他们没有疾病)或给出一些数字，该数字代表到达决策树该部分的训练数据的其他实例的平均标记值。

需要学习的决策树的关键部分是“分割”什么特征(即，在每个节点测试)以及是否值得分割该特征。

### 分类和回归树

分类和回归树(CART)是一种机器学习算法，允许我们学习决策树。该算法通过反复尝试要分割的特征来工作。无论哪种分割产生最佳结果，都被选择应用于数据(根据该规则在树中创建一个带有分支的新节点)。然后，该算法试图为每个分支找到新的分裂。然而，这种算法不一定是最好的，因为它只依赖于在那个时间点选择最佳分割，而不是尝试多种不同的树来查看一旦整个算法运行时什么是最好的。这种类型的算法被称为“贪婪”算法(在这种情况下，该算法是贪婪的，因为它决定了在训练过程中的单个点上看到的最佳分割)。

但是我们如何评价哪种拆分是“最好的”呢？对于分类任务，我们可以使用一个称为“基尼杂质”的术语可以通过将一个类中该节点的训练点比例乘以其补数(1-该比例)来计算每个节点的基尼系数。我们对所有类别的这些值求和，并对每个节点的值进行加权，以确定哪个分裂特征将导致最低的可能基尼不纯系数(0 =分配给每个分支的所有实例都属于同一类别，这意味着我们有一个完美的分类器；任何更高的值意味着在每个节点都有被错误分类的实例)。在一个等式中，基尼系数可以表示为:

![$$ G=\sum \limits_{i=1}^Cp(i)\ast \left(1-p(i)\right) $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equg.png)

其中 *G* 为基尼不纯系数， *C* 表示要分割的等级， *p* ( *i* )表示给定等级 *i* 中的点数比例。

然而，如果我们只是找到可能的最佳树，如果我们不惩罚它的增长，我们可能会得到一个非常复杂的树，有数百个节点和分支。毕竟，我们试图找到一棵可以解释的树。我们可以建立一个“停止标准”，如果在一个特定的分支上没有足够的元素通过分裂生成，它就对树的增长进行限制(例如，如果一个分裂向左推动一个数据点，向右推动三个数据点，但是我们的停止条件规定我们必须在一个节点上至少有五个元素，我们不会根据该标准进行分裂)。我们还可以通过设置一个名为“Cp”(复杂性的缩写)的超参数来“修剪”这棵树。这在基尼系数中增加了一项，惩罚在特定节点下创建的较小的树(子树)的数量(子树的数量越多，对树的生长的惩罚越大)。这两种方法都有助于确保我们不会创建过于复杂且无法在实际决策中使用的树。

您还应该知道决策树算法有多种版本。CART 依靠基尼杂质分数寻找最佳树；ID3、C4.5 和 C5.0 等其他标准依赖于另一种称为“信息增益”的衡量标准理解它如何工作的确切细节并不重要，但知道有其他决策树算法可以在您的数据集上试用是很有用的。

### 基于树的集成方法:Bagging、Random Forest 和 XGBoost

决策树世界中的集成方法通过创建多棵树并允许每棵树对特定结果进行“投票”来帮助优化预测。这些算法中最简单的一种被称为自举聚合(也称为“bagging”)。装袋包括创建多个类似于 CART 算法的树；但是，它会引导训练数据集，这意味着它会随机选择训练集的子集来创建树。它通过替换进行采样，这意味着一个训练样本可以出现多次。然而，由于所有的树都是在训练数据的子集上训练的，所以它对于一般的数据来说更健壮，因为它不容易过度拟合。在评估阶段，所有这些树都根据每个树的分支和节点，对正确的分类进行“投票”。多数预测获胜。

随机森林是另一种建立在 bagging 基础上的机器学习算法。random forest 并不只是选择带有替换的训练集样本并构建多个树，它还会选择随机的特征子集来对每个树中的每个节点进行分割。例如，如果您的数据集中的每个患者有 100 个您跟踪的特征，随机森林将创建许多树，这些树将只使用 100 个特征的某个子集在每个节点上进行分割(例如，20、30、42、…)。随机选择特征的数量可以通过一个称为“k”或“mtry”的超参数来改变通常，mtry 被设置为特征总数的 1/3(在我们的示例中是 33)，但是您应该为 mtry 尝试一些值。你也可以设置随机森林中生成的树的数量(通常使用更多的树更好；然而，在你需要微调的树的高值之后，回报减少了)。一些机器学习库还让您有机会指定这些树可以有多“深”(这基本上限制了一棵树可以分支的次数)，因为太深(并且有许多分支)的树会使训练数据过拟合。

XGBoost 是另一种集成算法，它产生与随机森林相似的输出；然而，它通过一种称为梯度推进的方法来构建这些树。“随机森林”独立地构建自己的“森林”,而“梯度增强”构建一棵树接一棵树，调整每棵树对最终决策的影响。它通过基于“学习率”超参数(基本上决定了权重在一次训练迭代中可以改变多少)来调整分配给每棵树的权重。如果学习率太高，我们可能永远也找不到最优解，或者可能偶然发现一个只对我们的训练数据有效的稍微最优的解。如果学习率太低，我们可能要花很长时间才能找到最优解。大多数库会建议 XGBoost 算法使用的值，或者自动提供给你。XGBoost 生成的树可以比随机森林好得多；然而，与随机森林相比，它们通常需要一段时间来训练。

*   在现实世界中的使用:Chang 等人在 2019 年表明，C4.5 决策树和 XGBoost 用于预测高血压患者的临床结果。他们使用的预测指标是体检指标(性别、年龄、身体质量指数、脉率、左臂收缩压、甲状腺功能[FT3]、呼吸睡眠测试 O2、收缩压[检查时和夜间]以及高血压药物的数量)，输出是患者是否患有心肌梗死、中风或其他危及生命的事件。他们最终发现，与正常的基于决策树的算法(C4.5 树实现了 86.30%的准确度)相比，XGBoost 实现了最佳的准确度(94.36%)和 AUC (0.927)。这篇论文强调了集成算法如何提供比树算法更大的优势。

## 聚类/降维

这类算法通常被认为是无监督学习算法。因此，使用这些算法，我们没有真正的准确性或误差的衡量标准；然而，我们仍然可以大致了解他们的表现。无监督算法对于聚类和降维非常有用。

聚类算法通常用于查找哪些数据点彼此密切相关，从而形成“聚类”这有助于确定疾病爆发的位置，也有助于确定在大型数据集中有多少组患者具有共同的特征。

降维正如其名字所暗示的那样。这些算法帮助我们减少了我们在绘制和解释高维数据时所考虑的不同因素的数量。这些算法通常用于群体遗传学研究，其中个体具有 1000 个“维度”(即感兴趣的遗传位置)，并且需要以某种方式分离以解释亚组中的差异。

### k 均值聚类

k-Means 聚类的工作原理是试图找到彼此密切相关的数据点的聚类。在高层次上，该算法首先随机选择“k”个数据点作为每个聚类的中心。然后，根据最接近的聚类，将其他数据点分配给“k”个聚类之一。一旦将点分配给某个聚类，该聚类的中心就必须进行更新，以考虑所有已添加的新点(该中心可能不会与现有的点重叠)。将点分配给聚类并更新中心位置的过程持续进行，直到算法达到“收敛”(即，中心不会继续显著变化，并且分配给特定聚类的点不会继续变化)。该过程如图 [4-6](#Fig6) 所示。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig6_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig6_HTML.jpg)

图 4-6

k-Means 算法步骤。黑色十字表示该步骤的中心。浅灰色十字表示前面步骤的中心(注意十字是如何穿过步骤的)。请注意，灰色矩形内的步骤会重复进行，直到中心达到收敛

显然，k-Means 有一个您必须调整的主要超参数:您想要在数据集中找到的聚类数(即中心数)k。有时，如果您事先了解数据集，您可能知道您想要什么“k”(例如，如果您知道数据集中有糖尿病患者和非糖尿病患者，一个好的 k 是 2)。其他时候，您不知道最佳的“k ”,只想为您的数据找到可能的最佳聚类(此时，您可以探索每个聚类中的点的特征，以了解聚类之间的区别)。

使用肘方法可以在没有先验知识的情况下找到最佳 k。它的工作原理是，我们应该尝试找到紧密聚集在一起的集群，也就是集群内误差平方和(WSS)。我们可以通过确定每个点到它所属的星团中心的距离来找到 WSS；我们想找到得到最低 WSS 的 k。然而，在一定数量的 k 之后，WSS 通常不会显著降低。在一个极端，我们可以设置 k 等于我们数据集中的点数；然而，这可能是没有价值的，因为小 k 可以给我们机会得到一个多样化的聚类进行分析。我们将收益递减的点定义为“肘部”，可以通过绘制 WSS 与产生 WSS 的 k 值来识别。在图 [4-7](#Fig7) 中可以看到一个弯管图的例子。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig7_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig7_HTML.jpg)

图 4-7

弯头图示例，弯头在 k = 3 左右

在这里，我们可以看到，在 k = 3 个集群附近，图中有一个“肘形”(即，在增加一个额外的集群时，WSS 损失急剧减少，也称为收益递减)。还有其他方法，例如轮廓法，该方法考虑了一个点与其自己的聚类有多相似以及它与其他聚类有多相似(在该方法中，为每个尝试的 k 产生一个轮廓分数，并且最高的轮廓分数被认为是聚类的最佳数量)。另一种称为间隙统计的测量方法也有助于确定最佳聚类数，方法是使用从零参考分布(通过自举生成)确定的期望值计算聚类内变化。产生最高值间隙统计的 k 是具有最佳聚类数的 k。

k-means 聚类算法的变体也可以产生分层聚类，其中我们产生一个树状图(一个紧密相关的点在树上彼此更靠近的树；参见图 [4-8](#Fig8) 。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig8_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig8_HTML.jpg)

图 4-8

描述物种间关联性的树状图示例

这些树通常用于测量物种之间的进化关系，甚至可以模拟我们肠道中微生物群之间的关系(基于微生物群样本中发现的物种之间的 DNA 相似性)。

需要注意的是，k-means 聚类对规模很敏感，这意味着我们在使用这些方法时需要重新调整和集中我们的数据(类似于 k-nearest neighbors)。

### 主成分分析

主成分分析(PCA)可以帮助我们可视化高维数据。这是通过识别可以产生新轴的预测因子组合来实现的。然后，我们可以使用这些新的轴来重新绘制数据，以帮助显示数据本身的变化程度。PCA 的最终产品是多维组合的一组轴(称为主成分)。我们可以使用这些轴中的一些(通常是前两个)来绘制我们现有的数据，但是是在一个新的坐标系中。在某些情况下，与仅在一对轴上可视化数据相比，这可以帮助看起来没有差异的数据看起来更加不同/可分。在图 [4-9](#Fig9) 中可以看到一个 PCA 转换的例子。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig9_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig9_HTML.jpg)

图 4-9

PCA 转换步骤。首先，绘制数据，然后找到 PCA 轴，然后将数据转换到 PCA 空间，在新的轴上重新绘制数据

粗略地说，PCA 旨在找到主成分轴，当投影到该轴上时，使数据中的方差最大化。要了解这意味着什么，请参考图 [4-10](#Fig10) 。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig10_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig10_HTML.jpg)

图 4-10

此处显示了 PC1 和 PC2 的差异。请注意 PC1 的方差比 PC2 高

在这张图片中，我们可以看到黑线将成为很好的轴，因为它们可以让我们清楚地看到点之间的差异。这些轴是 x 轴描述的任何特征和 y 轴描述的一点特征的组合。然后，我们继续寻找其他轴，一旦先前的轴应用于数据，这些轴可以帮助将点彼此分开。最终，我们得到了主成分(有很多)的排序，然后是它们可以解释的变异百分比。我们选择使用尽可能多的主成分轴来解释很大程度的变化，直到出现收益递减点(这可以通过“scree plot”来评估，该图看起来类似于 k-means 聚类中的肘方法生成的图)。

## 人工神经网络和深度学习

人工神经网络(ann)和深度学习(DL)被认为是目前 ML 世界的宠儿。这些算法有助于从输入数据中提取信息，以生成输入数据的“中间”表示。该中间表示通常是输入的某种变换，可以用来容易地“学习”该变换输入的哪些方面是输出的最佳预测。我们看到了转换输入如何在让支持向量机如此好地分类数据方面发挥作用，但这些转换将 SVM 带到了另一个完全不同的水平，定期学习人类通常无法理解的转换。

当我们谈论神经网络时，最明显的词是“neural ”,表示与神经元的某种关系。几乎每一篇介绍性的文章都展示了神经网络是如何部分受到人类神经元及其行为的启发；然而，这种比较开始与今天的神经网络大相径庭。无论如何，我们将涵盖这种比较，因为它有助于激发一个简单的神经网络的基本结构。

### 基础(感知器，多层感知器)

神经网络由称为神经元的单个单元组成。每个神经元可以具有多个输入，并且具有可以作为输入馈入其他神经元的输出(正如生物神经元具有分别充当多个输入和多个输出的树突和轴突末梢；参见图 [4-11](#Fig11) 。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig11_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig11_HTML.jpg)

图 4-11

生物神经元的结构

为了让生物神经元激发动作电位，膜电位需要超过特定的阈值。类似地，人工神经网络可以被设计来模仿这种行为(尽管这并不一定适用于所有的神经网络)。

抛开人工神经网络背后的生物学思维，人工神经网络的实际实现包括两个主要的构造:权重和偏差。对于人工神经网络的每个输入，权重将输入乘以某个数字。偏差是一个加到所有加权输入总和上的数字(正数或负数)。通常将某个函数(也称为“激活函数”)应用于加权输入和偏差的最终和:有时该函数只是恒等式(即和本身)，有时它是 sigmoid 函数(将最终值限制在 0 和 1 之间)，有时它可能是另一个函数，仅当和为正时，该函数才等于权重和偏差的最终和；否则，它为零(这称为 ReLU 函数)。一个人工神经元的图片可以在图 [4-12](#Fig12) 中找到。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig12_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig12_HTML.jpg)

图 4-12

具有输入 x1 和 x2 以及偏置项的人工神经元(又名“感知器”)。注意，w1 和 w2 可以采用不同的值，如线条粗细的差异所示

在这个图像中，我们可以看到这个人工神经网络单元(正式称为“感知器”)有两个输入，X1 和 X2，以及一个输出。它还有一个偏差，记为“b”，每个输入的权重称为“w1”和“w2”我们还看到，函数“f”(激活函数)应用于权重乘以输入加上偏差之和，以产生最终输出。

许多感知器可以排列在一起形成一层神经网络。这些层中的许多层可以链接在一起，形成一种称为多层感知器的人工神经网络，也称为 MLP(如图 [4-13](#Fig13) )。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig13_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig13_HTML.jpg)

图 4-13

具有三个输入、三个全连接层和一个输出的全多层感知器

MLP 通常具有被视为输入图层的第一个图层。这一层的每个感知器采用表格数据(例如，电子表格中的行)中的数据点的任一特征的值，或者甚至可以表示图像中像素的值(每个像素一个感知器)。然后，这个输入层被密集地连接到下一层感知器(即，每个感知器连接到下一层中的所有其他感知器一次)。然后，这些感知器可以被输入到另一层，以此类推。最后，倒数第二层通常被送入单个或多个输出感知器。如果我们只是试图在给定一些输入值的情况下预测一个数字，那么单个感知器输出将是有用的。如果我们想要预测输入数据的类别(例如，无糖尿病、糖尿病前期、糖尿病)，多个感知器输出将是有用的。在分类任务中，我们将找到产生最高值的输出感知器，或者甚至可以得到类似于属于由每个输出感知器指定的类别的输入的概率的东西(通过应用称为“softmax”的函数)。

你可能已经注意到我们在这个例子中包含了相当多的感知器，但是在我们的网络中包含这么多感知器有什么好处呢？考虑每个神经元可以接受多个输入，并将这些输入转换成完全不同的数字。这些数字中的每一个都可以输入到其他神经元中，这些神经元输出不同的数字，以此类推。在 MLP 的每一层中，我们将网络的输入完全转换成别的东西。这些表示中的一些可以使这些网络更容易找到最适合分类或回归任务的规则。

到目前为止，我们还没有触及的一个话题是，网络实际上是如何为这些连接和感知机中的每一个学习正确的权重和偏差的。人工神经网络通过一个叫做“反向传播”的过程做到这一点本质上，一个训练数据点(或多个训练数据点，也称为“批”)通过网络发送，并记录一组输出。对于每个输出，网络根据我们自己指定的称为“损失函数”的函数来计算它的“错误”程度(例如，对于回归，该错误可能只是输出数与地面真实数的平方之差)。网络在这一点上的目标是尝试并找到调整网络中存在的每个权重和偏差的最佳方式，以最小化损失。反向传播如何工作的证据可以在网上很多地方找到，但它确实涉及到相当多的多变量微积分。一旦网络调整了它的权重和偏差，它就通过新的训练数据(或者甚至可能再次通过训练数据)来继续更新权重和偏差。

重要的是，你可能会看到提到一种叫做“学习率”的东西，它对于确定神经网络能够多快收敛到使其误差最小化的最优解是很重要的。这是通过在看到一些训练数据后乘以每个权重和偏差的调整量来实现的。在大多数情况下，我们希望学习率相对较低，因为我们不想超过最优解，永远达不到它。但是，如果设置得太低，可能会导致网络需要很长时间才能收敛到最终解决方案。我们自己设定学习率，所以它是一个超参数。其他技术(如“批量标准化”,集中/缩放网络中每个感知器的输出)也可以帮助稳定神经网络的学习过程。

您可能需要调整的另一个主要超参数是“时期”的数量一个时期是一个超参数，它定义了人工神经网络/MLP 模型遍历整个数据集的次数。训练一个网络所需的历元数量没有规则，它完全取决于网络架构。一些简单的网络可能只需要十几个纪元来训练。更复杂的网络可能需要数十万个。为了帮助确定要设置的最佳时期，最好对神经网络的训练设置“停止条件”(例如，如果在十个时期后网络的总误差(也称为“损失”)没有减少 10%，则停止网络的训练)。由于神经网络可能会过度拟合训练数据，因此设置此停止条件非常重要。为了防止过度拟合，可以使用“正则化”技术(通常涉及类似于“放弃”的东西，也就是从网络中随机删除感知机，或者惩罚大权重的 L1/L2 范数正则化，类似于我们在套索/岭回归中看到的)。

您还可以设置其他超参数，如批量大小(网络在更新权重之前看到的训练数据点的数量)、优化器选择(除梯度下降之外还有其他算法确定网络如何学习)、权重初始化(确定网络在任何训练之前的第一个权重和偏差；通常这是随机的)、损失函数(你试图优化的)、层大小(网络的每层有多少个感知器)等等。

一般来说，至少在某些方面，MLP 是许多流行的神经网络架构的组件。大多数用于图像分类的复杂网络都有最后的两到三层，它们只是密集连接的感知机，有助于分类或回归任务。MLP 本身也可以用于预测表格数据。然而，MLPs 的规模变得难以处理，尤其是当使用图像作为输入时。考虑到如果一幅图像的大小为 400px x 400px，并用作网络的输入，这意味着有 160，000 个输入感知器进入该网络，因为每个感知器代表一个像素。此外，这些感知器中的每一个都需要连接到 MLP 中间层中的更多感知器，这表示要学习更多的权重和偏差。之后，使用这种架构处理图像在计算上变得非常困难。这就是我们转向卷积神经网络的地方。

### 卷积神经网络

为了帮助解决训练神经网络处理相当大的图像输入的问题，研究人员提出了对输入图像本身应用“卷积”的想法。卷积是一种线性运算，它将网络中的部分输入图像相乘并生成输出值。决定乘法如何发生的结构是过滤器(也称为内核)。将输入图像乘以过滤器将产生一个较小的图像。在多个卷积(即，多次运行滤波器乘以图像输入并产生输出图像)上这样做可以极大地减小输入的大小，甚至可以产生网络随后可以学习的图像的中间表示(称为“特征图”)。

但是到底什么是过滤器呢？过滤器是排列成矩阵(即网格)的一组数字。通常这些过滤器相当小(3x3，5x5)。包含在这些过滤器本身中的数字也由神经网络学习。为了产生特征图，过滤器以滑动方式(从左到右，然后从上到下)将输入图像的每个 3x3、5x5(或过滤器的任何大小)部分中的像素值相乘。该乘法运算然后产生另一个图像，该图像可能比原始输入图像小，但是看起来肯定与输入图像大不相同。在图 [4-14](#Fig14) 中可以找到卷积的示例。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig14_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig14_HTML.jpg)

图 4-14

图像上的卷积运算。这里，滤波器值在图像中的 3×3 补片上相乘。通过对该乘法的值求和来计算单个值。该值输出到输出图像，滤镜继续在原始图像上滑动

在卷积神经网络中，每一层都由一组应用于图像输入的过滤器(您可以指定数量)组成。假定输入到神经网络的大多数图像都是彩色图像(有一个红色通道、一个绿色通道和一个蓝色通道)，则滤镜的大小为 WxHx3 元素(其中 W =滤镜的宽度，H =滤镜的高度)。还有描述如何将这些内核应用于图像的操作的附加参数。其中一个参数称为填充，它有助于我们处理图像边缘发生的问题(如果我们一直向右移动，在图像的右侧大小，过滤器将继续移动不存在的像素，并超出图像边界)。我们可以通过在图像中添加填充(通常是值为 0 的像素)来解决这个问题。我们还可以设置过滤器的步幅，它决定了过滤器在每个方向上移动时“跳过”多少像素。如果我们保持步长为 1，输入和输出图像将具有相同的大小。如果我们增加步幅，我们告诉过滤器在每一步移动两个像素，而不是一个像素，从而减少输入的大小。

当输入图像通过网络时，还有其他方法可以帮助减小输入图像的大小。一种方法叫做“最大池化”这是一个通过传递滤镜(通常是大小为 2x2 的东西)和 stride(通常是 1 或 2)来操作的滤镜，并且只输出一个值(滤镜所在的图像块中的最大值像素)。在图 [4-15](#Fig15) 中可以找到 max pooling 对图像所做的示例。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig15_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig15_HTML.jpg)

图 4-15

最大池 2x2 操作。请注意，在每个 2x2 区域中，只有最大值保留在输出区域中

我们可以通过与处理在每个步骤设置和管理所有滤波器的所有工作的库一起工作，来指定我们希望我们的网络具有的卷积层的排列和集合。通常，这些库只需要指定我们在每一步需要的过滤器数量、内核大小、输入图像形状、批量大小(在更新权重之前在一次迭代中使用的图像数量)以及填充/步幅。在这些神经网络的末端，我们通常会通过创建一个与最后一个卷积层输出中的像素数量相对应的感知器层来“展平”最后一个卷积层，并创建与该展平层紧密连接的附加层。最后，我们将有几个输出神经元，对应于我们要在网络中预测的每个类别。

卷积神经网络被认为引领了对人工智能技术的新兴趣。一个名为 AlexNet 的特定卷积网络能够在一个名为 ImageNet 的标准化图像分类基准测试中实现高度的准确性。谷歌跟进了他们的初始网络架构，并向公众发布了这些网络的训练模型(考虑到这些网络需要大量计算能力来实现其准确性，这是特别慷慨的)。对于医学成像应用来说重要的是，这些卷积网络可以被重新训练以进行其他形式的图像分类。这个过程(称为迁移学习)通常在网络的初始部分冻结学习的滤波器值，并且只重新训练最后几层(密集连接的层)。这种转移学习范式允许个人非常快速地(甚至在他们的笔记本电脑上)重新训练网络，以完成新的图像分类或回归任务，而这些任务不是最初训练的。重要的是，迁移学习突出了应用于图像的卷积如何生成对分类任务的变化具有弹性的中间表示(即，网络真正学习从图像中提取可用于学习任何东西的显著特征)。

多年来，卷积神经网络增加了越来越多的层(我应该提到“深度学习”是指用于学习任务的网络有许多层/卷积运算)。这种趋势导致了网络的发展，这些网络非常大，但在标准化基准的准确性方面只有微小的改进。还有其他被称为“变压器”的网络架构，它们处于最先进的图像分类技术的前沿，性能更好(但通常深度卷积网络足以完成大多数医学成像任务)。

### 其他网络(RCNNs、LSTMs/RNNs、GANs)和任务(图像分割、关键点检测、图像生成)

对于不同的任务，还有许多其他的网络体系结构。其中一些可能对医学研究有用；然而，其中相当一部分被限制在人工智能的技术领域，还没有在医学界得到广泛采用。

递归神经网络(RNNs)和长短期记忆网络是可用于处理数据流或基于时间的数据的网络。例如，在给定一组病历的情况下，试图预测急诊室入院可能性的任务可以通过使用 RNN 或 LSTM 来解决。LSTMs 被认为比 RNNs 更先进，因为它们解决了探索和消失梯度的问题(这些问题导致训练过程变得非常长或不可能)。这些网络已经开始进入医学研究应用的自然语言处理(即解释文本)领域。

RCNNs(或基于区域的卷积神经网络)用于医学成像中的对象检测任务。这些网络可用于在图像中实际寻找对象实例的过程(例如肺结节、骨折等。).它们还可以用于自动化医学分割任务(即，描绘各种结构/病理)，甚至用于界标检测(即，为每个检测到的对象找到相关的解剖关键点，例如用于脊柱侧凸检测的椎体的角点)。

gan 用于根据它之前看到的图像对生成图像(或其他数据)。这个网络由两部分组成:一个生成器，负责在给定一些输入的情况下生成图像建议；一个鉴别器，负责尝试找出这些新图像生成方式中的错误。发生器和鉴别器试图击败对方，因为发生器试图“愚弄”鉴别器，使其相信生成的图像等同于地面真相。GANs 可用于各种医疗应用，例如 CT 成像的去噪、CT 到 MRI 的转换(反之亦然)，甚至分割。

还有其他几个网络用于医疗任务(如 UNet 和 Siamese 网络)；然而，它们都基于相同的基本概念运行，或者旨在完成与前面章节所述相同的任务(UNet 最适合于分割，而 siame 网络适合于图像分类)。

至此，我们已经完成了 ML 算法和神经网络之旅。在我们实际编写这些网络代码之前，让我们花点时间讨论一下如何评估这些网络的基础知识，以及确保我们报告的结果有效的关键步骤。

## 其他主题

### 评估指标

为了提供可解释的网络结果，我们必须报告某些评估指标，以确保我们研究的读者知道网络实际上按预期工作。

就回归度量而言，报告评估度量的最常见方式是报告平均绝对误差(它只是预测数和实际数之间的差值的绝对值，是所有测试实例的平均值)。但是，一些医学杂志希望您提供一个均方误差(MSE ),它会惩罚离实际值较远的预测，而不会惩罚非常接近的预测，因为我们对预测值和实际值之间的差进行平方(这是“均方根误差”的一个变体，它只取均方误差的平方根)。

在分类度量上，最简单的就是准确度；然而，这并不代表全部情况。通常，ML 算法在每次预测时都会输出某事物在特定类别中的概率。如果你有一个概率，你可以生成一个 ROC 曲线(如前所述)并报告曲线下的面积(AUC)。您还可以报告精确度(真阳性除以真阳性+假阳性)或 F1 分数(

![$$ F1=2\ast \frac{\mathrm{precision}\ast \mathrm{recall}}{\ \mathrm{precision}+\mathrm{recall}} $$](img/502243_1_En_4_Chapter/502243_1_En_4_Chapter_TeX_Equh.png)

).

您可能还需要提供一条校准曲线，以显示算法在何处高估或低估了某个特定类别的可能性。这些曲线是通过将 x 轴分割成固定数量的条块来制作的，这些条块表示某个事物属于某个特定类别的可能预测概率(这来自于您的预测算法)。y 轴上的值对应于该类被正确预测的次数。例如，如果我们有 100 个测试数据点，我们可以将它们在 x 轴上的预测概率分成 5 段(预测概率< 0.20, 0.20–0.39, 0.40–0.59, 0.60–0.79, 0.80–1). For each of the test points that produce a probability that falls into one of the bins, we also count the frequency of the true class of that test point being present. Specifically, in the first bin, we would expect a 20% of points to have the true class (since the algorithm predicted these points would have a probability of 0.20). For the next bin, it would be 40% of points, etc. We plot the proportion of true instances of a class in a given bucket on the y axis. In the case that the model is predicting probabilities that are too large, we would see a value to the far right on the x axis but with a low y value (since, in reality, there weren’t that many classes). Conversely, if the model is predicting probabilities that are too small, we would see a value to the left-hand side of the x axis, but with a high y value. An example of calibration curves can be found in Figure [4-16](#Fig16) )。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig16_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig16_HTML.jpg)

图 4-16

校准曲线。浅虚线是一个完美校准的预测器。粗实线曲线没有被很好地校准，粗虚线是相对较好地校准的曲线

在此图中，实线表示低值概率箱的预测概率过低，而高值概率箱的预测概率过高(我们根据 y=x 线进行判断)。您可以使用不同的方法(如“普拉特法”对未校准的概率进行逻辑回归拟合)来校准这些未校准的值(最终产生“拥抱”对角线的粗虚线；这里，浅虚线是完美校准的预测器)。

总的来说，你报告的统计数据高度依赖于你寻求完成的最终任务。在分类任务中，准确性可能足以进行报告，但医学期刊也希望看到 ROC/AUC 来确认您的算法的弹性。有时，您肯定希望看到敏感度和特异性，以了解算法如何处理真阳性、假阳性、真阴性和假阴性情况。最好是退后一步，弄清楚使用你的模型的人在使用它之前需要知道什么。幸运的是，机器学习库产生了大量可供使用的统计数据，甚至可以为您生成一些图表。

### k 倍交叉验证

在前一章中，我们已经讨论了将训练集划分为训练集和验证集。然而，在这种情况下，我们仍然可能在算法的性能方面欺骗自己，特别是如果我们在同一个训练集上不断尝试多种算法。据我们所知，与现实生活中的数据相比，我们最初划分的训练集和验证集可能相对“更容易”学习。

为了解决这个潜在的问题，我们可以训练我们的算法并进行多次验证，但是是在不同的数据子集上。我们可以这样做:(1)将数据分割成“k”个段(称为折叠)，其中 k 是你输入的一个数字(通常 5 或 10 就足够了)。(2)挑选 k-1 个折叠的数据用作训练数据。(3)训练你的算法。(4)使用剩余的折叠作为您的验证数据。(5)跟踪评估指标。(6)继续执行步骤 2 至 5，在步骤 2 中选择一组不同的褶皱用于训练。

图 [4-17](#Fig17) 说明了在 k 折交叉验证的每次迭代中，通常如何选取训练和验证折叠。

![img/502243_1_En_4_Chapter/502243_1_En_4_Fig17_HTML.jpg](img/502243_1_En_4_Chapter/502243_1_En_4_Fig17_HTML.jpg)

图 4-17

此图描述了 5 重交叉验证。数据集的 20%用于测试。剩余的 80%训练数据被分成五个折叠，并且每个折叠用作至少一次验证折叠

最后，您可以使用您感兴趣的任何评估指标的平均值来比较各种算法或超参数选择。一些库甚至允许你用交叉验证进行网格搜索(例如，尝试许多可能的超参数组合)来提供一组严格的可能性。

## 后续步骤

在本节中，我们讨论了许多不同的机器学习算法，并了解了哪些算法适用于哪些类型的数据/任务。有监督的算法可能是医学文献中使用最多的，因为我们经常试图确定算法是否与医生的诊断结果相匹配，但无监督的算法在医学界也有一席之地，特别是在遗传学研究中。然后，我们探索了基于感知器的算法(神经网络)，了解了什么是真正的深度神经网络(只是一个由许多层感知器相互连接的网络)，并涵盖了一些流行的神经网络架构类别(重点是卷积神经网络)。

在接下来的章节中，我们将看到如何在数据集上实际使用这些算法。