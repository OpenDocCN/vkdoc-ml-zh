# 6.梯度推进机

到目前为止，我们已经考虑了决策树和随机森林算法。我们看到随机森林是一种 bagging (bootstrap aggregating)算法，它结合了多个决策树的输出来进行预测。通常，在 bagging 算法中，并行生长树以获得所有树的平均预测，其中每棵树都建立在原始数据的样本上。

另一方面，梯度增强使用不同的格式进行预测。boosting 采用顺序方法来获得预测，而不是并行化树构建过程。在梯度提升中，每个决策树预测前一个决策树的误差，从而提升(改善)误差(梯度)。

在本章中，您将学习以下内容:

*   梯度增强的工作细节
*   梯度增强与随机森林有何不同
*   AdaBoost 的工作细节
*   各种超参数对增压的影响
*   如何在 R 和 Python 中实现梯度提升

## 梯度推进机

梯度是指建立模型后获得的误差或残差。助推指的是提高。这项技术被称为梯度推进机，简称 GBM。梯度增强是一种逐渐改善(减少)误差的方法。

为了了解 GBM 是如何工作的，让我们从一个简单的例子开始。假设给你一个 M 模型(基于决策树)来改进。假设当前模型准确率为 80%。我们希望在这方面有所改进。

我们将模型表述如下:

```py
Y = M(x) + error

```

y 是因变量，M(x)是使用 x 个自变量的决策树。

现在我们将预测上一个决策树的错误:

```py
error = G(x) + error2

```

G(x)是另一个决策树，它试图使用 x 个独立变量来预测误差。

在下一步中，与上一步类似，我们构建一个模型，尝试使用 x 个独立变量来预测`error2`:

```py
error2 = H(x) + error3

```

现在我们将所有这些结合在一起:

```py
Y = M(x) + G(x) + H(x) + error3

```

前面的等式可能具有大于 80%的准确度，因为单独的模型 M(单个决策树)具有 80%的准确度，而在上面的等式中，我们考虑 3 个决策树。

下一节将探讨 GBM 如何工作的细节。在后面的部分中，我们将了解 AdaBoost(自适应增强)算法是如何工作的。

## GBM 的工作细节

以下是梯度增强的算法:

1.  用简单的决策树初始化预测。
2.  计算残差，即(实际预测)值。
3.  构建另一个基于所有独立值预测残差的浅层决策树。
4.  用新预测乘以学习率来更新原始预测。
5.  重复第 2 步到第 4 步一定次数的迭代(迭代次数就是树的数量)。

实现上述算法的代码如下(github 中的代码和数据集为“GBM working details.ipynb”):

```py
import pandas as pd
# importing dataset
data=pd.read_csv('F:/course/Logistic regression/credit_training.csv')
# removing irrelevant variables
data=data.drop(['Unnamed: 0'],axis=1)
# replacing null values
data['MonthlyIncome']=data['MonthlyIncome'].fillna(value=data['MonthlyIncome'].median())
data['NumberOfDependents']=data['NumberOfDependents'].fillna(value=data['NumberOfDependents'].median())
from sklearn.model_selection import train_test_split
# creating independent variables
X = data.drop('SeriousDlqin2yrs',axis=1)
# creating dependent variables
y = data['SeriousDlqin2yrs']
# creating train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

```

在前面的代码中，我们将数据集分为 70%的训练数据集和 30%的测试数据集。

```py
# Build a decision tree
from sklearn.tree import DecisionTreeClassifier
depth_tree = DecisionTreeClassifier(criterion = "gini",max_depth=4, min_samples_leaf=10)
depth_tree.fit(X_train, y_train)

```

在前面的代码中，我们在原始数据上构建了一个简单的决策树，将`SeriousDlqin2yrs`作为因变量，其余变量作为自变量。

```py
#Get the predictions on top of train dataset itself
dt_pred = depth_tree.predict_proba(X_train)
X_train['prediction']=dt_pred[:,1]

```

在前面的代码中，我们预测了第一个决策树的输出。这将有助于我们得出残差。

```py
#Get the predictions on top of test dataset
X_test['prediction']=depth_tree.predict_proba(X_test)[:,1]

```

在前面的代码中，虽然我们计算了测试数据集中的输出概率，但请注意，我们不能计算残差，因为实际上，我们不允许查看测试数据集的因变量。作为前面代码的延续，我们将在下面的代码中构建 20 个残差决策树:

```py
from sklearn.tree import DecisionTreeRegressor
import numpy as np
from sklearn.metrics import roc_auc_score
depth_tree2 = DecisionTreeRegressor(criterion = "mse",max_depth=4, min_samples_leaf=10)
for i in range(20):
    # Calculate residual
    train_errorn=y_train-X_train['prediction']
    # remove prediction variable that got appended to independent variable earlier
    X_train2=X_train.drop(['prediction'],axis=1)
    X_test2=X_test.drop(['prediction'],axis=1)

```

在前面的代码中，注意我们正在计算第 n 棵决策树的残差。我们将从`X_train2`数据集中删除`prediction`列，因为`prediction`列不能是在`for`循环的下一次迭代中构建的后续模型中的独立变量之一。

```py
    # Build a decision tree to predict the residuals using independent variables
    dt2=depth_tree2.fit(X_train2, train_errorn)
    # predict the residual
    dt_pred_train_errorn = dt2.predict(X_train2)

```

在前面的代码中，我们正在拟合决策树，其中因变量是残差，自变量是数据集的原始自变量。

一旦决策树被拟合，下一步就是预测残差(它是因变量):

```py
    # update the predictions based on predicted residuals
    X_train['prediction']=(X_train['prediction']+dt_pred_train_errorn*1)
    # Calculate AUC
    train_auc=roc_auc_score(y_train,X_train['prediction'])
    print("AUC on training data set is: "+str(train_auc))

```

在这段代码中，原始预测(存储在`X_train`数据集中)用我们在上一步中获得的预测残差进行了更新。

注意，我们是通过残差的预测(`dt_pred_train_errorn`)来更新我们的预测。我们在前面的代码中明确给出了一个`*1`，因为收缩率或学习率的概念将在下一节解释(T2 将被替换为`*learning_rate`)。

一旦预测被更新，我们计算训练数据集的 AUC:

```py
    # update the predictions based on predicted residuals for test dataset
    dt_pred_test_errorn = dt2.predict(X_test2)
    X_test['prediction']=(X_test['prediction']+dt_pred_test_errorn)
    # Calculate AUC
    test_auc=roc_auc_score(y_test,X_test['prediction'])
    print("AUC on test data set is: "+str(test_auc))

```

在这里，我们更新了测试数据集上的预测。我们不知道测试数据集的残差，但是我们基于为预测训练数据集的残差而构建的决策树来更新对测试数据集的预测。理想情况下，如果测试数据集没有残差，预测的残差应该接近 0，如果测试数据集的原始决策树有一些残差，那么预测的残差将远离 0。

一旦测试数据集的预测被更新，我们打印出测试数据集的 AUC。

让我们看看前面代码的输出:

![A463052_1_En_6_Figb_HTML.jpg](A463052_1_En_6_Figb_HTML.jpg)

![A463052_1_En_6_Figa_HTML.jpg](A463052_1_En_6_Figa_HTML.jpg)

注意，训练数据集的 AUC 随着更多的树而持续增加。但是测试数据集的 AUC 在某次迭代后降低。

## 收缩

GBM 是基于决策树的。因此，就像随机森林算法一样，GBM 的准确性取决于所考虑的树的深度、建立的树的数量和终端节点中的最小观察数量。收缩率是 GBM 中的一个附加参数。让我们看看，如果我们改变学习率/收缩，训练和测试数据集 AUC 的输出会发生什么。我们将学习率初始化为 0.05，并运行更多的树:

```py
from sklearn.model_selection import train_test_split
# creating independent variables
X = data.drop('SeriousDlqin2yrs',axis=1)
# creating dependent variables
y = data['SeriousDlqin2yrs']
# creating train and test datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.tree import DecisionTreeClassifier
depth_tree = DecisionTreeClassifier(criterion = "gini",max_depth=4, min_samples_leaf=10)
depth_tree.fit(X_train, y_train)

#Get the predictions on top of train and test datasets
dt_pred = depth_tree.predict_proba(X_train)
X_train['prediction']=dt_pred[:,1]
X_test['prediction']=depth_tree.predict_proba(X_test)[:,1]
from sklearn.tree import DecisionTreeRegressor
import numpy as np
from sklearn.metrics import roc_auc_score
depth_tree2 = DecisionTreeRegressor(criterion = "mse",max_depth=4, min_samples_leaf=10)
learning_rate = 0.05
for i in range(20):
    # Calculate residual
    train_errorn=y_train-X_train['prediction']
    # remove prediction variable that got appended to independent variable earlier
    X_train2=X_train.drop(['prediction'],axis=1)
    X_test2=X_test.drop(['prediction'],axis=1)
    # Build a decision tree to predict the residuals using independent variables
    dt2=depth_tree2.fit(X_train2, train_errorn)
    # predict the residual
    dt_pred_train_errorn = dt2.predict(X_train2)
    # update the predictions based on predicted residuals
    X_train['prediction']=(X_train['prediction']+dt_pred_train_errorn*learning_rate)
    # Calculate AUC
    train_auc=roc_auc_score(y_train,X_train['prediction'])
    print("AUC on training data set is: "+str(train_auc))
    # update the predictions based on predicted residuals for test dataset
    dt_pred_test_errorn = dt2.predict(X_test2)
    X_test['prediction']=(X_test['prediction']+dt_pred_test_errorn*learning_rate)
    # Calculate AUC
    test_auc=roc_auc_score(y_test,X_test['prediction'])
    print("AUC on test data set is: "+str(test_auc))

```

上述代码的输出是:

下面是前几棵树的输出:

![A463052_1_En_6_Figc_HTML.jpg](A463052_1_En_6_Figc_HTML.jpg)

这是最后几棵树的产量:

![A463052_1_En_6_Figd_HTML.jpg](A463052_1_En_6_Figd_HTML.jpg)

与前面的情况不同，其中`learning_rate` = 1，较低的`learning_rate`导致测试数据集 AUC 随着训练数据集 AUC 一致增加。

## adaboost 算法

在讨论其他增强方法之前，我想和我们在前面章节中看到的做一个比较。在计算逻辑回归的误差度量时，我们可以使用传统的平方误差。但是我们转向了熵误差，因为它对大量的误差惩罚更大。

以类似的方式，残差计算可以根据因变量的类型而变化。对于连续的因变量，残差计算可以是高斯型的(因变量的(实际-预测))，而对于离散变量，残差计算可以不同。

### AdaBoost 理论

AdaBoost 是 adaptive boosting 的缩写。下面是高级算法:

1.  仅使用几个独立变量构建一个弱学习器(在这种情况下是决策树技术)。
    1.  注意，在构建第一个弱学习者时，与每个观察相关联的权重是相同的。 
2.  识别基于弱学习者的不正确分类的观察。
3.  以这样的方式更新观察值的权重，使得先前弱学习器中的错误分类被给予更大的权重，而先前弱学习器中的正确分类被给予更小的权重。
4.  根据预测的准确性为每个弱学习者分配一个权重。
5.  最终预测将基于多个弱学习者的加权平均预测。

自适应潜在地指的是根据先前的分类是正确的还是不正确的来更新观察值的权重。提升可能指的是给每个弱学习者分配权重。

### AdaBoost 的工作细节

让我们看一个 AdaBoost 的例子:

1.  Build a weak learner. Let’s say the dataset is the first two columns in the following table (available as “adaboost.xlsx” in github):

    ![A463052_1_En_6_Fige_HTML.jpg](A463052_1_En_6_Fige_HTML.jpg)

    Once we have the dataset, we build a weak learner (decision tree) according to the steps laid out to the right in the preceding table. From the table we can see that X <= 4 is the optimal splitting criterion for this first decision tree.  
2.  Calculate the error metric (the reason for having “Changed Y” and “Yhat” as new columns in the below table will be explained after step 4):

    ![A463052_1_En_6_Figf_HTML.jpg](A463052_1_En_6_Figf_HTML.jpg)

    The formulae used to obtain the preceding table are as follows:

    ![A463052_1_En_6_Figg_HTML.jpg](A463052_1_En_6_Figg_HTML.jpg)

3.  计算应该与第一个弱学习者关联的权重:0.5×log((1-误差)/误差)= 0.5×log(0.9/0.1)= 0.5×log(9)= 0.477
4.  Update the weights associated with each observation in such a way that the previous weak learner’s misclassifications have high weight and the correct classifications have low weight (essentially, we are tuning the weights associated with each observation in such a way that, in the new iteration, we try and make sure that the misclassifications are predicted more accurately):

    ![A463052_1_En_6_Figh_HTML.jpg](A463052_1_En_6_Figh_HTML.jpg)

    Note that the updated weights are calculated by the following formula:

    ![ $$ {original\kern0.34em weight}^{\ast}\kern0.125em {e}^{\left(- weightage\ oflearner\ast yhat\ast changedy\right)} $$ ](A463052_1_En_6_Chapter_Equa.gif)

    That formula should explain the need for changing the discrete values of y from {0,1} to {–1,1}. By changing 0 to –1, we are in a position to perform the multiplication better. Also note that in the preceding formula, weightage associated with the learner in general would more often than not be positive. When `yhat` and `changed_y` are the same, the exponential part of formula would be a lower number (as the – weightage of learner × `yhat` × `changed_y` part of the formula would be negative, and an exponential of a negative is a small number). When `yhat` and `changed_y` are different values, that’s when the exponential would be a bigger number, and hence the updated weight would be more than the original weight.  
5.  We observe that the updated weights we obtained earlier do not sum up to 1\. We update each weight in such a way that the sum of weights of all observations is equal to 1\. Note that, the moment weights are introduced, we can consider this as a regression exercise. Now that the weight for each observation is updated, we repeat the preceding steps until the weight of the misclassified observations increases so much that it is now correctly classified:

    ![A463052_1_En_6_Figi_HTML.jpg](A463052_1_En_6_Figi_HTML.jpg)

    Note that the weights in the third column in the preceding table are updated based on the formula we derived earlier post normalization (ensuring that the sum of weights is 1). You should be able to see that the weight associated with misclassification (the eighth observation with the independent variable value of 8) is more than any other observation. Note that although everything is similar to a typical decision tree till the prediction columns, error calculation gives emphasis to weights of observations. error in the left node is the summation of the weight of each observation that was misclassified in the left node and similarly for the right node. overall error is the summation of error across both nodes (error in left node + error in right node). In this instance, overall error is still the least at the fourth observation. The updated weights based on the previous step are as follows:

    ![A463052_1_En_6_Figj_HTML.jpg](A463052_1_En_6_Figj_HTML.jpg)

    Continue the process one more time:

    ![A463052_1_En_6_Figk_HTML.jpg](A463052_1_En_6_Figk_HTML.jpg)

    From the preceding table, note that overall error in this iteration is minimum at X <= 8, as the weight associated with the eighth observation is a lot more than other observations, and hence overall error came up as the least at the eighth observation this time. However, note that the weightage associated with the preceding tree would be low, because the accuracy of that tree is low when compared to the previous two trees.  
6.  一旦做出所有预测，观察的最终预测被计算为与每个弱学习者相关联的权重的总和乘以每个观察的概率输出。

## GBM 的附加功能

在上一节中，我们看到了如何手工构建 GBM。在本节中，我们将了解可以内置的其他参数:

*   行抽样:在随机森林中，我们看到对随机选择的行进行抽样会产生一个更通用、更好的模型。在 GBM 中，我们也可以对行进行采样，以进一步提高模型性能。
*   列采样:与行采样类似，通过对每个决策树的列进行采样，可以避免一定程度的过度拟合。

随机森林和 GBM 技术都是基于决策树的。然而，随机森林可以被认为是并行构建多棵树，最终我们将所有多棵树的平均值作为最终预测。在 GBM 中，我们构建多棵树，但是是按顺序的，其中每棵树都试图预测其前一棵树的残差。

## 用 Python 实现 GBM

GBM 可以使用如下的`scikit-learn`库在 Python 中实现(代码在 github 中以“GBM.ipynb”的形式提供):

```py
from sklearn import ensemble
gb_tree = ensemble.GradientBoostingClassifier(loss='deviance',learning_rate=0.05,n_estimators=100,min_samples_leaf=10,max_depth=13,max_features=2,subsample=0.7,random_state=10)
gb_tree.fit(X_train, y_train)

```

注意，关键输入参数是损失函数(无论是正常残差方法还是基于 AdaBoost 的方法)、学习速率、树的数量、每棵树的深度、列采样和行采样。

一旦建立了 GBM，就可以进行如下预测:

```py
from sklearn.metrics import roc_auc_score
gb_pred=gb_tree.predict_proba(X_test)
roc_auc_score(y_test, gb_pred[:,1])

```

## 在 R 中实现 GBM

R 中的 GBM 和 Python 中的 GBM 有相似的参数。GBM 可以按如下方式实施:

![A463052_1_En_6_Figl_HTML.jpg](A463052_1_En_6_Figl_HTML.jpg)

在该公式中，我们以下列方式指定因变量和自变量:`dependent_variable` `~`要使用的自变量集合。

该分布指定它是高斯、伯努利还是 AdaBoost 算法。

```py
library(gbm)
gb=gbm(SeriousDlqin2yrs~.,data=train,n.trees=10,interaction.depth=5,shrinkage=0.05)

```

*   `n. trees`指定要建造的树的数量。
*   `interaction.depth`是树木的`max_depth`。
*   `n.minobsinnode`是一个节点中的最小观察次数。
*   `shrinkage`是学习率。
*   `bag.fraction`是随机选择的训练集观察值的一部分，用于提出扩展中的下一棵树。
*   R 中的 GBM 算法运行如下:

可以做出如下预测:

```py
pred=predict(gb,test,n.trees=10,type="response")

```

## 摘要

在本章中，您学习了以下内容:

*   GBM 是一种基于决策树的算法，它试图预测给定决策树中前一个决策树的残差。
*   收缩和深度是 GBM 中需要调整的一些更重要的参数。
*   梯度增强和自适应增强的区别。
*   调整学习率参数如何提高 GBM 中的预测精度。