# 8\. Word2vec

Word2vec 是一种基于神经网络的方法，在传统的文本挖掘分析中非常方便。

传统文本挖掘方法的一个问题是数据的维度问题。考虑到典型文本中有大量不同的单词，构建的列数可能会非常多(其中每列对应一个单词，列中的每个值指定该单词是否存在于与该行相对应的文本中——本章稍后将详细介绍)。

Word2vec 有助于以更好的方式表示数据:彼此相似的单词具有相似的向量，而彼此不相似的单词具有不同的向量。在这一章中，我们将探索计算单词向量的不同方法。

为了了解 Word2vec 如何有用，让我们探索一个问题。假设我们有两个输入句子:

![A463052_1_En_8_Figa_HTML.png](A463052_1_En_8_Figa_HTML.png)

直觉上，我们知道享受和喜欢是相似的词。然而，在传统的文本挖掘中，当我们对单词进行一次热编码时，我们的输出看起来像这样:

![A463052_1_En_8_Figb_HTML.png](A463052_1_En_8_Figb_HTML.png)

注意，一键编码导致每个单词被分配一列。one-hot 编码的主要问题是单词{I，enjoy}之间的欧几里德距离与单词{enjoy，like}之间的距离相同。但是我们知道{我，享受}之间的距离应该大于{享受，喜欢}之间的距离，因为享受和喜欢彼此更同义。

## 手工构建单词向量

在构建单词向量之前，我们将假设表述如下:

"相关的词周围会有相似的词."

例如,“国王”和“王子”这两个词周围经常会出现类似的词。本质上，单词的上下文(周围的单词)是相似的。

有了这个假设，让我们把每个单词看做输出，把所有的上下文单词(周围的单词)看做输入。因此，我们的数据集翻译如下(在 github 中以“word2vec.xlsx”的形式提供):

![A463052_1_En_8_Figc_HTML.jpg](A463052_1_En_8_Figc_HTML.jpg)

通过使用上下文单词作为输入，我们试图预测给定单词作为输出。

前面的输入和输出单词的矢量化形式如下所示(注意，第 3 行中给出的列名{I，enjoy，playing，TT，like}仅供参考):

![A463052_1_En_8_Figd_HTML.jpg](A463052_1_En_8_Figd_HTML.jpg)

注意，给定输入单词——{享受，演奏，TT }——向量形式是{0，1，1，1，0}，因为输入不包含 I 和 like，所以第一个和最后一个索引是 0(注意在第一页中完成的 one-hot 编码)。

现在，假设我们想将 5 维输入向量转换成 3 维向量。在这种情况下，我们的隐藏层有三个相关的神经元。我们的神经网络看起来会如图 [8-1](#Fig1) 所示。

![A463052_1_En_8_Fig1_HTML.png](A463052_1_En_8_Fig1_HTML.png)

图 8-1

Our neural network

每层的尺寸如下:

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| 层 | 大小 | 评论 |
| :-- | :-- | :-- |
| 输入层 | 8 × 5 | 因为有 8 个输入和 5 个索引(唯一字) |
| 隐藏层的权重 | 5 × 3 | 因为 3 个神经元各有 5 个输入 |
| 隐藏层输出 | 8 × 3 | 输入和隐藏层的矩阵乘法 |
| 从隐藏到输出的权重 | 3 × 5 | 隐藏层的 3 个输出列映射到 5 个原始输出列 |
| 输出层 | 8 × 5 | 隐藏层输出与从隐藏层到输出层的权重之间的矩阵乘法 |

下面显示了每种方法的工作原理:

![A463052_1_En_8_Fige_HTML.jpg](A463052_1_En_8_Fige_HTML.jpg)

注意，输入向量乘以随机初始化的隐藏层权重矩阵，以获得隐藏层的输出。给定输入大小为 8 × 5，隐藏层大小为 5 × 3，矩阵乘法的输出为 8 × 3。而且，与传统的神经网络不同，在 Word2vec 方法中，我们不在隐藏层上应用任何激活:

![A463052_1_En_8_Figf_HTML.jpg](A463052_1_En_8_Figf_HTML.jpg)

一旦我们有了隐藏层的输出，我们就把它们乘以一个从隐藏层到输出层的权重矩阵。假设隐藏层的输出大小为 8 × 3，要输出的隐藏层大小为 3 × 5，则我们的输出层为 8 × 5。但是请注意，输出图层有一系列数字，包括正数和负数，以及大于 1 或的数字

因此，正如我们在神经网络中所做的那样，我们通过 softmax 将数字转换为 0 到 1 之间的数字:

![A463052_1_En_8_Figg_HTML.jpg](A463052_1_En_8_Figg_HTML.jpg)

为了方便起见，我将 softmax 分解为两个步骤:

1.  对数字应用指数。
2.  将步骤 1 的输出除以步骤 1 输出的行和。

在前面的输出中，我们看到第一列的输出非常接近第一行的 1，第二列的输出是第二行的 0.5，依此类推。

获得预测值后，我们将它们与实际值进行比较，以计算整个批次的交叉熵损失，如下所示:

![A463052_1_En_8_Figh_HTML.jpg](A463052_1_En_8_Figh_HTML.jpg)

交叉熵损失=–∑实际值× Log(概率，2)

既然我们已经计算了总体交叉熵误差，我们的任务是通过使用选择的优化器改变随机初始化的权重来减少总体交叉熵误差。一旦我们达到了最佳的权重值，我们剩下的隐藏层看起来像这样:

![A463052_1_En_8_Figi_HTML.jpg](A463052_1_En_8_Figi_HTML.jpg)

既然我们已经计算了输入单词和隐藏层权重，现在可以通过将输入单词乘以隐藏层表示来在较低的维度中表示单词。

输入层(每个单词 1 × 5)和隐藏层(5 × 3 权重)的矩阵乘法是一个大小为(1 × 3)的向量:

![A463052_1_En_8_Figj_HTML.jpg](A463052_1_En_8_Figj_HTML.jpg)

如果我们现在考虑单词{enjoy，like}，我们应该注意到这两个单词的向量彼此非常相似(也就是说，这两个单词之间的距离很小)。

这样，我们将原始输入的 one-hot-encoded 向量(其中{enjoy，like}之间的距离较大)转换为转换后的单词向量(其中{enjoy，like}之间的距离较小)。

## 构建单词向量的方法

我们在前面的部分中采用的构建单词向量的方法被称为连续单词包(CBOW)模型。

取一句话“那只敏捷的棕色狐狸跳过了那只狗。”CBOW 模型是这样处理这句话的:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 输入单词 | 输出字 |
| :-- | :-- |
| {那个，快，狐狸，跳了} | {布朗} |
| {快速，棕色，跳跃，结束} | {fox} |
| {布朗，福克斯，完毕，该} | {jumped} |
| {狐狸，跳了，狗} | {结束} |

1.  固定窗口大小。也就是说，选择给定单词左边和右边的 n 个单词。例如，假设窗口大小是给定单词左右各 2 个单词。
2.  给定窗口大小，输入和输出向量如下所示:

另一种建立单词向量的方法叫做跳格模型。在 skip-gram 模型中，前面的步骤是相反的，如下所示:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 输入单词 | 输出字 |
| :-- | :-- |
| {布朗} | {那个，快，狐狸，跳了} |
| {fox} | {快速，棕色，跳跃，结束} |
| {jumped} | {布朗，福克斯，完毕，该} |
| {结束} | {狐狸，跳了，狗} |

无论是 skip-gram 模型还是 CBOW 模型，获得隐藏层向量的方法都是相同的。

## 在 Word2vec 模型中需要注意的问题

对于到目前为止所讨论的计算方法，本节着眼于我们可能面临的一些常见问题。

### 常用词

像 the 这样的典型常用词在词汇中经常出现。在这种情况下，输出中出现的单词更频繁。如果不进行处理，这可能会导致大部分输出是最常用的单词，比如 the，而不是其他单词。我们需要有一种方法来惩罚一个频繁出现的单词在训练数据集中出现的次数。

在典型的 Word2vec 分析中，我们惩罚频繁出现的单词的方式如下。选择一个单词的概率是这样计算的:

![$$ P\left({w}_i\right)=\left(\sqrt{\frac{z\left({w}_i\right)}{0.001}}+1\right)\cdot \frac{0.001}{z\left({w}_i\right)} $$](A463052_1_En_8_Chapter_Equa.gif)

z(w)是一个单词在任何单词的总出现次数中出现的次数。该公式的绘图显示了图 [8-2](#Fig2) 中的曲线。

![A463052_1_En_8_Fig2_HTML.jpg](A463052_1_En_8_Fig2_HTML.jpg)

图 8-2

The resultant curve

请注意，随着 z(w) (x 轴)的增加，选择概率(y 轴)急剧下降。

### 负采样

让我们假设在我们的数据集中总共有 10，000 个唯一的单词，也就是说，每个向量有 10，000 个维度。我们还假设我们正在从原始的 10，000 维向量创建一个 300 维向量。这意味着，从隐藏层到输出层，总共有 300×10000 = 3000000 个权重。

权重数量如此之大的一个主要问题是，它可能会导致数据上的过度拟合。这也可能导致更长的训练时间。

负采样是克服这个问题的一种方法。假设不是检查所有 10，000 个维度，而是选择输出为 1(正确标签)的索引和标签为 0 的五个随机索引。这样，我们将在单次迭代中更新的权重数量从 300 万减少到 300 × 6 = 1800 个权重。

我说过负索引的选择是随机的，但是在 Word2vec 的实际实现中，选择是基于一个单词与其他单词相比的频率。与频率较低的单词相比，频率较高的单词被选中的几率更高。

选择五个否定词的概率如下:

![$$ P\left({w}_i\right)=\frac{f{\left({w}_i\right)}^{3/4}}{\sum \limits_{j=0}^n\left(f{\left({w}_j\right)}^{3/4}\right)} $$](A463052_1_En_8_Chapter_Equb.gif)

f(w)是给定词的出现频率。

一旦计算出每个单词的概率，单词的选择过程如下:频率较高的单词重复频率较高，频率较低的单词重复频率较低，并存储在一个表中。鉴于高频词出现的频率更高，从表格中随机选择五个词时，它们被选中的几率更高。

## 用 Python 实现 Word2vec

Word2vec 可以使用`gensim`包在 Python 中实现(Python 实现在 github 中的名称是“word2vec.ipynb”)。

第一步是初始化软件包:

```py
import nltk
import gensim
import pandas as pd

```

导入包后，我们需要提供前面几节中讨论的参数:

*   `logging`本质上帮助我们跟踪单词向量计算完成的程度。
*   `num_features`是隐含层的神经元个数。
*   `min_word_count`是被接受用于计算的单词频率的截止值。
*   `context`是窗口大小
*   `downsampling`有助于降低选择更常用单词的概率。

```py
import logging
logging.basicConfig(format=’%(asctime)s : %(levelname)s : %(message)s’,\
    level=logging.INFO)

# Set values for various parameters
num_features = 100    # Word vector dimensionality  
min_word_count = 50   # Minimum word count    
num_workers = 4       # Number of threads to run in parallel
context = 9           # Context window size 
downsampling = 1e-4   # Downsample setting for frequent words

```

模型的输入词汇应该如下所示:

![A463052_1_En_8_Figk_HTML.jpg](A463052_1_En_8_Figk_HTML.jpg)

注意，所有输入的句子都被标记化了。

Word2vec 模型训练如下:

```py
from gensim.models import word2vec
print(“Training model...”)
w2v_model = word2vec.Word2Vec(t2, workers=num_workers,
            size=num_features, min_count = min_word_count,
            window = context, sample = downsampling)

```

一旦模型被训练，满足指定标准的词汇表中任何单词的权重向量可以如下获得:

```py
model['word'] # replace the "word" with the word of your interest

```

类似地，与给定单词最相似的单词可以如下获得:

```py
model.most_similar('word')

```

## 摘要

在本章中，您学习了以下内容:

*   Word2vec 是一种可以帮助将文本单词转换成数字向量的方法。
*   这对下游的多种方法来说是一个强大的第一步——例如，我们可以在构建模型时使用单词 vectors。
*   Word2vec 使用一种 CBOW 或 skip-gram 模型来提供向量，这种模型具有神经网络架构，有助于提供向量。
*   神经网络中的隐含层是生成单词向量的关键。