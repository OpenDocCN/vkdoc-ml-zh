# 10.递归神经网络

在第 [9](09.html) 章中，我们看了卷积神经网络(CNN)如何改进传统的图像分类神经网络架构。尽管细胞神经网络在处理图像平移和旋转的图像分类中表现很好，但它们不一定有助于识别时间模式。本质上，人们可以认为 CNN 是识别静态模式。

递归神经网络(RNNs)旨在解决识别时间模式的问题。

在本章中，您将学习以下内容:

*   RNN 的工作细节
*   在 RNN 使用嵌入
*   使用 RNN 生成文本
*   使用 RNN 进行情感分类
*   从 RNN 搬到 LSTM

RNN 可以用多种方式来设计。一些可能的方法如图 [10-1](#Fig1) 所示。

![A463052_1_En_10_Fig1_HTML.png](A463052_1_En_10_Fig1_HTML.png)

图 10-1

RNN examples

在图 [10-1](#Fig1) 中注意以下几点:

*   底部的方框是输入
*   中间的方框是隐藏层
*   顶部的方框是输出

所示一对一架构的一个例子是我们在第 [7](07.html) 章中看到的典型神经网络，在输入和输出层之间有一个隐藏层。一对多 RNN 架构的一个例子是输入图像并输出图像的标题。多对一 RNN 架构的一个例子可以是作为输入给出的电影评论和作为输出的电影情感(正面、负面或中性评论)。最后，多对多 RNN 架构的一个例子是从一种语言到另一种语言的机器翻译。

## 理解架构

让我们看一个例子，更仔细地看看 RNN 建筑。我们的任务如下:“给定一串单词，预测下一个单词。”我们将尝试预测“这是一个 _____”后面的单词。假设实际的句子是“这是一个例子。”

传统的文本挖掘技术将以下列方式解决这个问题:

1.  对每个单词进行编码，如果需要，为额外的单词留出空间:

    ```
    This: {1,0,0,0}
    is: {0,1,0,0}
    an: {0,0,1,0}

    ```

2.  对句子进行编码:

    ```
    "This is an": {1,1,1,0}

    ```

3.  创建训练数据集:

    ```
    Input --> {1,1,1,0}
    Output --> {0,0,0,1}

    ```

4.  建立一个有输入和输出的模型。

这里的一个主要缺点是，如果输入句子是“这是一个”或“这是一个”或“这是一个”，输入表示不会改变。我们知道，其中每一个都非常不同，不能用数学上的相同结构来表示。

这种认识要求有不同的架构，一个看起来更像图 [10-2](#Fig2) 的架构。

![A463052_1_En_10_Fig2_HTML.png](A463052_1_En_10_Fig2_HTML.png)

图 10-2

A change in the architecture

在图 [10-2](#Fig2) 所示的架构中，句子中的每个单词都放在三个输入框中的一个单独的框中。此外，由于“this”进入第一个框，“is”进入第二个框，“an”进入第三个框，因此句子的结构得以保留。

输出“示例”应该在顶部的输出框中。

## 解读 RNN

我们可以认为 RNN 是一种保存记忆的机制，其中记忆包含在隐藏层中。如图 [10-3](#Fig3) 所示。

![A463052_1_En_10_Fig3_HTML.png](A463052_1_En_10_Fig3_HTML.png)

图 10-3

Memory in the hidden layer

图 [10-3](#Fig3) 中右边的网络是左边网络的展开版本。左边的网络是一个传统的网络，有一个变化:隐藏层在连接到输入的同时连接到自身(隐藏层是图中的圆)。

请注意，当隐藏层与输入层一起连接到自身时，它将连接到隐藏层的“以前版本”和当前输入层。我们可以把这种隐藏层被连接回自身的现象看作是 RNN 创造记忆的机制。

权重 U 表示将输入层连接到隐藏层的权重，权重 W 表示隐藏层到隐藏层的连接，权重 V 表示隐藏层到输出层的连接。

Why Store Memory?

需要存储记忆，因为在前面的例子中和一般的文本生成中，下一个单词不一定依赖于前面的单词，而是依赖于该单词前面的几个单词的上下文来预测。

鉴于我们正在看前面的单词，应该有一种方法将它们保存在内存中，这样我们就可以更准确地预测下一个单词。此外，我们还应该按顺序进行记忆——通常情况下，在预测下一个单词时，较新的单词比距离被预测的单词较远的单词更有用。

## RNN 的工作细节

请注意，典型的神经网络有一个输入层，然后在隐藏层激活，然后在输出层激活 softmax。RNN 是相似的，但是有记忆。我们再来看另一个例子:“这是一个例子”。给定一个输入“This”，我们被期望预测“is”，类似地，对于一个输入“is”，我们被期望得出一个“an”的预测和一个作为输入的“an”的“example”的预测。该数据集在 github 中以“RNN 维度直觉. xlsx”的名称提供。

编码的输入和输出字如下:

![A463052_1_En_10_Figa_HTML.jpg](A463052_1_En_10_Figa_HTML.jpg)

RNN 的结构如图 [10-4](#Fig4) 所示。

![A463052_1_En_10_Fig4_HTML.png](A463052_1_En_10_Fig4_HTML.png)

图 10-4

The RNN structure

让我们来解构每个相关权重矩阵的维度:

![A463052_1_En_10_Figb_HTML.jpg](A463052_1_En_10_Figb_HTML.jpg)

wxh 随机初始化，维数为 4 × 3。每个输入的尺寸为 1 × 4。因此，隐藏层是输入和 wxh 之间的矩阵乘法，每个输入行的维数为 1 × 3。预期的输出是句子中紧挨着输入单词的单词的一次性编码版本。请注意，最后一个预测“空白”是不准确的，因为我们的预期输出都是 0。理想情况下，我们应该在一键编码版本中有一个新的列来处理所有看不见的单词。然而，为了理解 RNN 的工作细节，我们将保持简单，在预期输出中有 4 列。

正如我们前面看到的，在 RNN，一个隐藏层在展开时会连接到另一个隐藏层。假设一个隐藏层连接到下一个隐藏层，与前一个隐藏层和当前隐藏层之间的连接相关联的权重(whh)在维度上将是 3 × 3，因为 1 × 3 矩阵乘以 3 × 3 矩阵将产生 1 × 3 矩阵。下图中的最终隐藏层计算将在后续页面中解释。

![A463052_1_En_10_Figc_HTML.jpg](A463052_1_En_10_Figc_HTML.jpg)

注意，wxh 和 whh 是随机初始化，而隐藏层和最终隐藏层是计算出来的。在接下来的几页中，我们将看看这些计算是如何完成的。

隐藏层在不同时间步的计算如下:

![$$ {h}^{(t)}={\phi}_h\left({z}_h^{(t)}\right)={\phi}_h\left({W}_{xh}{x}^{(t)}+{W}_{hh}{h}^{\left(t-1\right)}\right) $$](A463052_1_En_10_Chapter_Equa.gif)

其中![A463052_1_En_10_Figd_HTML.gif](A463052_1_En_10_Figd_HTML.gif)是执行的激活(一般为 tanh 激活)。

从输入层到隐藏层的计算包括两个部分:

*   输入层和 wxh 的矩阵乘法。
*   隐藏层和 whh 的矩阵乘法。

给定时间步长的隐藏层值的最终计算将是前面两个矩阵乘法的总和，并将结果传递给双曲正切激活函数。

输入层和 wxh 的矩阵乘法如下所示:

![A463052_1_En_10_Fige_HTML.jpg](A463052_1_En_10_Fige_HTML.jpg)

以下部分介绍了不同时间步长下隐藏层值的计算。

### 时间步长 1

第一个时间步长的隐藏层值将是输入层和 wxh 之间的矩阵乘法的值(因为在前一个时间步长中没有隐藏层值):

![A463052_1_En_10_Figf_HTML.jpg](A463052_1_En_10_Figf_HTML.jpg)

### 时间步长 2

开始第二次输入，隐藏层由当前时间步的隐藏层分量和来自前一时间步的隐藏层分量组成:

![A463052_1_En_10_Figh_HTML.jpg](A463052_1_En_10_Figh_HTML.jpg)

![A463052_1_En_10_Figg_HTML.jpg](A463052_1_En_10_Figg_HTML.jpg)

### 时间步长 3

类似地，在第三时间步，输入将是当前时间步的输入和来自前一时间步的隐藏单位值。注意，前一时间步(t-1)中的隐藏单元也受到来自(t-2)的隐藏值的影响。

![A463052_1_En_10_Figi_HTML.jpg](A463052_1_En_10_Figi_HTML.jpg)

类似地，在第四个时间步计算隐藏层值。

现在我们已经计算出了隐藏层，我们让它通过一个激活，就像我们在传统 NN 中做的那样:

![A463052_1_En_10_Figj_HTML.jpg](A463052_1_En_10_Figj_HTML.jpg)

假设对于每个输入，隐藏层激活的输出大小为 1 × 3，为了获得大小为 1 × 4 的输出(因为预期输出“示例”的一次热编码版本的大小为 4 列)，隐藏层的大小为什么应该为 3 × 4:

![A463052_1_En_10_Figk_HTML.jpg](A463052_1_En_10_Figk_HTML.jpg)

根据中间输出，我们执行 softmax 激活，如下所示:

![A463052_1_En_10_Figl_HTML.jpg](A463052_1_En_10_Figl_HTML.jpg)

softmax 的第二步是归一化每个像元值以获得概率值:

![A463052_1_En_10_Figm_HTML.jpg](A463052_1_En_10_Figm_HTML.jpg)

一旦获得概率，就可以通过计算预测和实际输出之间的交叉熵损失来计算损失。

最后，我们将以类似于 NN 的方式，通过前向和后向传播时期的组合来最小化损耗。

## 实现 RNN:简单

要了解 RNN 在 keras 中是如何实现的，我们先通过一个简单化的例子(只了解 RNN 的 keras 实现，然后通过在 Excel 中实现来巩固我们的理解):对两个句子进行分类(这两个句子有三个单词的穷举列表)。通过这个玩具示例，我们应该能够更好地快速理解输出(在 github 中代码为“simpleRNN.ipynb”):

```
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers.recurrent import SimpleRNN
from keras.layers.embeddings import Embedding
from keras.layers import LSTM
import numpy as np

```

初始化文档并对与这些文档相对应的单词进行编码:

```
# define documents
docs = ['very good',
             'very bad']
# define class labels
labels = [1,0]
from collections import Counter
counts = Counter()
for i,review in enumerate(docs):
    counts.update(review.split())
words = sorted(counts, key=counts.get, reverse=True)
vocab_size=len(words)
word_to_int = {word: i for i, word in enumerate(words, 1)}
encoded_docs = []
for doc in docs:
    encoded_docs.append([word_to_int[word] for word in doc.split()])

```

将文档填充到两个单词的最大长度，这是为了保持一致性，以便所有输入都具有相同的大小:

![A463052_1_En_10_Fign_HTML.jpg](A463052_1_En_10_Fign_HTML.jpg)

```
# pad documents to a max length of 2 words
max_length = 2
padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding="pre")
print(padded_docs)

```

### 编译模型

SimpleRNN 函数的输入形状应为(时间步长数，每个时间步长的要素数)的形式。此外，一般来说，RNN 使用双曲正切作为激活函数。以下代码将输入形状指定为(2，1)，因为每个输入都基于两个时间步长，并且每个时间步长只有一列表示它。`unroll=True`表示我们正在考虑以前的时间步骤:

```
# define the model
embed_length=1
max_length=2
model = Sequential()
model.add(SimpleRNN(1,activation='tanh', return_sequences=False,recurrent_initializer='Zeros',input_shape=(max_length,embed_length),unroll=True))
model.add(Dense(1, activation="sigmoid"))
# compile the model
model.compile(optimizer='adam', loss="binary_crossentropy", metrics=['acc'])
# summarize the model
print(model.summary())

```

`SimpleRNN` `(1,)`表示隐藏层中有一个神经元。`return_sequences`为假，因为我们没有返回任何输出序列，而且是单个输出:

![A463052_1_En_10_Figo_HTML.jpg](A463052_1_En_10_Figo_HTML.jpg)

模型编译完成后，让我们继续拟合模型，如下所示:

![A463052_1_En_10_Figp_HTML.jpg](A463052_1_En_10_Figp_HTML.jpg)

```
model.fit(padded_docs.reshape(2,2,1),np.array(labels).reshape(max_length,1),epochs=500)

```

注意我们已经重塑了`padded_docs`。这是因为我们需要在拟合时将训练数据集转换为如下格式:{数据大小，时间步长数，每个时间步长的要素}。此外，标签应该采用数组格式，因为编译模型中的最终密集层需要一个数组。

### 验证 RNN 的输出

现在我们已经安装好了玩具模型，让我们来验证之前创建的 Excel 计算。请注意，我们将输入作为原始编码{1，2，3 }—实际上，我们绝不会照原样采用原始编码，而是对输入进行一次热编码或创建嵌入。我们在本节中采用原始输入，只是为了比较 keras 的输出和我们将在 Excel 中进行的手工计算。

`model.layers`指定模型中的层，`weights`让我们了解与模型相关的层:

![A463052_1_En_10_Figq_HTML.jpg](A463052_1_En_10_Figq_HTML.jpg)

`model.weights`为我们提供了与模型中的权重相关的名称指示:

![A463052_1_En_10_Figr_HTML.jpg](A463052_1_En_10_Figr_HTML.jpg)

`model.get_weights()`给出了与模型相关的权重的实际值:

![A463052_1_En_10_Figs_HTML.jpg](A463052_1_En_10_Figs_HTML.jpg)

请注意，权重是有序的，即第一个权重值对应于`kernel:0`。换句话说，它与 wxh 相同，wxh 是与输入相关联的权重。

`recurrent_kernel:0`与 whh 相同，whh 是与先前的隐藏层和当前时间步的隐藏层之间的连接相关联的权重。`bias:0`是与输入相关的偏置。`dense_2/kernel:0`就是为什么——也就是连接隐藏层和输出的权重。`dense_2/bias:0`是与隐藏层和输出之间的连接相关的偏差。

让我们验证输入[1，3]的预测:

![A463052_1_En_10_Figt_HTML.jpg](A463052_1_En_10_Figt_HTML.jpg)

```
padded_docs[0].reshape(1,2,1)

```

![A463052_1_En_10_Figu_HTML.jpg](A463052_1_En_10_Figu_HTML.jpg)

```
import numpy as np
model.predict(padded_docs[0].reshape(1,2,1))

```

假设输入[1，3]的预测值为 0.53199(按此顺序)，让我们在 Excel 中进行验证(在 github 中以“简单 RNN 工作验证. xlsx”的形式提供):

![A463052_1_En_10_Figv_HTML.jpg](A463052_1_En_10_Figv_HTML.jpg)

两个时间步长的输入值如下:

![A463052_1_En_10_Figw_HTML.jpg](A463052_1_En_10_Figw_HTML.jpg)

输入和权重之间的矩阵乘法计算如下:

![A463052_1_En_10_Figx_HTML.jpg](A463052_1_En_10_Figx_HTML.jpg)

现在矩阵乘法已经完成，我们将继续计算时间步长 0 中的隐藏层值:

![A463052_1_En_10_Figy_HTML.jpg](A463052_1_En_10_Figy_HTML.jpg)

时间步长 1 中的隐藏层值如下:

tanh(时间步长 1 中的隐藏层值×与隐藏层到隐藏层连接相关的权重(whh) +前一个隐藏层值)

让我们先计算一下双曲正切函数的内部:

![A463052_1_En_10_Figz_HTML.jpg](A463052_1_En_10_Figz_HTML.jpg)

现在我们将计算时间步骤 1 的最终隐藏层值:

![A463052_1_En_10_Figaa_HTML.jpg](A463052_1_En_10_Figaa_HTML.jpg)

一旦计算出最终的隐藏层值，它将通过一个 sigmoid 层，因此最终的输出计算如下:

![A463052_1_En_10_Figab_HTML.jpg](A463052_1_En_10_Figab_HTML.jpg)

我们从 Excel 得到的最终输出与从 keras 得到的输出相同，因此是对我们之前看到的公式的验证:

![A463052_1_En_10_Figac_HTML.jpg](A463052_1_En_10_Figac_HTML.jpg)

## 实施 RNN:文本生成

现在我们已经看到了典型的 RNN 是如何工作的，让我们看看如何使用 keras 为 RNN 提供的 API(在 github 中以“RNN 文本生成. ipynb”的形式提供)来生成文本。

对于这个例子，我们将处理爱丽丝数据集( [`www.gutenberg.org/ebooks/11`](http://www.gutenberg.org/ebooks/11) ):

1.  导入包:

    ```
    from keras.models import Sequential
    from keras.layers import Dense,Activation
    from keras.layers.recurrent import SimpleRNN
    import numpy as np

    ```

2.  读取数据集:

    ```
    fin=open('/home/akishore/alice.txt',encoding='utf-8-sig')
    lines=[]
    for line in fin:
      line = line.strip().lower()
      line = line.decode("ascii","ignore")
      if(len(line)==0):
        continue
      lines.append(line)
    fin.close()
    text = " ".join(lines)

    ```

3.  Normalize the file to have only small case and remove punctuation, if any:

    ![A463052_1_En_10_Figad_HTML.jpg](A463052_1_En_10_Figad_HTML.jpg)

    ```
    text[:100]

    ```

    ```
    # Remove punctuations in dataset
    import re
    text = text.lower()
    text = re.sub('[^0-9a-zA-Z]+',' ',text)

    ```

4.  对单词进行一次热编码:

    ```
    from collections import Counter
    counts = Counter()
    counts.update(text.split())
    words = sorted(counts, key=counts.get, reverse=True)
    chars = words
    total_chars = len(set(chars))
    nb_chars = len(text.split())
    char2index = {word: i for i, word in enumerate(chars)}
    index2char = {i: word for i, word in enumerate(chars)}

    ```

5.  Create the input and target datasets :

    ![A463052_1_En_10_Figae_HTML.jpg](A463052_1_En_10_Figae_HTML.jpg)

    ```
    SEQLEN = 10
    STEP = 1
    input_chars = []
    label_chars = []
    text2=text.split()
    for i in range(0,nb_chars-SEQLEN,STEP):
        x=text2[i:(i+SEQLEN)]
        y=text2[i+SEQLEN]
        input_chars.append(x)
        label_chars.append(y)
    print(input_chars[0])
    print(label_chars[0])

    ```

6.  Encode the input and output datasets:

    ![A463052_1_En_10_Figaf_HTML.jpg](A463052_1_En_10_Figaf_HTML.jpg)

    ```
    X = np.zeros((len(input_chars), SEQLEN, total_chars), dtype=np.bool)
    y = np.zeros((len(input_chars), total_chars), dtype=np.bool)
    # Create encoded vectors for the input and output values
    for i, input_char in enumerate(input_chars):
        for j, ch in enumerate(input_char):
            X[i, j, char2index[ch]] = 1
        y[i,char2index[label_chars[i]]]=1
    print(X.shape)
    print(y.shape)

    ```

    Note that, the shape of X indicates that we have a total 30,407 rows that have 10 words each, where each of the 10 words is expressed in a 3,028-dimensional space (since there are a total of 3,028 unique words).  
7.  Build the model:

    ![A463052_1_En_10_Figag_HTML.jpg](A463052_1_En_10_Figag_HTML.jpg)

    ```
    HIDDEN_SIZE = 128
    BATCH_SIZE = 128
    NUM_ITERATIONS = 100
    NUM_EPOCHS_PER_ITERATION = 1
    NUM_PREDS_PER_EPOCH = 100
    model = Sequential()
    model.add(SimpleRNN(HIDDEN_SIZE,return_sequences=False,input_shape=(SEQLEN,total_chars),unroll=True))
    model.add(Dense(nb_chars, activation="sigmoid"))
    model.compile(optimizer='rmsprop', loss="categorical_crossentropy")
    model.summary()

    ```

8.  运行该模型，我们随机生成一个种子文本，并尝试根据给定的种子单词集预测下一个单词:

    ```
    for iteration in range(150):
        print("=" * 50)
        print("Iteration #: %d" % (iteration))
        # Fitting the values
        model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)

        # Time to see how our predictions fare
        # We are creating a test set from a random location in our dataset
        # In the code below, we are selecting a random input as our seed value of words
        test_idx = np.random.randint(len(input_chars))
        test_chars = input_chars[test_idx]
        print("Generating from seed: %s" % (test_chars))
        print(test_chars)
        # From the seed words, we are tasked to predict the next words
        # In the code below, we are predicting the next 100 words (NUM_PREDS_PER_EPOCH) after the seed words
        for i in range(NUM_PREDS_PER_EPOCH):
            # Pre processing the input data, just like the way we did before training the model
            Xtest = np.zeros((1, SEQLEN, total_chars))
            for i, ch in enumerate(test_chars):
                Xtest[0, i, char2index[ch]] = 1
            # Predict the next word
            pred = model.predict(Xtest, verbose=0)[0]
            # Given that, the predictions are probability values, we take the argmax to fetch the location of highest probability
            # Extract the word belonging to argmax
            ypred = index2char[np.argmax(pred)]
            print(ypred,end=' ')
            # move forward with test_chars + ypred so that we use the original 9 words + prediction for the next prediction
            test_chars = test_chars[1:] + [ypred]

    ```

初始迭代中的输出只是一个词——always！

150 次迭代结束时的输出如下(注意，下面只是部分输出):

![A463052_1_En_10_Figah_HTML.jpg](A463052_1_En_10_Figah_HTML.jpg)

前面的输出几乎没有损失。如果您在执行代码后仔细查看输出，经过一些迭代后，它会重新生成数据集中存在的精确文本—这是一个潜在的过拟合问题。另外，请注意我们输入的形状:大约 30K 个输入，其中有 3028 列。鉴于行与列的比率较低，有可能会过度匹配。随着输入样本数量的增加，这种方法可能会更有效。

拥有大量列的问题可以通过使用嵌入来解决，这与我们计算单词向量的方式非常相似。本质上，嵌入表示一个低得多的维度空间中的单词。

## RNN 的嵌入层

为了了解嵌入是如何工作的，让我们来看一个数据集，该数据集试图根据客户推文预测航空公司的客户情绪(代码在 github 中为“RNNsentiment.ipynb”):

1.  As always, import the relevant packages :

    ![A463052_1_En_10_Figai_HTML.jpg](A463052_1_En_10_Figai_HTML.jpg)

    ```
     #import relevant packages
    from keras.layers import Dense, Activation
    from keras.layers.recurrent import SimpleRNN
    from keras.models import Sequential
    from keras.utils import to_categorical
    from keras.layers.embeddings import Embedding
    from sklearn.cross_validation import train_test_split
    import numpy as np
    import nltk
    from nltk.corpus import stopwords
    import re
    import pandas as pd
    #Let us go ahead and read the dataset:
    t=pd.read_csv('/home/akishore/airline_sentiment.csv')
    t.head()

    ```

    ```
    import numpy as np
    t['sentiment']=np.where(t['airline_sentiment']=="positive",1,0)

    ```

2.  鉴于文本有噪声，我们将通过删除标点符号并将所有单词转换成小写字母来对其进行预处理:

    ```
     def preprocess(text):
        text=text.lower()
        text=re.sub('[^0-9a-zA-Z]+',' ',text)
        words = text.split()
        #words2=[w for w in words if (w not in stop)]
        #words3=[ps.stem(w) for w in words]
        words4=' '.join(words)
        return(words4)
    t['text'] = t['text'].apply(preprocess)

    ```

3.  Similar to how we developed in the previous section, we convert each word into an index value as follows:

    ![A463052_1_En_10_Figaj_HTML.jpg](A463052_1_En_10_Figaj_HTML.jpg)

    ```
    from collections import Counter
    counts = Counter()
    for i,review in enumerate(t['text']):
        counts.update(review.split())
    words = sorted(counts, key=counts.get, reverse=True)
    words[:10]

    ```

    ```
    chars = words
    nb_chars = len(words)
    word_to_int = {word: i for i, word in enumerate(words, 1)}
    int_to_word = {i: word for i, word in enumerate(words, 1)}
    word_to_int['the']
    #3
    int_to_word[3]
    #the

    ```

4.  Map each word in a review to its corresponding index :

    ![A463052_1_En_10_Figak_HTML.jpg](A463052_1_En_10_Figak_HTML.jpg)

    ```
     mapped_reviews = []
    for review in t['text']:
        mapped_reviews.append([word_to_int[word] for word in review.split()])
    t.loc[0:1]['text']

    ```

    ![A463052_1_En_10_Figal_HTML.jpg](A463052_1_En_10_Figal_HTML.jpg)

    ```
    mapped_reviews[0:2]

    ```

    Note that, the index of `virginamerica` is the same in both reviews (104).  
5.  初始化长度为 200 的零序列。请注意，我们选择了 200 作为序列长度，因为没有评论超过 200 个单词。此外，以下代码的第二部分确保对于所有小于 200 个单词的评论，所有开始的索引都被零填充，只有最后的索引被填充了与评论中出现的单词相对应的索引:

    ```
     sequence_length = 200
    sequences = np.zeros((len(mapped_reviews), sequence_length),dtype=int)
    for i, row in enumerate(mapped_reviews):
        review_arr = np.array(row)
        sequences[i, -len(row):] = review_arr[-sequence_length:]

    ```

6.  我们进一步将数据集分为训练和测试数据集，如下:

    ```
     y=t['sentiment'].values
    X_train, X_test, y_train, y_test = train_test_split(sequences, y, test_size=0.30,random_state=10)
    y_train2 = to_categorical(y_train)
    y_test2 = to_categorical(y_test)

    ```

7.  Once the datasets are in place, we go ahead and create our model, as follows. Note that embedding as a function takes in as input the total number of unique words, the reduced dimension in which we express a given word, and the number of words in an input:

    ![A463052_1_En_10_Figam_HTML.jpg](A463052_1_En_10_Figam_HTML.jpg)

    ```
     top_words=12679
    embedding_vecor_length=32
    max_review_length=200
    model = Sequential()
    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
    model.add(SimpleRNN(1, return_sequences=False,unroll=True))
    model.add(Dense(2, activation="softmax"))
    model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
    print(model.summary())
    model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=50, batch_size=1024)

    ```

现在让我们看看前面模型的汇总输出。数据集中总共有 12，679 个唯一单词。嵌入层确保我们在 32 维空间中表示每个单词，因此在嵌入层中有 405，728 个参数。

现在我们有 32 个嵌入的维度输入，每个输入现在都连接到一个隐藏层单元——因此有 32 个权重。加上这 32 个砝码，我们就会有偏差。对应于该层的最终权重将是连接先前的隐藏单位值和当前隐藏单位的权重。因此总共有 34 个砝码。

注意，假设有一个来自嵌入层的输出，我们不需要在 SimpleRNN 层中指定输入形状。一旦模型运行，输出分类准确率接近 87%。

## 传统 RNN 的问题

图 [10-5](#Fig5) 显示了一个传统的 RNN，它考虑了多个时间步长来进行预测。

![A463052_1_En_10_Fig5_HTML.png](A463052_1_En_10_Fig5_HTML.png)

图 10-5

An RNN with multiple time steps

请注意，随着时间步长的增加，来自更早层的输入对后面层的输出的影响会小得多。这可以从以下内容中看出(现在，我们将忽略偏差项):

*   h <sub>1</sub> = Wx <sub>1</sub>
*   h<sub>2</sub>= Wx<sub>2</sub>+Uh<sub>1</sub>= Wx<sub>2</sub>+UWx<sub>1</sub>
*   h<sub>3</sub>= Wx<sub>3</sub>+Uh<sub>2</sub>= Wx<sub>3</sub>+UWx<sub>2</sub>+U<sup>2</sup>Wx<sub>1</sub>
*   h<sub>4</sub>= Wx<sub>4</sub>+Uh<sub>3</sub>= Wx<sub>4</sub>+UWX<sub>3</sub>+U<sup>2</sup>WX<sub>2</sub>+U<sup>3</sup>WX<sub>1</sub>
*   h<sub>5</sub>= Wx<sub>5</sub>+Uh<sub>4</sub>= Wx<sub>5</sub>+UWX<sub>4</sub>+U<sup>2</sup>WX<sub>3</sub>+U<sup>3</sup>WX<sub>2</sub>+U<sup>4</sup>WX<sub>1</sub>

注意，随着时间戳的增加，如果 U > 1，隐藏层的值高度依赖于 X <sub>1</sub> ，如果 U < 1，则稍微依赖于 X <sub>1</sub> 。

### 消失梯度问题

U <sup>4</sup> 相对于 U 的斜率为 4 × U <sup>3</sup> 。在这种情况下，请注意，如果 U <为 1，则梯度非常小，因此，如果在更晚的时间步长的输出取决于给定时间步长的输入，则达到理想权重需要很长时间。这导致了一个问题，即在一些句子中，对时间步长中出现得更早的单词存在依赖性。比如“我来自印度。我说一口流利的 ____”在这种情况下，如果我们不考虑第一句话，第二句话的输出“我说流利的 ____”可能是任何语言的名称。因为我们在第一句中提到了国家，我们应该能够将事情缩小到特定于印度的语言。

### 爆炸梯度的问题

在前面的场景中，如果 U > 1，则梯度增加的量要大得多。这将导致在时间步长中较早出现的输入具有非常高的权重，而在我们试图预测的单词附近出现的输入具有低的权重。

因此，根据 U(隐藏层的权重)的值，权重要么更新得非常快，要么需要很长时间。

鉴于消失/爆炸梯度是一个问题，我们应该以稍微不同的方式处理 rnn。

## -什么

长短期记忆(LSTM)是一种架构，有助于克服我们之前看到的消失或爆炸梯度问题。在这一节，我们将看看 LSTM 的建筑，看看它如何帮助克服传统 RNN 的问题。

LSTM 如图 [10-6](#Fig6) 所示。

![A463052_1_En_10_Fig6_HTML.png](A463052_1_En_10_Fig6_HTML.png)

图 10-6

LSTM

注意，尽管隐藏层的输入 X 和输出(h)保持不变，但是隐藏层内发生的激活是不同的。不像传统的 RNN，有 tanh 激活，在 LSTM 有不同的激活发生。我们将逐一介绍。

在图 [10-7](#Fig7) 中，X 和 h 代表输入和隐藏层，正如我们前面看到的。

![A463052_1_En_10_Fig7_HTML.png](A463052_1_En_10_Fig7_HTML.png)

图 10-7

Various components of LSTM

c 表示单元状态。您可以将单元格状态视为获取长期依赖关系的一种方式。

f 代表了忘门:

![$$ {f}_t=\sigma \left({W}_{xf}{x}^{(t)}+{W}_{hf}{h}^{\left(t-1\right)}+{b}_f\right) $$](A463052_1_En_10_Chapter_Equb.gif)

请注意，乙状结肠给了我们一种机制来指定需要忘记什么。这样 h<sup>(t–1)</sup>中捕捉到的一些历史词汇就被选择性遗忘了。

一旦我们发现需要忘记什么，单元格状态就会更新如下:

![$$ {c}_t=\left({c}_{t-1}\otimes f\right) $$](A463052_1_En_10_Chapter_Equc.gif)

注意，![A463052_1_En_10_Figan_HTML.jpg](A463052_1_En_10_Figan_HTML.jpg)代表元素间的乘法。

想象一下，一旦我们填写了“我住在印度”中的空白。我说 ____”有了一个印度语的名字，我们就不再需要“我住在印度”的语境了。这就是“遗忘之门”有助于有选择地遗忘不再需要的信息的地方。

一旦我们弄清楚了单元格状态中需要忘记什么，我们就可以根据当前输入更新单元格状态。

在下一步中，需要更新单元状态的输入通过输入之上的 sigmoid 应用程序实现，更新幅度(正或负)通过 tanh 激活获得。

输入可以指定如下:

![$$ {i}_t=\sigma \left({W}_{xi}{x}^{(t)}+{W}_{hi}{h}^{\left(t-1\right)}+{b}_i\right) $$](A463052_1_En_10_Chapter_Equd.gif)

调制可以这样指定:

![$$ {g}_t=\tanh \left({W}_{xg}{x}^{(t)}+{W}_{hg}{h}^{\left(t-1\right)}+{b}_g\right) $$](A463052_1_En_10_Chapter_Eque.gif)

因此，单元状态最终被更新如下:

![$$ {C}^{(t)}=\left({C}^{\left(t-1\right)}\odot {f}_t\right)\oplus \left({i}_t\odot {g}_t\right) $$](A463052_1_En_10_Chapter_Equf.gif)

在最终的 gate 中，我们需要指定输入和单元格状态组合的哪一部分需要输出到下一个隐藏层:

![$$ {o}_t=\sigma \left({W}_{xo}{x}^{(t)}+{W}_{ho}{h}^{\left(t-1\right)}+{b}_o\right) $$](A463052_1_En_10_Chapter_Equg.gif)

最后隐藏的图层是这样表示的:

![$$ {h}^{(t)}={o}_t\odot \tanh \left({C}^{(t)}\right) $$](A463052_1_En_10_Chapter_Equh.gif)

鉴于细胞状态可以记住稍后需要的值，LSTM 在预测下一个单词方面提供了比传统 RNN 更好的结果，通常是在情感分类方面。这在有长期依赖关系需要处理的场景中特别有用。

## 在 keras 实施基本 LSTM

为了了解到目前为止提出的理论如何转化为行动，让我们重新看看我们之前看到的玩具示例(在 github 中代码为“LSTM 玩具示例. ipynb”):

1.  导入相关包:

    ```
    from keras.preprocessing.text import one_hot
    from keras.preprocessing.sequence import pad_sequences
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Flatten
    from keras.layers.recurrent import SimpleRNN
    from keras.layers.embeddings import Embedding
    from keras.layers import LSTM
    import numpy as np

    ```

2.  定义文件和标签:

    ```
    # define documents
    docs = ['very good',
                 'very bad']
    # define class labels
    labels = [1,0]

    ```

3.  One-hot-encode the documents:

    ![A463052_1_En_10_Figao_HTML.jpg](A463052_1_En_10_Figao_HTML.jpg)

    ```
    from collections import Counter
    counts = Counter()
    for i,review in enumerate(docs):
        counts.update(review.split())
    words = sorted(counts, key=counts.get, reverse=True)
    vocab_size=len(words)
    word_to_int = {word: i for i, word in enumerate(words, 1)}
    encoded_docs = []
    for doc in docs:
        encoded_docs.append([word_to_int[word] for word in doc.split()])
    encoded_docs

    ```

4.  Pad documents to a maximum length of two words :

    ![A463052_1_En_10_Figap_HTML.jpg](A463052_1_En_10_Figap_HTML.jpg)

    ```
    max_length = 2
    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding="pre")
    print(padded_docs)

    ```

5.  Build the model :

    ![A463052_1_En_10_Figaq_HTML.jpg](A463052_1_En_10_Figaq_HTML.jpg)

    ```
    model = Sequential()
    model.add(LSTM(1,activation='tanh', return_sequences=False,recurrent_initializer='Zeros',recurrent_activation='sigmoid',
                   input_shape=(2,1),unroll=True))
    model.add(Dense(1, activation="sigmoid"))
    model.compile(optimizer='adam', loss="binary_crossentropy", metrics=['acc'])
    print(model.summary())

    ```

请注意，在前面的代码中，我们将递归初始化器和递归激活初始化为某些值，只是为了让这个玩具示例在 Excel 中实现时更容易理解。目的是帮助您理解后端发生了什么。

一旦模型如所讨论的那样被初始化，让我们继续拟合模型:

![A463052_1_En_10_Figar_HTML.jpg](A463052_1_En_10_Figar_HTML.jpg)

```
model.fit(padded_docs.reshape(2,2,1),np.array(labels).reshape(max_length,1),epochs=500)

```

该模型的各层如下。这里是`model.layers`:

![A463052_1_En_10_Figas_HTML.jpg](A463052_1_En_10_Figas_HTML.jpg)

权重和权重的顺序可以如下获得:

![A463052_1_En_10_Figaw_HTML.jpg](A463052_1_En_10_Figaw_HTML.jpg)

```
model.layers[1].trainable_weights

```

![A463052_1_En_10_Figav_HTML.jpg](A463052_1_En_10_Figav_HTML.jpg)

```
model.layers[1].get_weights()

```

![A463052_1_En_10_Figau_HTML.jpg](A463052_1_En_10_Figau_HTML.jpg)

```
model.layers[0].trainable_weights

```

![A463052_1_En_10_Figat_HTML.jpg](A463052_1_En_10_Figat_HTML.jpg)

```
model.layers[0].get_weights()

```

从前面的代码中，我们可以看到，首先获得输入的权重(`kernel`)，然后是对应于隐藏层的权重(`recurrent_kernel`)，最后是 LSTM 层中的偏差。

同样，在`dense`层(连接隐藏层和输出的层)中，要与隐藏层相乘的权重排在第一位，其次是偏差。

另请注意，权重和偏差在 LSTM 图层中出现的顺序如下:

1.  输入门
2.  忘记大门
3.  调制门(单元门)
4.  输出门

现在我们有了输出，让我们继续计算输入的预测。注意，就像上一节一样，我们使用原始编码输入(1，2，3)而不对它们进行进一步处理，只是为了看看计算是如何进行的。

在实践中，我们将进一步处理输入，可能将它们编码成向量以获得预测，但在本例中，我们感兴趣的是通过在 Excel 中复制 LSTM 的预测来巩固我们对 LSTM 如何运作的知识:

![A463052_1_En_10_Figax_HTML.jpg](A463052_1_En_10_Figax_HTML.jpg)

```
padded_docs[1].reshape(1,2,1)

```

![A463052_1_En_10_Figay_HTML.jpg](A463052_1_En_10_Figay_HTML.jpg)

```
model.predict(padded_docs[1].reshape(1,2,1))

```

现在，我们已经从模型中获得了 0.4485 的预测概率，让我们在 Excel(github 中以“LSTM 工作细节. xlsx”的名称提供)中手工计算这些值:

![A463052_1_En_10_Figaz_HTML.jpg](A463052_1_En_10_Figaz_HTML.jpg)

注意，这里的值取自 keras 的`model.layers[0].get_weights()`输出。

在继续计算各个门的值之前，请注意，我们已经将 recurrent layer (h <sub>t-1</sub> )的值初始化为 0。在第一个时间步长中，输入值为 1。让我们计算一下各个关口的价值:

![A463052_1_En_10_Figba_HTML.jpg](A463052_1_En_10_Figba_HTML.jpg)

获得上述输出的计算如下:

![A463052_1_En_10_Figbb_HTML.jpg](A463052_1_En_10_Figbb_HTML.jpg)

现在已经计算了各个门的所有值，我们将计算输出(隐藏层):

![A463052_1_En_10_Figbc_HTML.jpg](A463052_1_En_10_Figbc_HTML.jpg)

刚刚显示的隐藏层值是输入为 1 的时间步长的隐藏层输出。

现在，我们将继续计算输入为 2 时的隐藏层值(这是我们之前在代码中预测的数据点的第二个时间步长的输入):

![A463052_1_En_10_Figbd_HTML.jpg](A463052_1_En_10_Figbd_HTML.jpg)

让我们看看如何获得第二个输入的各种门和隐藏层的值。这里需要注意的关键是，第一个时间步长输出的隐藏层是第二个输入中所有门的计算输入:

![A463052_1_En_10_Figbe_HTML.jpg](A463052_1_En_10_Figbe_HTML.jpg)

最后，假设我们已经计算了第二时间步的隐藏层输出，我们计算输出如下:

![A463052_1_En_10_Figbf_HTML.jpg](A463052_1_En_10_Figbf_HTML.jpg)

前面计算的最终输出如下所示:

![A463052_1_En_10_Figbg_HTML.jpg](A463052_1_En_10_Figbg_HTML.jpg)

请注意，我们得到的输出与我们在 keras 输出中看到的相同。

## 实现用于情感分类的 LSTM

在最后一节中，我们使用 keras 中的 RNN 实现了情感分类。在这一节中，我们将看看如何使用 LSTM 来实现这一点。我们在上面看到的代码中唯一的变化将是模型编译部分，我们将使用 LSTM 代替`SimpleRNN`——其他一切都将保持不变(代码可在 github 的“RNN 情绪. ipynb”文件中获得):

```
top_words=nb_chars
embedding_vecor_length=32
max_review_length=200
model = Sequential()
model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))
model.add(LSTM(10))
model.add(Dense(2, activation="softmax"))
model.compile(loss='categorical_crossentropy', optimizer="adam", metrics=['accuracy'])
print(model.summary())
model.fit(X_train, y_train2, validation_data=(X_test, y_test2), epochs=50, batch_size=1024)

```

一旦你实现了这个模型，你会发现 LSTM 的预测精度比 RNN 的略高。实际上，对于我们之前看到的数据集，LSTM 给出了 91%的准确率，而 RNN 给出了 87%的准确率。这可以通过调整函数提供的各种超参数来进一步微调。

## 在 R 中实现 RNN

为了了解如何在 R 中实现 RNN/LSTM，我们将使用随`kerasR`包一起预构建的 IMDB 情感分类数据集(在 github 中代码为“kerasR_code_RNN.r”):

```
# Load the dataset
library(kerasR)
imdb <- load_imdb(num_words = 500, maxlen = 100)

```

注意，我们通过将`num_words`指定为一个参数，只获取前 500 个单词。我们也只获取那些长度不超过 100 字的 IMDB 评论。

让我们探索一下数据集的结构:

```
str(imdb)

```

我们应该注意到，在随`kerasR`包一起提供的预构建的 IMDB 数据集中，每个单词都被它默认表示的索引所替换。因此我们不必执行单词到索引的映射步骤:

```
# Build the model with an LSTM
model <- Sequential()

model$add(Embedding(500, 32, input_length = 100, input_shape = c(100)))

model$add(LSTM(32)) # Use SimpleRNN, if we were to perform a RNN function
model$add(Dense(256))
model$add(Activation('relu'))
model$add(Dense(1))
model$add(Activation('sigmoid'))
# Compile and fit the model
keras_compile(model,  loss = 'binary_crossentropy', optimizer = Adam(),metrics='binary_accuracy')
keras_fit(model, X_train, Y_train, batch_size = 1024, epochs = 50, verbose = 1,validation_data = list(X_test,Y_test))

```

上述结果使测试数据集预测的准确率接近 79%。

## 摘要

在本章中，您学习了以下内容:

*   rnn 在处理具有时间相关性的数据时非常有用。
*   在处理数据的长期依赖性时，rnn 面临梯度消失或爆炸的问题。
*   在这种情况下，LSTM 和其他最新的建筑就派上了用场。
*   LSTM 的工作原理是将信息存储在单元状态中，忘记不再有帮助的信息，根据当前输入选择信息以及需要添加到单元状态的信息量，最后选择需要输出到下一个状态的信息。