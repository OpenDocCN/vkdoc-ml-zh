# 4.决策图表

在前几章中，我们已经考虑了基于回归的算法，这些算法通过改变系数或权重来优化某个指标。决策树形成了基于树的算法的基础，该算法帮助识别规则以分类或预测我们感兴趣的事件或变量。此外，与针对回归或分类进行优化的线性或逻辑回归不同，决策树能够同时执行这两种操作。

决策树的主要优势来自于它们对业务用户友好的事实——也就是说，决策树的输出对业务用户来说是直观且易于解释的。

在本章中，我们将学习以下内容:

*   决策树如何在分类和回归练习中工作
*   当自变量是连续或离散时，决策树如何工作
*   提出最佳决策树所涉及的各种技术
*   各种超参数对决策树的影响
*   如何在 Excel、Python 和 R 中实现决策树

决策树是一种算法，有助于对事件进行分类或预测变量的输出值。您可以将决策树想象为一组规则，基于这些规则可以预期不同的结果。比如看图 [4-1](#Fig1) 。

![A463052_1_En_4_Fig1_HTML.jpg](A463052_1_En_4_Fig1_HTML.jpg)

图 4-1

An example decision tree

在图 [4-1](#Fig1) 中，我们可以看到一个数据集(左边的表格)同时使用连续变量(应税收入)和分类变量(退税、婚姻状况)作为独立变量来分类某人是否在偷税漏税(分类因变量)。

右边的树有几个组成部分:根节点、决策节点和叶节点(我将在下一节详细讨论这些)来分类某人是否会作弊(是/否)。

从所示的树中，用户可以导出以下规则:

1.  婚姻状况为“是”的人通常不是骗子。
2.  一个离了婚但较早拿到退款的人也不会出轨。
3.  一个离了婚，没有得到退款，但应税收入少于 80K 的人也不是骗子。
4.  那些不属于任何上述类别的人在这个特定的数据集中是骗子。

与回归相似，我们推导出一个等式(例如，根据客户特征预测信用违约)，决策树也可以根据客户特征(例如，上例中的婚姻状况、退款和应税收入)来预测或预报事件。

当一个新客户申请信用卡时，规则引擎(在后端运行的决策树)将检查该客户在通过决策树的所有规则后是属于风险类别还是非风险类别。通过规则后，系统将根据用户所处的阶段批准或拒绝信用卡。

决策树的明显优势是直观的输出和可视化，可以帮助业务用户做出决策。在分类的情况下，决策树比典型的回归技术对异常值更不敏感。此外，就构建模型、解释模型甚至实现模型而言，决策树是最简单的算法之一。

## 决策树的组成部分

决策树的所有组成部分如图 [4-2](#Fig2) 所示。

![A463052_1_En_4_Fig2_HTML.png](A463052_1_En_4_Fig2_HTML.png)

图 4-2

Components of a decision tree

这些组件包括以下内容:

*   根节点:该节点代表整个总体或样本，并被分成两个或多个同类集合。
*   拆分:根据一定的规则将一个节点划分为两个或多个子节点的过程。
*   决策节点:当一个子节点分裂成更多的子节点时，它被称为决策节点。
*   叶/终端节点:决策树中的最后一个节点。
*   修剪:从决策节点中删除子节点的过程，与拆分相反。
*   分支/子树:整个树的一个子部分称为分支或子树。
*   父节点和子节点:被划分为子节点的节点称为这些子节点的父节点，子节点是父节点的子节点。

## 存在多个离散自变量时的分类决策树

根据因变量是连续变量还是分类变量，在根节点进行分割的标准因我们预测的变量类型而异。在这一节中，我们将通过一个例子来看看从根节点到决策节点的分裂是如何发生的。在这个例子中，我们试图根据几个独立变量(教育、婚姻状况、种族和性别)来预测雇员的工资(`emp_sal`)。

以下是数据集(在 github 中以“分类相关和独立变量. xlsx”的形式提供):

![A463052_1_En_4_Figa_HTML.jpg](A463052_1_En_4_Figa_HTML.jpg)

这里，`Emp_sal`为因变量，其余变量为自变量。

分割根节点(原始数据集)时，首先需要确定进行第一次分割所基于的变量，例如，是否基于教育程度、婚姻状况、种族或性别进行分割。为了想出一种方法来筛选出一个独立变量，我们使用信息增益标准。

### 信息增益

将信息增益与不确定性联系起来可以更好地理解它。让我们假设有两个政党在两个不同的州竞选。在一个州，双方获胜的机会是 50:50，而在另一个州，甲方获胜的机会是 90%，而乙方获胜的机会是 10%。

如果我们要预测选举的结果，后一种状态比前一种状态更容易预测，因为在这种状态下不确定性最小(甲方获胜的概率为 90%)。因此，信息增益是分裂节点后不确定性的度量。

### 计算不确定性:熵

不确定性，也称为熵，由公式

![$$ -\left( plo{g}_2p+ qlo{g}_2q\right) $$](A463052_1_En_4_Chapter_Equa.gif)

来度量

其中 p 是事件 1 发生的概率，q 是事件 2 发生的概率。

让我们考虑一下双方的双赢方案:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 方案 | 甲方不确定性 | 乙方不确定性 | 总体不确定性 |
| :-- | :-- | :-- | :-- |
| 获胜的机会均等 | 0.5log <sub>2</sub> (0.5) = 0.5 | 0.5log <sub>2</sub> (0.5) = 0.5 | 0.5 + 0.5 =1 |
| 甲方有 90%的胜算 | 0.9log <sub>2</sub> (0.9) = 0.1368 | 0.1log <sub>2</sub> (0.1) = 0.3321 | 0.1368 + 0.3321 = 0.47 |

我们看到，根据前面的等式，第二种情况比第一种情况具有更少的总体不确定性，因为第二种情况有 90%的机会使甲方获胜。

### 计算信息增益

我们可以把根节点想象成存在最大不确定性的地方。随着我们明智地进一步分裂，不确定性会减少。因此，选择分割(分割应基于的变量)取决于哪些变量最能减少不确定性。

为了查看计算是如何进行的，让我们基于数据集构建一个决策树。

### 原始数据集中的不确定性

在原始数据集中，九个观测值的薪水为<= 50K, while five have a salary > 50K:

![A463052_1_En_4_Figb_HTML.jpg](A463052_1_En_4_Figb_HTML.jpg)

让我们计算 p 和 q 的值，以便计算总不确定度:

![A463052_1_En_4_Figc_HTML.jpg](A463052_1_En_4_Figc_HTML.jpg)

p 和 q 的公式如下:

![A463052_1_En_4_Figd_HTML.jpg](A463052_1_En_4_Figd_HTML.jpg)

因此，根节点的总体不确定性如下:

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| 不确定度< =50K | 不确定性大于 50K | 总体不确定性 |
| –0.64×log<sub>2</sub>(0.64)= 0.41 | 0.36 × log <sub>2</sub> (0.36) = 0.53 | 0.41 + 0.53 = 0.94 |

根节点中的总体不确定性为 0.94。

为了完成第一步，查看筛选变量的过程，我们将计算出如果我们在第一次拆分中考虑所有四个独立变量，总体不确定性降低的量。我们将考虑第一次分裂的教育(我们将计算不确定性的改善)，接下来我们将以同样的方式考虑婚姻状况，然后是种族，最后是员工的性别。最能减少不确定性的变量将是我们在第一次分割时应该使用的变量。

### 测量不确定度的改善

要了解不确定性的改善是如何计算的，请考虑以下示例。让我们考虑一下是否要按员工的性别来划分变量:

![A463052_1_En_4_Fige_HTML.jpg](A463052_1_En_4_Fige_HTML.jpg)

我们计算每个变量的每个不同值的不确定性—(plog<sub>2</sub>p+qlog<sub>2</sub>q)。其中一个变量(`Sex`)的不确定度计算表如下:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 性 | P | Q | ——(plog〔t0〕2〔t1〕p) | -(qlog<sub>2</sub>q) | ——(plog<sub>2</sub>p+qlog<sub>2</sub>q) | 加权不确定性 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 女性的 | 4/5 | 1/5 | Zero point two five seven | Zero point four six | Zero point seven two | 0.72 × 5/14 = 0.257 |
| 男性的 | 5/9 | 4/9 | Zero point four seven one | Zero point five two | Zero point nine nine | 0.99 × 9/14 = 0.637 |
| 全部的 |   |   |   |   |   | Zero point eight nine four |

将进行类似的计算来测量所有变量的总体不确定性。如果我们用变量`Sex`分割根节点，信息增益如下:

(原始熵-熵，如果我们除以变量`Sex`)= 0.94–0.894 = 0.046

基于总的不确定性，最大化信息增益(不确定性的减少)的变量将被选择用于分裂树。

在我们的示例中，变量方面的总体不确定性如下:

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| 可变的 | 总体不确定性 | 从根节点减少不确定性 |
| :-- | :-- | :-- |
| 教育 | Zero point six seven nine | 0.94 – 0.679 = 0.261 |
| 婚姻状况 | Zero point eight zero three | 0.94 – 0.803 = 0.137 |
| 人种 | Zero point eight zero three | 0.94 – 0.803 = 0.137 |
| 性 | Zero point eight nine four | 0.94 – 0.894 = 0.046 |

由此我们可以观察到，分割决策应该基于`Education`而不是任何其他变量，因为它是最大程度地降低总体不确定性的变量(从 0.94 到 0.679)。

一旦做出了关于分割的决定，下一步(对于具有两个以上不同值的变量—在我们的示例中为 education)将是确定哪个唯一值应该进入右决策节点，哪个唯一值应该进入根节点之后的左决策节点。

让我们看看教育的所有独特价值，因为它是最能减少不确定性的变量:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 独特的价值 | obs 的百分比。< =50K |
| :-- | :-- |
| 第 11 | 100% |
| 第九届 | 100% |
| Assoc-acdm | 100% |
| 学士 | 67% |
| HS 度 | 50% |
| 主人 | 50% |
| 某大学 | 0% |
| 全部的 | 64% |

### 哪些不同的值位于左侧和右侧节点

在前面的部分中，我们得出结论，教育是第一个分裂的变量。下一个要做的决定是，哪些不同的教育价值观属于左边的节点，哪些不同的价值观属于右边的节点。

在这种情况下，基尼系数就派上了用场。

#### 基尼杂质

基尼系数是指一个节点内的不平等程度。如果一个节点的所有值都属于一个类而不属于另一个类，那么它就是最纯粹的节点。如果一个节点有一个类的 50%的观测值，其余的是另一个类的观测值，那么它就是节点最不纯的形式。

基尼系数被定义为![$$ 1-\left({p}^2+{q}^2\right) $$](A463052_1_En_4_Chapter_IEq1.gif)，其中 p 和 q 是与每一类相关的概率。

考虑以下场景:

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| P | Q | 基尼指数值 |
| :-- | :-- | :-- |
| Zero | one | 1–0<sup>2</sup>–1<sup>2</sup>= 0 |
| one | Zero | 1–1<sup>2</sup>–0<sup>2</sup>= 0 |
| Zero point five | Zero point five | 1–0.5<sup>2</sup>–0.5<sup>2</sup>= 0.5 |

我们用基尼杂质来做员工工资预测问题:从上一节的信息增益计算中，我们观察到`Education`是用作第一次拆分的变量。

为了确定哪些不同的值进入左侧节点，哪些进入右侧节点，让我们进行以下计算:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
|   | 观察次数 |   | 左侧节点 | 右节点 | 不纯 | obs 的数量 |   |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|   | < =50K | > 50K | 大共计 | p | q | p | q | 左侧节点 | 右节点 | 左侧节点 | 右节点 | 加权杂质 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 第 11 | one |   | one | 100% | 0% | 62% | 38% | - | Zero point four seven | one | Thirteen | Zero point four four |
| 第九届 | one |   | one | 100% | 0% | 58% | 42% | - | Zero point four nine | Two | Twelve | Zero point four two |
| Assoc-acdm | one |   | one | 100% | 0% | 55% | 45% | - | Zero point five | three | Eleven | Zero point three nine |
| 学士 | four | Two | six | 78% | 22% | 40% | 60% | Zero point three five | Zero point four eight | nine | five | Zero point three nine |
| HS 度 | one | one | Two | 73% | 27% | 33% | 67% | Zero point four | Zero point four four | Eleven | three | Zero point four one |
| 主人 | one | one | Two | 69% | 31% | 0% | 100% | Zero point four three | - | Thirteen | one | Zero point four |
| 某大学 |   | one | one |   |   |   |   |   |   |   |   |   |

我们可以通过以下步骤来理解上表:

1.  等级按属于某一类的观察值的百分比对不同的值进行排序。这将导致不同的值被重新排序如下:第 11

    <colgroup><col align="left"> <col align="left"></colgroup> 
    | 独特的价值 | obs 的百分比< = 50K |
    | :-- | :-- |
    |  |
    | 100%第九届 |  |
    | 100% |  |
    | 学士 | 67% |
    | HS 度 | 50% |
    | 主人 | 50% |
    | 某大学 |  |

    0%
2.  第一步，我们假设只有对应于`11th`的 distinct 值到左节点，其余的 distinct 观察值对应于右节点。左侧节点中的杂质为 0，因为它只有一个观测值，右侧节点中会有一些杂质，因为八个观测值属于一个类，五个属于另一个类。
3.  总杂质计算如下:((obs 左侧节点中的杂质× #)。左侧节点中)+(右侧节点中的杂质×# OBS。右侧节点中)obs 总数。)
4.  我们重复第 2 步和第 3 步，但是这一次我们在左侧节点中包含了`11th`和`9th`，在右侧节点中包含了其余的不同值。
5.  重复该过程，直到在左节点中考虑了所有不同的值。
6.  具有最小总加权杂质的组合是将在左节点和右节点中选择的组合。

在我们的示例中，{ `11th,9th,Assoc-acdm` }的组合去往左侧节点，其余的去往右侧节点，因为该组合具有最小的加权杂质。

#### 进一步分裂子节点

根据目前的分析，我们将原始数据分为以下几部分:

![A463052_1_En_4_Figf_HTML.jpg](A463052_1_En_4_Figf_HTML.jpg)

现在有机会进一步拆分正确的节点。让我们看看下一个分裂的决定是怎样的。要分割的数据是属于右侧节点的所有数据点，如下所示:

![A463052_1_En_4_Figg_HTML.png](A463052_1_En_4_Figg_HTML.png)

根据前面的数据，我们将执行以下步骤:

1.  使用信息增益度量来确定用于分割数据的变量。
2.  Use the Gini index to figure out the distinct values within that variable that should belong to the left node and the ones that should belong to the right node. The overall impurity in the parent node for the preceding dataset is as follows:

    ![A463052_1_En_4_Figi_HTML.jpg](A463052_1_En_4_Figi_HTML.jpg)

    ![A463052_1_En_4_Figh_HTML.jpg](A463052_1_En_4_Figh_HTML.jpg)

现在整体杂质是~0.99，让我们看看减少整体杂质最多的变量。我们将经历的步骤将与对整个数据集的上一次迭代相同。请注意，当前版本与先前版本之间的唯一区别在于，在根节点中考虑了整个数据集，而在子节点中仅考虑了数据的子集。

让我们分别计算每个变量获得的信息增益(类似于上一节我们计算的方式)。以婚姻状况为变量的总杂质计算如下:

![A463052_1_En_4_Figj_HTML.jpg](A463052_1_En_4_Figj_HTML.jpg)

同样，与员工种族相关的杂质如下:

![A463052_1_En_4_Figk_HTML.jpg](A463052_1_En_4_Figk_HTML.jpg)

与性别相关的杂质计算如下:

![A463052_1_En_4_Figl_HTML.jpg](A463052_1_En_4_Figl_HTML.jpg)

与员工教育相关的杂质计算如下:

![A463052_1_En_4_Figm_HTML.jpg](A463052_1_En_4_Figm_HTML.jpg)

从上面，我们注意到员工教育程度作为一个变量是从父节点减少杂质最多的一个，也就是说，从员工教育程度变量获得的信息增益最高。

注意，巧合的是，同一个变量在父节点和子节点中两次拆分了数据集。这种模式可能不会在不同的数据集上重复。

### 分裂过程什么时候停止？

理论上，分裂过程可以发生，直到决策树的所有终端(叶/最后)节点都是纯的(它们都属于一个类或另一个类)。

然而，这种方法的缺点是它过度拟合数据，因此可能不具有普遍性。因此，决策树是树的复杂性(树中终端节点的数量)和准确性之间的折衷。在终端节点较多的情况下，训练数据的准确率可能较高，但验证数据的准确率可能不高。

这让我们想到了树的复杂性参数和开箱验证的概念。随着树的复杂性增加，也就是说，树的深度变得更高，训练数据集的准确性会不断增加，但测试数据集的准确性可能会在超过树的特定深度后开始变低。

分割过程应在验证数据集精度不再提高时停止。

## 连续自变量的分类决策树

到目前为止，我们认为自变量和因变量都是分类变量。但实际上，我们可能也在处理连续变量，作为独立变量。本节讨论如何为连续变量构建决策树。

在接下来的章节中，我们将使用以下数据集(github 中的“分类相关连续独立变量. xlsx ”)来研究如何为分类相关变量和连续独立变量构建决策树:

![A463052_1_En_4_Fign_HTML.png](A463052_1_En_4_Fign_HTML.png)

在这个数据集中，我们将尝试根据`Age`变量来预测某人是否会幸存。

因变量为`Survived`，自变量为`Age`。

1.  Sort the dataset by increasing independent variable. The dataset thus transforms into the following:

    ![A463052_1_En_4_Figo_HTML.png](A463052_1_En_4_Figo_HTML.png)

2.  测试多个规则。例如，当年龄小于 7、小于 10 等等直到年龄小于 94 时，我们可以测试左右节点中的杂质。
3.  计算基尼杂质:T29】左侧节点 T31T33】右节点 T35T37】总杂质T49】右节点 T51】pT53】qT55】不纯 十六 零点二四 【T3337】55%

    <colgroup><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"></colgroup>
    |  |  |  | OBS 的数量 |
    | :-- | :-- | :-- | :-- |
    | T43】幸存 T45】幸存人数 2 | 左侧节点 | p | q | 不纯 |
    | :-- | :-- | :-- | :-- | :-- |
    | 三 | 一 | 一 |  | 十一 |  |  |  |  |  |  |  |
    | 七 | 一 | 一 |  | 三 | 九 | 100% | 0% | - | 33% | 67% | 零点四四 | 零点三六 |
    | 十五 | 一 | 一 | 五 | 七 | 100% | 0% | - | 14% | 86% | 零点二四 | 零点一六 | 二十六 | 零点 | 一 | 七 | 五 | 86% | 14% | 零点二四 | 0% | 100% | - |
    | 三十四 | 零点 | 一 | 九 | 三 | 67% | 33% | 零点四四 | 0% | 100% |  | - | 零点三九 |
    | 七十六 | 零 | 一 | 十一 | 一 | 45% | 零 |  |  |

    从上表中我们应该注意到，当自变量取值小于 26 时，基尼杂质最少。 因此，我们将选择小于 26 的`Age`作为分割原始数据集的规则。注意，`Age` > 20 和`Age` < 26 将数据集分割到相同程度的错误率。在这种情况下，我们需要想出一种方法在两个规则中选择一个规则。我们将取这两个规则的平均值，因此`Age` < = 23 将是介于这两个规则之间的规则，因此比这两个规则中的任何一个都好。

## 有多个自变量时的分类决策树

为了了解当多个独立变量连续时决策树是如何工作的，我们来看一下下面的数据集(github 中的“分类相关的多个连续独立变量. xlsx”):

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| 幸存 | 年龄 | 未知的 |
| :-- | :-- | :-- |
| one | Thirty | Seventy-nine |
| one | seven | Sixty-seven |
| one | One hundred | Fifty-three |
| one | Fifteen | Thirty-three |
| one | Sixteen | Thirty-two |
| one | Twenty | five |
| Zero | Twenty-six | Fourteen |
| Zero | Twenty-eight | Sixteen |
| Zero | Thirty-four | Seventy |
| Zero | Sixty-two | Thirty-five |
| Zero | Seventy-six | Sixty-six |
| Zero | Ninety-four | Twenty-two |

到目前为止，我们已经按照以下顺序进行了计算:

1.  通过使用信息增益，首先确定应该用于分割的变量。
2.  一旦确定了变量，在离散变量的情况下，确定应该属于左节点和右节点的唯一值。
3.  在连续变量的情况下，测试所有的规则，并列出导致最小总杂质的规则。

在这种情况下，我们将反转场景—也就是说，如果我们要对任何变量进行拆分，我们将首先找出拆分到左侧节点和右侧节点的规则。一旦确定了左右节点，我们将计算通过分割获得的信息增益，从而列出应该分割整个数据集的变量。

首先，我们将计算两个变量的最优分割。我们将从第一个变量`Age`开始:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
|   |   |   | obs 的数量 | 左侧节点 |   | 右节点 |   |   |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 年龄 | 幸存 | 幸存人数 2 | 左侧节点 | 右节点 | p | q | 不纯 | p | q | 不纯 | 总杂质 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| seven | one | one |   | Eleven |   |   |   |   |   |   |   |
| Fifteen | one | one | Two | Ten | 100% | 0% | - | 40% | 60% | Zero point four eight | Zero point four |
| Sixteen | one | one | three | nine | 100% | 0% | - | 33% | 67% | Zero point four four | Zero point three three |
| Twenty | one | one | four | eight | 100% | 0% | - | 25% | 75% | Zero point three eight | Zero point two five |
| Twenty-six | Zero | one | five | seven | 80% | 20% | Zero point three two | 29% | 71% | Zero point four one | Zero point three seven |
| Twenty-eight | Zero | one | six | six | 67% | 33% | Zero point four four | 33% | 67% | Zero point four four | Zero point four four |
| Thirty | one | one | seven | five | 71% | 29% | Zero point four one | 20% | 80% | Zero point three two | Zero point three seven |
| Thirty-four | Zero | one | eight | four | 63% | 38% | Zero point four seven | 25% | 75% | Zero point three eight | Zero point four four |
| Sixty-two | Zero | one | nine | three | 56% | 44% | Zero point four nine | 33% | 67% | Zero point four four | Zero point four eight |
| Seventy-six | Zero | one | Ten | Two | 50% | 50% | Zero point five | 50% | 50% | Zero point five | Zero point five |
| Ninety-four | Zero | one | Eleven | one | 45% | 55% | Zero point five | 100% | 0% | - | Zero point four five |
| One hundred | one | one | Twelve |   |   |   |   |   |   |   |   |

<colgroup><col align="left"> <col align="left"></colgroup> 
| 年龄 | 幸存 |
| :-- | :-- |
| seven | one |
| Fifteen | one |
| Sixteen | one |
| Twenty | one |
| Twenty-six | Zero |
| Twenty-eight | Zero |
| Thirty | one |
| Thirty-four | Zero |
| Sixty-two | Zero |
| Seventy-six | Zero |
| Ninety-four | Zero |
| One hundred | one |

从数据中我们可以看出，推导出的规律应该是`Age` < = 20 或者`Age` > = 26。所以我们还是用中间值:`Age` < = 23。

现在我们已经推导出了规则，让我们来计算与分割相对应的信息增益。在计算信息增益之前，我们将计算原始数据集中的熵:

![A463052_1_En_4_Figp_HTML.jpg](A463052_1_En_4_Figp_HTML.jpg)

假设 0 和 1 都是 6(各有 50%的概率)，总熵就是 1。

根据下面的内容，我们注意到，如果我们首先使用`Age`变量分割数据集，熵从值 1 减少到 0.54:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
|   | 幸存 |   |   |   |   |   |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|   | Zero | one | 大共计 | p | q | -(plogp+qlogq) | 加权熵 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 年龄< =23 岁 |   | four | four | Zero | one | Zero | Zero |
| 年龄> 23 岁 | six | Two | eight | Zero point seven five | Zero point two five | 0.811278 | 0.540852 |
| 大共计 | six | six | Twelve |   | 总熵 |   | 0.540852 |

类似地，如果我们按照名为`Unknown`的列分割数据集，当`Unknown` <的值= 22 时出现最小值，如下所示:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
|   |   |   | obs 的数量 | 左侧节点 |   | 右节点 |   |   |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 未知的 | 幸存 | 幸存人数 2 | 左侧节点 | 右节点 | p | q | 不纯 | p | q | 不纯 | 总杂质 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| five | one | one |   | Eleven |   |   |   |   |   |   |   |
| Fourteen | Zero | one | Two | Ten | 50% | 50% | Zero point five | 50% | 50% | Zero point five | Zero point five |
| Sixteen | Zero | one | three | nine | 33% | 67% | Zero point four four | 56% | 44% | Zero point four nine | Zero point four eight |
| Twenty-two | Zero | one | four | eight | 25% | 75% | Zero point three eight | 63% | 38% | Zero point four seven | Zero point four four |
| Thirty-two | one | one | five | seven | 40% | 60% | Zero point four eight | 57% | 43% | Zero point four nine | Zero point four nine |
| Thirty-three | one | one | six | six | 50% | 50% | Zero point five | 50% | 50% | Zero point five | Zero point five |
| Thirty-five | Zero | one | seven | five | 43% | 57% | Zero point four nine | 60% | 40% | Zero point four eight | Zero point four nine |
| Fifty-three | one | one | eight | four | 50% | 50% | Zero point five | 50% | 50% | Zero point five | Zero point five |
| Sixty-six | Zero | one | nine | three | 44% | 56% | Zero point four nine | 67% | 33% | Zero point four four | Zero point four eight |
| Sixty-seven | one | one | Ten | Two | 50% | 50% | Zero point five | 50% | 50% | Zero point five | Zero point five |
| Seventy | Zero | one | Eleven | one | 45% | 55% | Zero point five | 100% | 0% | - | Zero point four five |
| Seventy-nine | one | one | Twelve |   |   |   |   |   |   |   |   |

因此，所有小于或等于 22 的值属于一个组(左节点)，其余的属于另一个组(右节点)。注意，实际上我们会选择 22 到 32 之间的中间值。

在通过`Unknown`变量进行分割的情况下，总熵如下:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
|   | 幸存 |   |   |   |   |
| :-- | :-- | :-- | :-- | :-- | :-- |
|   | Zero | one | 大共计 | p | q | -(plogp+qlogq)我们给出了熵 |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- |
| 未知< =22 | three | one | four | Zero point seven five | Zero point two five | Zero point eight one | Zero point two seven |
| 未知> 22 | three | five | eight | Zero point three seven five | Zero point six two five | Zero point nine five | Zero point six four |
| 大共计 | six | six | Twelve |   | 总熵 | Zero point nine one |

从数据中我们看到，由于被`Unknown`变量分割，信息增益仅为 0.09。因此，分割将基于`Age`，而不是`Unknown`。

## 存在连续和离散自变量时的分类决策树

我们已经看到了当所有独立变量都是连续的和当所有独立变量都是离散的时候，构建分类决策树的方法。

如果一些独立变量是连续的，其余的是离散的，那么我们构建决策树的方式与我们在前面章节中构建的方式非常相似:

1.  对于连续的自变量，我们计算最优分裂点。
2.  一旦计算出最佳分裂点，我们就计算与之相关的信息增益。
3.  对于离散变量，我们通过计算基尼系数来确定独立变量中不同值的分组。
4.  最大化信息增益的变量是首先分裂决策树的变量。
5.  我们继续前面的步骤，进一步构建树的子节点。

## 如果响应变量是连续的呢？

如果响应变量是连续的，我们在上一节中构建决策树的步骤保持不变，只是我们不是计算基尼系数杂质或信息增益，而是计算平方误差(类似于我们在回归技术中最小化平方误差和的方法)。减少数据集总均方误差的变量将是分割数据集的变量。

为了查看决策树在连续因变量和自变量的情况下如何工作，我们将通过以下数据集作为示例(在 github 中以“连续变量因变量和自变量. xlsx”的形式提供):

<colgroup><col align="left"> <col align="left"></colgroup> 
| 可变的 | 反应 |
| :-- | :-- |
| -0.37535 | One thousand five hundred and ninety |
| -0.37407 | Two thousand three hundred and nine |
| -0.37341 | Eight hundred and fifteen |
| -0.37316 | Two thousand two hundred and twenty-nine |
| -0.37263 | Eight hundred and thirty-nine |
| -0.37249 | Two thousand two hundred and ninety-five |
| -0.37248 | One thousand nine hundred and ninety-six |

这里自变量命名为`variable`，因变量命名为`response`。第一步是按照自变量对数据集进行排序，就像我们在分类决策树示例中所做的那样。

一旦数据集按感兴趣的独立变量排序，我们的下一步是确定将数据集分成左右节点的规则。我们可能会提出多种可能的规则。我们将执行的练习将有助于筛选出一个最佳分割数据集的规则。

![A463052_1_En_4_Figq_HTML.jpg](A463052_1_En_4_Figq_HTML.jpg)

![A463052_1_En_4_Figz_HTML.jpg](A463052_1_En_4_Figz_HTML.jpg)

从前面我们可以看出，当`variable` < -0.37249 时，总误差最小。因此，属于左侧节点的点的平均响应为 1，556，属于右侧节点的点的平均响应为 2，146。请注意，1，556 是小于我们之前得出的阈值的所有变量值的平均响应。同样，2，146 是大于或等于我们得出的阈值(0.37249)的所有变量值的平均响应。

### 连续因变量和多个连续自变量

在分类中，我们将信息增益作为一种度量来决定应该首先分割原始数据集的变量。类似地，在连续变量预测中有多个相互竞争的独立变量的情况下，我们将列出导致总体误差最小的变量。

我们将在之前考虑的数据集中添加一个额外的变量:

![A463052_1_En_4_Figr_HTML.png](A463052_1_En_4_Figr_HTML.png)

在前一节中，我们已经计算了`variable`的各种可能规则的总误差。让我们计算一下`var2`的各种可能规则的总体误差。

第一步是通过增加`var2`的值对数据集进行排序。因此，我们现在将处理的数据集将转换为以下内容:

<colgroup><col align="left"> <col align="left"></colgroup> 
| var2 | 反应 |
| :-- | :-- |
| Zero point one | One thousand nine hundred and ninety-six |
| Zero point three | Eight hundred and thirty-nine |
| Zero point four four | Two thousand two hundred and twenty-nine |
| Zero point five one | Two thousand three hundred and nine |
| Zero point seven five | Eight hundred and fifteen |
| Zero point seven eight | Two thousand two hundred and ninety-five |
| Zero point eight four | One thousand five hundred and ninety |

使用`var2`开发的各种可能规则的总误差计算如下:

![A463052_1_En_4_Figs_HTML.jpg](A463052_1_En_4_Figs_HTML.jpg)

注意，当`var2` <为 0.44 时，总体误差最小。然而，当我们比较由`variable`产生的最小总体误差和由`var2`产生的最小总体误差时，`variable`产生最小总体误差，因此应该是分割数据集的变量。

### 连续因变量和离散自变量

为了了解如何使用离散自变量预测连续因变量，我们将使用以下数据集作为示例，其中“var”是自变量，“response”是因变量:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 定义变量 | 反应 |
| :-- | :-- |
| `a` | One thousand five hundred and ninety |
| `b` | Two thousand three hundred and nine |
| `c` | Eight hundred and fifteen |
| `a` | Two thousand two hundred and twenty-nine |
| `b` | Eight hundred and thirty-nine |
| `c` | Two thousand two hundred and ninety-five |
| `a` | One thousand nine hundred and ninety-six |

让我们按如下方式透视数据集:

![A463052_1_En_4_Figt_HTML.jpg](A463052_1_En_4_Figt_HTML.jpg)

我们将通过增加平均`response`值对数据集进行排序:

![A463052_1_En_4_Figu_HTML.jpg](A463052_1_En_4_Figu_HTML.jpg)

现在我们将计算最佳的左节点和右节点组合。在第一种情况下，只有`c`在左节点，而`a,b`在右节点。左侧节点的平均响应将是 1555，右侧节点的平均响应将是{1574，1938} = {1756}的平均值。

这种情况下的总体误差计算如下:

![A463052_1_En_4_Figv_HTML.png](A463052_1_En_4_Figv_HTML.png)

在第二个场景中，我们将认为{`c,b`}属于左侧节点，而{`a`}属于右侧节点。在这种情况下，左侧节点的平均响应将是{1555，1574} = {1564.5}的平均值

这种情况下的总误差计算如下:

![A463052_1_En_4_Figw_HTML.png](A463052_1_En_4_Figw_HTML.png)

我们可以看到，后一种左右节点的组合与前一种组合相比，总误差较小。因此，在这种情况下，理想的分割应该是{`b,c`}属于一个节点，而{`a`}属于另一个节点。

### 连续因变量和离散、连续自变量

在有多个独立变量的情况下，有些变量是离散的，有些是连续的，我们遵循与前面相同的步骤:

1.  分别确定每个变量的最佳分界点。
2.  了解最能减少不确定性的变量。

要遵循的步骤与前面几节中的步骤相同。

## 用 R 语言实现决策树

分类的实现不同于回归(连续变量预测)的实现。因此，需要模型类型的规范作为输入。

下面的代码片段展示了我们如何在 R 中实现一个决策树。github 中的 r”)。

```py
# import dataset
t=read.csv("D:/Pro ML book/Decision tree/dt_continuous_dep_indep.csv")
library(rpart)
# fit a decision tree using rpart function
fit=rpart(response~variable,method="anova", data=t
      ,control=rpart.control(minsplit=1,minbucket=2,maxdepth=2))

```

决策树是使用名为`rpart`的包中的可用函数实现的。帮助构建决策树的函数也叫做`rpart`。注意，在`rpart`中，我们指定了`method`参数。

因变量连续时使用方法`anova`，因变量离散时使用方法`class`。

您还可以指定附加参数:`minsplit`、`minbucket`和`maxdepth`(下一节将详细介绍)。

## 用 Python 实现决策树

分类问题的 Python 实现将利用`sklearn`包中的`DecisionTreeClassifier`函数(代码在 github 中以“Decision tree.ipynb”的形式提供):

```py
from sklearn.tree import DecisionTreeClassifier
depth_tree = DecisionTreeClassifier()
depth_tree.fit(X, y)

```

对于回归问题，Python 实现将利用`sklearn`包中的`DecisionTreeRegressor`函数:

```py
from sklearn.tree import DecisionTreeRegressor
depth_tree = DecisionTreeRegressor()
depth_tree.fit(X, y)

```

## 树木建造的常用技术

我们之前看到，复杂性参数(终端节点的数量)可以是我们在检查开箱验证时进行优化的一个参数。其他常用技术包括:

*   将每个终端节点中的观测值数量限制到最小数量(例如，在一个节点中至少 20 个观测值)
*   手动指定树的最大深度
*   指定节点中的最小观察数，以便算法考虑进一步拆分

我们做以上所有事情是为了避免树过度适应我们的数据。为了理解过度拟合的问题，让我们来看下面的场景:

1.  该树总共有 90 个深度级别(`maxdepth` = 90)。在这种情况下，构建的树会有太多的分支，以至于在训练数据集上过度拟合，但不一定在测试数据集上一般化。
2.  类似于`maxdepth`的问题，终端节点中最小数量的观察也可能导致过度拟合。如果我们不指定`maxdepth`并且在终端节点参数中的最小观察数量中具有较小的数量，则结果树同样可能是巨大的，具有多个分支，并且将同样可能导致对训练数据集的过度拟合，而不是对测试数据集的一般化。
3.  要进一步分割的节点中的最小观测值数量是一个与节点中的最小观测值非常相似的参数，只是该参数限制了父节点而不是子节点中的观测值数量。

## 可视化树构建

在 R 中，可以使用`rpart`包中的`plot`函数来绘制树结构。`plot`函数绘制了树的框架，`text`函数编写了在树的各个部分派生的规则。下面是可视化决策树的一个示例实现:

```py
# import dataset
t=read.csv("D:/Pro ML book/Decision tree/dt_continuous_dep_discrete_indep.csv")
library(rpart)
# fit a decision tree using rpart function
fit=rpart(response~variable,method="anova",data=t
      ,control=rpart.control(minsplit=1,minbucket=2,maxdepth=2))
plot(fit, margin=0.2)
text(fit, cex=.8)

```

上述代码的输出如下所示:

![A463052_1_En_4_Figy_HTML.jpg](A463052_1_En_4_Figy_HTML.jpg)

从这个图我们可以推导出，当`var`是`b`或者`c`中的一个时，那么输出就是 1564。如果不是，则输出 1938。

在 Python 中，可视化决策树的一种方法是使用一组有助于显示的函数的包:`Ipython.display`、`sklearn.externals.six`、`sklearn.tree`、`pydot`、`os`。

```py
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot
features = list(data.columns[1:])

```

```py
import os
os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
dot_data = StringIO()  
export_graphviz(depth_tree, out_file=dot_data,feature_names=features,filled=True,rounded=True)
graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())

```

在前面的代码中，您必须更改数据框名称来代替我们在第一个代码片段中使用的`(data.columns[1:])`。本质上，我们提供独立变量名作为特性。

在第二段代码中，您必须指定安装`graphviz`的文件夹位置，并将决策树的名称更改为用户在第四行给出的名称(用您为`DecisionTreeRegressor`或`DecisionTreeClassifier`创建的变量名替换`dtree`)。

前面代码片段的输出如图 [4-3](#Fig3) 所示。

![A463052_1_En_4_Fig3_HTML.jpg](A463052_1_En_4_Fig3_HTML.jpg)

图 4-3

The output of the code

## 离群值对决策树的影响

在前面的章节中，我们已经看到异常值对线性回归有很大的影响。然而，在决策树中，离群值对分类几乎没有影响，因为我们会查看多个可能的规则，并在对感兴趣的变量进行排序后列出最大化信息增益的规则。假设我们按照自变量对数据集进行排序，那么自变量中的异常值就没有影响。

然而，如果数据集中存在异常值，那么在连续变量预测的情况下，因变量中的异常值将是具有挑战性的。这是因为我们使用总体平方误差作为最小化的指标。如果因变量包含异常值，它会导致类似的问题，就像我们在线性回归中看到的那样。

## 摘要

决策树构建简单，理解直观。当因变量是分类变量时，用于构建决策树的突出方法是信息增益和基尼不纯的组合，当因变量是连续变量时，是总体平方误差的组合。