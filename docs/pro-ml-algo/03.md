# 3.逻辑回归

在第 [2](02.html) 章中，我们看了基于自变量估计变量的方法。我们估计的因变量是连续的(冰淇淋的销量，婴儿的体重)。然而，在大多数情况下，我们需要对离散变量进行预测——例如，客户是否会流失，或者是否会赢得比赛。这些事件没有太多独特的价值。它们只有 1 或 0 的结果——无论事件是否发生。

虽然线性回归有助于预测变量的值(大小)，但它在预测只有两个不同类别(1 或 0)的变量时有局限性。逻辑回归有助于解决这类问题，因为因变量的不同值数量有限。

在本章中，我们将学习以下内容:

*   线性回归和逻辑回归的区别
*   在 Excel、R 和 Python 中构建逻辑回归
*   衡量逻辑回归模型性能的方法

## 为什么线性回归对离散结果会失败？

为了理解这一点，让我们举一个假设的例子:根据棋手之间的 Elo 评分差异来预测一场象棋比赛的结果。

<colgroup><col align="left"> <col align="left"></colgroup> 
| 黑白棋子玩家之间的等级差异 | 白棋赢了？ |
| :-- | :-- |
| Two hundred | Zero |
| –200 | one |
| Three hundred | Zero |

在前面的简单例子中，如果我们应用线性回归，我们将得到下面的等式:

白色韩元= 0.55–0.00214×(黑白评级差异)

让我们使用该公式来推断上表:

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| 黑白之间的评级差异 | 白棋赢了？ | 线性回归预测 |
| :-- | :-- | :-- |
| Two hundred | Zero | Zero point one one |
| –200 | one | Zero point nine seven |
| Three hundred | Zero | –0.1 |

如您所见，300 的差异导致预测值小于 0。类似地，对于–300 的差异，线性回归的预测将超过 1。但是，在这种情况下，超过 0 或 1 的值没有意义，因为 win 是一个离散值(0 或 1)。

因此，预测应该仅绑定到 0 到 1-任何高于 1 的预测应该上限为 1，任何低于 0 的预测应该下限为 0。

这转化为一条拟合线，如图 [3-1](#Fig1) 所示。

![A463052_1_En_3_Fig1_HTML.jpg](A463052_1_En_3_Fig1_HTML.jpg)

图 3-1

The fitted line

图 3.1 显示了线性回归在预测离散变量(本例中为二元变量)时的以下主要局限性:

*   线性回归假设变量是线性相关的:然而，随着玩家实力差异的增加，获胜的机会呈指数变化。
*   线性回归没有给出失败的机会:在实践中，即使有 500 分的差异，劣势玩家也有获胜的外部机会(假设有 1%的机会)。但是如果用线性回归封顶，另一个玩家就没有机会赢了。一般来说，线性回归不能告诉我们某一事件在一定范围后发生的概率。
*   线性回归假设概率随着自变量的增加而成比例增加:无论评分差异是+400 还是+500(因为差异显著)，获胜的概率都很高。同样，无论差额是–400 还是–500，获胜的概率都很低。

## 更一般的解决方案:s 形曲线

如上所述，线性回归的主要问题是它假设所有的关系都是线性的，尽管实际上很少是线性的。

为了解决线性回归的局限性，我们将探索一种称为 sigmoid 曲线的曲线。曲线如图 [3-2](#Fig2) 所示。

![A463052_1_En_3_Fig2_HTML.jpg](A463052_1_En_3_Fig2_HTML.jpg)

图 3-2

A sigmoid curve

s 形曲线的特征如下:

*   它在值 0 和 1 之间变化
*   它在某个阈值之后(在图 [3-2](#Fig2) 中的值 3 或-3 之后)稳定下来

sigmoid 曲线将帮助我们解决线性回归面临的问题，即无论黑白棋子玩家之间的评分差异是+400 还是+500，赢的概率都很高，无论差异是–400 还是–500，赢的概率都很低。

### 形式化 s 形曲线(s 形激活)

我们已经看到，sigmoid 曲线比线性回归更能解释离散现象。

sigmoid 曲线可以用如下数学公式表示:

![$$ S(t)=\frac{1}{1+{e}^{-t}} $$](A463052_1_En_3_Chapter_IEq1.gif)

在该等式中，t 的值越高，![$$ {e}^{-t}, $$](A463052_1_En_3_Chapter_IEq2.gif)的值越低，因此 S(t)接近 1。并且 t 的值越低(比如说–100)，![$$ {e}^{-t} $$](A463052_1_En_3_Chapter_IEq3.gif)的值越高，【1 +】的值也越高，因此 S(t)非常接近 0。

### 从 Sigmoid 曲线到逻辑回归

线性回归假设因变量和自变量之间存在线性关系。它被写成 Y = a + b × X。逻辑回归通过应用 sigmoid 曲线，摆脱了所有关系都是线性的约束。

逻辑回归的数学模型如下:

![$$ Y=\frac{1}{\left(1+{e}^{-\left(a+{b}^{\ast }X\right)}\right)} $$](A463052_1_En_3_Chapter_Equa.gif)

我们可以看到，逻辑回归以与线性回归相同的方式使用独立变量，但通过 sigmoid 激活传递它们，以便输出介于 0 和 1 之间。

在存在多个独立变量的情况下，该方程转化为通过 sigmoid 激活的多元线性回归。

### 解读逻辑回归

线性回归可以用一种直截了当的方式来解释:自变量的值增加 1 个单位，产出(因变量)增加 b 个单位。

要了解逻辑回归中的输出如何变化，让我们看一个例子。让我们假设我们构建的逻辑回归曲线(我们将在接下来的章节中研究如何构建逻辑回归)如下:

![$$ Y=\frac{1}{\left(1+{e}^{-\left(2+{3}^{\ast }X\right)}\right)} $$](A463052_1_En_3_Chapter_Equb.gif)

*   如果 X = 0，Y 的值= 1/(1+exp(–(2)))= 0.88。
*   如果 X 增加 1 个单位(即 X = 1)，则 Y 的值为 Y = 1/(1+exp(–(2+3×1)))= 1/(1+exp(–(5)))= 0.99。

如你所见，当 X 从 0 变到 1 时，Y 的值从 0.88 变到 0.99。类似地，如果 X 是–1，Y 应该是 0.27。如果 X 是 0，Y 应该是 0.88。当 X 从-1 到 0 时，Y 从 0.27 到 0.88 有一个剧烈的变化，但当 X 从 0 到 1 时，变化不那么剧烈。

因此，X 单位变化对 Y 的影响取决于等式。

X = 0 时的值 0.88 可以解释为概率。换句话说，平均在 88%的情况下，当 X = 0 时，Y 的值是 1。

### 逻辑回归的工作细节

为了了解逻辑回归是如何工作的，我们将进行与上一章学习线性回归相同的练习:我们将在 Excel 中建立一个逻辑回归方程。在本练习中，我们将使用虹膜数据集。挑战在于能够根据一些变量(萼片、花瓣长度和宽度)预测该物种是否为刚毛藻。

以下数据集包含我们将要执行的练习的自变量和因变量的值(在 github 中作为“iris sample estimation.xlsx”数据集提供):

1.  将自变量的权重初始化为随机值(假设每个 1)。
2.  Once the weights and the bias are initialized, we’ll estimate the output value (the probability of the species being Setosa) by applying sigmoid activation on the multivariate linear regression of independent variables. The next table contains information about the (a + b × X) part of the sigmoid curve and ultimately the sigmoid activation value.

    ![A463052_1_En_3_Figb_HTML.jpg](A463052_1_En_3_Figb_HTML.jpg)

    The formula for how the values in the preceding table are obtained is given in the following table:

    ![A463052_1_En_3_Figc_HTML.jpg](A463052_1_En_3_Figc_HTML.jpg)

    The `ifelse` condition in the preceding sigmoid activation column is used only because Excel has limitations in calculating any value greater than `exp(500)`—hence the clipping.  

![A463052_1_En_3_Figa_HTML.jpg](A463052_1_En_3_Figa_HTML.jpg)

### 估计误差

在第 [2](02.html) 章中，我们考虑了实际值和预测值之间的最小二乘法(平方差)来估计总体误差。在逻辑回归中，我们将使用不同的误差度量，称为交叉熵。

交叉熵是两种不同分布(实际分布和预测分布)之间差异的度量。为了理解交叉熵，让我们看一个例子:两个政党在一次选举中竞争，其中甲方获胜。在一种情况下，每一方获胜的机会是 0.5——换句话说，很少能得出结论，信息也很少。但是，如果甲方有 80%的机会获胜，而乙方有 20%的机会获胜，我们就可以得出关于选举结果的结论，因为实际值和预测值的分布更接近。

交叉熵的公式如下:

![$$ -\left( ylo{g}_2p+\left(1-y\right) lo{g}_2\left(1-p\right)\right) $$](A463052_1_En_3_Chapter_Equc.gif)

其中 y 是事件的实际结果，p 是事件的预测结果。

让我们把这两种选举情景代入这个等式。

#### 场景 1

在这种情况下，模型预测甲方获胜的概率为 0.5，而甲方的实际结果是 1:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 甲方模型预测 | 甲方的实际成果 |
| :-- | :-- |
| Zero point five | one |

这个模型的交叉熵如下:

![$$ -\left(1 lo{g}_20.5+\left(1-1\right) lo{g}_2\left(1-0.5\right)\right)=1 $$](A463052_1_En_3_Chapter_Equd.gif)

#### 场景 2

在这种情况下，模型预测甲方获胜的概率为 0.8，而甲方的实际结果是 1:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 甲方模型预测 | 甲方的实际成果 |
| :-- | :-- |
| Zero point eight | one |

这个模型的交叉熵如下:

![$$ -\left(1 lo{g}_20.8+\left(1-1\right) lo{g}_2\left(1-0.8\right)\right)=0.32 $$](A463052_1_En_3_Chapter_Eque.gif)

我们可以看到，与场景 1 相比，场景 2 具有更低的交叉熵。

### 最小二乘法与线性假设

假设在前面的例子中，当概率为 0.8 时，交叉熵比概率为 0.5 时更低，我们是否可以不使用预测概率和实际值之间的最小二乘方差，并以类似于线性回归的方式进行处理？本节讨论选择交叉熵误差而不是最小二乘法。

逻辑回归的一个典型例子是其在基于某些属性预测癌症是良性还是恶性中的应用。

让我们比较一下因变量(恶性肿瘤)为 1 的情况下的两种成本函数(最小二乘法和熵成本):

![A463052_1_En_3_Figd_HTML.jpg](A463052_1_En_3_Figd_HTML.jpg)

获取上表的公式如下:

![A463052_1_En_3_Fige_HTML.jpg](A463052_1_En_3_Fige_HTML.jpg)

请注意，与平方误差相比，交叉熵对高预测误差的惩罚较重:较低的误差值在平方误差和交叉熵误差方面具有相似的损失，但是对于实际值和预测值之间的较高差异，交叉熵的惩罚比平方误差方法更大。因此，我们将坚持交叉熵误差作为我们的误差度量，而不是离散变量预测的平方误差。

对于前面提到的 Setosa 分类问题，我们用交叉熵误差代替平方误差，如下:

![A463052_1_En_3_Figf_HTML.jpg](A463052_1_En_3_Figf_HTML.jpg)

现在我们已经建立了我们的问题，让我们以这样一种方式改变参数，使总误差最小化。这一步也是通过梯度下降来执行的，梯度下降可以通过使用 Excel 中的规划求解功能来完成。

## 在 R 中运行逻辑回归

现在我们已经有了一些逻辑回归的背景知识，我们将深入研究 R(可作为“逻辑回归”使用)中相同内容的实现细节。github 中的 r”)。

```
# import dataset
data=read.csv("D:/Pro ML book/Logistic regression/iris_sample.csv")
# build a logistic regression model
lm=glm(Setosa~.,data=data,family=binomial(logit))
# summarize the model
summary(lm)

```

上述代码的第二行指定我们将使用`glm`(广义线性模型)，其中考虑了二项式家族。请注意，通过指定“~”我们确保所有的变量都被认为是独立变量。

逻辑模型的`summary`给出了一个高层次的总结，类似于我们在线性回归中得到总结结果的方式:

![A463052_1_En_3_Figg_HTML.jpg](A463052_1_En_3_Figg_HTML.jpg)

## 在 Python 中运行逻辑回归

现在让我们看看如何在 Python 中构建逻辑回归方程(在 github 中以“逻辑回归. ipynb”的形式提供):

```
# import relevant packages
# pandas package is used to import data
# statsmodels is used to invoke the functions that help in lm
import pandas as pd
import statsmodels.formula.api as smf

```

一旦我们导入了包，我们就在逻辑回归的情况下使用`logit`方法，如下所示:

```
# import dataset
data = pd.read_csv('D:/Pro ML book/Logistic regression/iris_sample.csv')
# run regression
est = smf.logit(formula='Setosa~Slength+Swidth+Plength+Pwidth',data=data)
est2=est.fit()
print(est2.summary())

```

前面代码中的`summary`函数给出了模型的摘要，类似于我们在线性回归中获得摘要结果的方式。

## 确定感兴趣的度量

在线性回归中，我们将均方根误差(RMSE)视为一种测量误差的方法。

在逻辑回归中，我们衡量模型性能的方式不同于我们在线性回归中衡量模型性能的方式。让我们探讨一下为什么线性回归误差度量不能用于逻辑回归。

我们将研究如何构建一个模型来预测欺诈交易。假设总交易量的 1%是欺诈交易。我们希望预测交易是否可能是欺诈。在这种特殊情况下，我们使用逻辑回归通过使用一组独立变量来预测因变量欺诈交易。

为什么我们不能用一个精确度来衡量呢？假设所有交易中只有 1%是欺诈，让我们考虑一个所有预测都为 0 的场景。在这个场景中，我们的模型有 99%的准确率。但该模型在减少欺诈交易方面毫无用处，因为它预测每笔交易都不是欺诈。

在典型的现实场景中，我们会构建一个模型来预测交易是否可能是欺诈，并且只标记欺诈可能性高的交易。被标记的交易随后被发送给运营团队进行人工审查，从而降低欺诈交易率。

虽然我们通过让运营团队审查高可能性交易来降低欺诈交易率，但我们会产生额外的人力成本，因为需要人工来审查交易。

欺诈交易预测模型可以帮助我们减少需要人工(运营团队)审查的交易数量。假设总共有 1，000，000 笔交易。在这 100 万笔交易中，1%是欺诈性的，因此，总共有 10，000 笔交易是欺诈性的。

在这种特殊情况下，如果没有模型，平均每 100 笔交易中就有 1 笔是欺诈性的。下表显示了随机猜测模型的性能:

![A463052_1_En_3_Figh_HTML.png](A463052_1_En_3_Figh_HTML.png)

如果我们要绘制这些数据，它看起来会如图 [3-3](#Fig3) 所示。

![A463052_1_En_3_Fig3_HTML.jpg](A463052_1_En_3_Fig3_HTML.jpg)

图 3-3

Cumulative frauds captured by random guess model

现在让我们来看看构建一个模型有什么帮助。我们将创建一个简单的示例来得出误差度量:

1.  以数据集为输入，计算每个交易 id 的概率:T12】欺诈的可能性

    <colgroup><col align="left"><col align="left"><col align="left"></colgroup>
    | 交易 id | 实际欺诈 |
    | :-- | :-- |
    | 一 | 一 | 零点五六 |
    | 二 | 零点七 |  |
    | 三 | 一 | 零点三九 |
    | 四 | 一 |  | 零点五五 |
    | 五 | 一 | 零点零三 |
    | 六 | 零 | 零点八四 |
    | 七 | 零 | 零点零五 |

2.  按照欺诈概率从高到低对数据集进行排序。直觉是，当在按概率降序排序后，数据集顶部有更多“实际欺诈”1 时，模型表现良好:T12】欺诈的可能性

    <colgroup><col align="left"><col align="left"><col align="left"></colgroup>
    | 交易 id | 实际欺诈 |
    | :-- | :-- |
    | 九 | 零点八六 |
    | 六 | 零点八四 |
    | 两 | 零点 | 零点七 |
    | 一 | 一 | 零点五六 |
    | 四 | 一 | 零点五五五 |
    | 八 | 零点四六 |
    |  | 零点零五 |
    | 五 | 一 | 零点零三 |

3.  计算从排序后的表中捕获的累计交易数:

    <colgroup><col align="left"><col align="left"><col align="left"><col align="left"><col align="left"></colgroup>
    | 交易 id | 实际欺诈 | 欺诈的可能性 | 已审核的累计交易 | 捕获的累积欺诈 |
    | :-- | :-- | :-- | :-- | :-- |
    | 九 | 零点八六 | 一 | 零点 |
    | 六 | 零点 | 零点八四 | 两 | 零点 |
    | 两个 | 零个 | 零点七个 | 三个 |
    | 一个 | 一个 | 零点五六个 | 四个 | 一个 |
    | 四个 | 一个 | 一 | 零点三九 | 七 | 三 |
    | 十 | 一 | 零点一一 | 八 | 四 |
    | 七 | 零 |  |

在这种情况下，假设 10 笔交易中有 5 笔是欺诈性的，平均每 2 笔交易中就有 1 笔是欺诈性的。因此，与使用随机猜测相比，使用模型捕获的累积欺诈如下:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 交易 id | 实际欺诈 | 已审核的累计交易 | 捕获的累积欺诈 | 通过随机猜测捕获的累积欺诈 |
| :-- | :-- | :-- | :-- | :-- |
| nine | Zero | one | Zero | Zero point five |
| six | Zero | Two | Zero | one |
| Two | Zero | three | Zero | One point five |
| one | one | four | one | Two |
| four | one | five | Two | Two point five |
| eight | Zero | six | Two | three |
| three | one | seven | three | Three point five |
| Ten | one | eight | four | four |
| seven | Zero | nine | four | Four point five |
| five | one | Ten | five | five |

我们可以绘制随机模型和逻辑回归模型捕获的累积欺诈，如图 [3-4](#Fig4) 所示。

![A463052_1_En_3_Fig4_HTML.jpg](A463052_1_En_3_Fig4_HTML.jpg)

图 3-4

Comparing the models

在这种特殊情况下，对于上面的例子，随机猜测比逻辑回归模型更好-在最初的几次猜测中，随机猜测比模型做出更好的预测。

现在，让我们回到之前的欺诈交易示例场景，假设模型的结果如下所示:

<colgroup><col align="left"> <col align="left"> <col align="left"></colgroup> 
| 捕获的累积欺诈 |
| :-- |
| 审查的交易数量 | 通过随机猜测捕获的累积欺诈 | 按模型捕获的累积欺诈 |
| :-- | :-- | :-- |
| - | - | Zero |
| One hundred thousand | One thousand | Four thousand |
| Two hundred thousand | Two thousand | Six thousand |
| Three hundred thousand | Three thousand | 7600 |
| Four hundred thousand | Four thousand | Eight thousand one hundred |
| Five hundred thousand | Five thousand | Eight thousand five hundred |
| Six hundred thousand | Six thousand | Eight thousand eight hundred and fifty |
| Seven hundred thousand | Seven thousand | Nine thousand one hundred and fifty |
| Eight hundred thousand | Eight thousand | Nine thousand four hundred and fifty |
| Nine hundred thousand | Nine thousand | Nine thousand seven hundred and fifty |
| One million | Ten thousand | ten thousand |

我们展示了随机猜测捕获的累积欺诈与模型捕获的累积欺诈之间的图表，如图 [3-5](#Fig5) 所示。

![A463052_1_En_3_Fig5_HTML.jpg](A463052_1_En_3_Fig5_HTML.jpg)

图 3-5

Comparing the two approaches

请注意，随机猜测线和模型线之间的面积越大，模型性能越好。衡量模型线下覆盖面积的指标称为曲线下面积(AUC)。

因此，AUC 指标是帮助我们评估逻辑回归模型性能的更好指标。

在实践中，当评分的数据集根据概率被分成十个桶(组)(代码在 github 中提供为“credit default prediction.ipynb”)时，稀有事件建模的输出看起来如下:

![A463052_1_En_3_Figi_HTML.jpg](A463052_1_En_3_Figi_HTML.jpg)

上表中的`prediction_rank`表示概率的十分位数，即每笔交易按概率排序，然后根据其所属的十分位数分组到存储桶中。注意，第三列(`total_observations`)在每个十分位数中有相同数量的观察值。

第二列——`prediction avg_default`——代表我们建立的模型得出的平均违约概率。第四列—`SeriousDlqin2yrs avg_default`—代表每个时段的平均实际违约。最后一列表示每个时段中捕获的实际违约数量。

请注意，在理想的情况下，所有的默认值都应该被捕获到最有可能的桶中。另请注意，在上表中，该模型在最高概率时段中捕获了大量欺诈。

## 常见陷阱

本节讨论分析师在构建分类模型时应该小心的一些常见陷阱:

### 预测和事件发生之间的时间

让我们来看一个案例研究:预测一个客户的违约。

我们应该说，今天预测明天有人很可能信用卡违约是没有用的。在预测某人会违约的时间和事件实际发生的时间之间应该有一些时间差。原因是运营团队需要一些时间来干预并帮助减少违约交易的数量。

### 独立变量中的异常值

类似于独立变量中的异常值如何影响线性回归中的总体误差，最好对异常值进行封顶，以便它们不会对逻辑回归中的回归产生太大影响。注意，与线性回归不同，当有异常值输入时，逻辑回归不会有巨大的异常值输出；在逻辑回归中，输出总是被限制在 0 和 1 之间，并且相应的交叉熵损失与之相关联。

但是有异常值的问题仍然会导致很高的交叉熵损失，所以限制异常值是一个更好的主意。

## 摘要

在本章中，我们经历了以下内容:

*   逻辑回归用于预测二元(分类)事件，线性回归用于预测连续事件。
*   逻辑回归是线性回归的扩展，其中线性方程通过 sigmoid 激活函数。
*   逻辑回归中使用的主要损失度量之一是交叉熵误差。
*   sigmoid 曲线有助于限制 0 到 1 之间的输出值，从而估计与事件相关的概率。
*   AUC 度量是评估逻辑回归模型的更好的度量。