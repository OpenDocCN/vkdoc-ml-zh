# 2.线性回归

为了理解线性回归，让我们来解析它:

*   直线的:排列成直线的或沿直线延伸的，如“直线运动”
*   回归:一种确定两个或多个变量之间统计关系的技术，其中一个变量的变化是由另一个变量的变化引起的。

综合这些因素，我们可以将线性回归定义为两个变量之间的关系，其中一个变量的增加会影响另一个变量成比例地增加或减少(即线性增加或减少)。

在本章中，我们将学习以下内容:

*   线性回归如何工作
*   构建线性回归时要避免的常见陷阱
*   如何在 Excel、Python 和 R 中构建线性回归

## 引入线性回归

线性回归有助于根据已知值对未知变量(连续变量)的值进行插值。它的一个应用可以是，“当产品的价格变化时，对产品的需求是多少？”在这个应用程序中，我们将不得不根据历史价格查看需求，并根据新的价格点对需求进行估计。

考虑到我们是为了估计一个新的价格点而查看历史，这就变成了一个回归问题。价格和需求彼此线性相关的事实(价格越高，需求越低，反之亦然)使其成为线性回归问题。

### 变量:因变量和自变量

因变量是我们预测的值，自变量是我们用来预测因变量的变量。

例如，温度与购买冰淇淋的数量成正比。随着温度的升高，购买冰淇淋的数量也会增加。在这里，温度是独立变量，并根据它来预测购买冰淇淋的数量(因变量)。

### 相互关系

从前面的例子中，我们可能会注意到冰淇淋的购买量与温度直接相关(也就是说，它们与独立变量温度的方向相同或相反)。在这个例子中，相关性是正的:随着温度的升高，冰淇淋的销量增加。在其他情况下，相关性可能是负的:例如，一个项目的销售可能会随着该项目的价格下降而增加。

### 原因

我们来翻转一下冰淇淋销量随着温度升高而增加的场景(高+ ve 相关性)。相反，随着冰淇淋销量的增加，温度也会升高(也是高+ ve 相关性)。

然而，凭直觉我们可以自信地说，温度不受冰淇淋销售的控制，尽管事实正好相反。这就引出了因果关系的概念——也就是说，哪个事件影响另一个事件。温度影响冰淇淋的销售，但不是相反。

## 简单与多元线性回归

我们已经讨论了两个变量(因变量和自变量)之间的关系。然而，因变量不仅受一个变量的影响，还受多个变量的影响。例如，冰淇淋的销售受温度的影响，但也受冰淇淋的销售价格以及其他因素如位置、冰淇淋品牌等的影响。

在多元线性回归的情况下，一些变量将与因变量正相关，一些变量将与因变量负相关。

## 形式化简单线性回归

现在我们已经有了基本的术语，让我们深入线性回归的细节。简单的线性回归表示为:

![$$ Y=a+{b}^{\ast }X $$](A463052_1_En_2_Chapter_Equa.gif)

*   y 是我们预测的因变量。
*   x 是独立变量。
*   a 是偏差项。
*   b 是变量的斜率(分配给自变量的权重)。

y 和 X，因变量和自变量现在应该够清楚了。让我们介绍一下偏差和权重项(上式中的 a 和 b)。

### 偏差项

我们通过一个例子来看偏差项:用婴儿的月龄来估计婴儿的体重。我们假设婴儿的体重完全取决于婴儿几个月大。婴儿出生时重 3 公斤，体重以每月 0.75 公斤的恒定速度增加。

在年末，婴儿体重的图表看起来像图 [2-1](#Fig1) 。

![A463052_1_En_2_Fig1_HTML.jpg](A463052_1_En_2_Fig1_HTML.jpg)

图 2-1

Baby weight over time in months

在图 [2-1](#Fig1) 中，婴儿的体重从 3 (a，偏差)开始，每月线性增加 0.75 (b，斜率)。注意，当所有自变量都为 0 时，偏差项是因变量的值。

### 斜坡

直线的斜率是直线长度上直线两端的 x 和 y 坐标之差。在前面的示例中，斜率(b)的值如下:

(两端 y 坐标之差)/(两端 x 坐标之差)

![$$ b=\frac{12-3\ }{\left(12-0\right)}=9/12=0.75 $$](A463052_1_En_2_Chapter_Equb.gif)

## 求解简单的线性回归

我们已经看到了一个简单的线性回归输出的简单例子(求解偏差和斜率)。在本节中，我们将采取第一步，提出一种更通用的方法来生成回归线。提供的数据集如下:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 月龄 | 重量(千克) |
| :-- | :-- |
| Zero | three |
| one | Three point seven five |
| Two | Four point five |
| three | Five point two five |
| four | six |
| five | Six point seven five |
| six | Seven point five |
| seven | Eight point two five |
| eight | nine |
| nine | Nine point seven five |
| Ten | Ten point five |
| Eleven | Eleven point two five |
| Twelve | Twelve |

数据的可视化如图 [2-2](#Fig2) 所示。

![A463052_1_En_2_Fig2_HTML.jpg](A463052_1_En_2_Fig2_HTML.jpg)

图 2-2

Visualizing baby weight

在图 [2-2](#Fig2) 中，x 轴是婴儿的月龄，y 轴是婴儿在给定月份的体重。比如宝宝第一个月的体重是 3.75 斤。

让我们从基本原则来解决这个问题。我们将假设数据集只有 2 个数据点，而不是 13 个，但是，只有前 2 个数据点。数据集将如下所示:

<colgroup><col align="left"> <col align="left"></colgroup> 
| 月龄 | 重量(千克) |
| :-- | :-- |
| Zero | three |
| one | Three point seven five |

假设我们根据婴儿的年龄来估计婴儿的体重，那么可以建立如下的线性回归:

![$$ 3=a+{b}^{\ast }(0) $$](A463052_1_En_2_Chapter_Equc.gif)

![$$ 3.75=a+{b}^{\ast }(1) $$](A463052_1_En_2_Chapter_Equd.gif)

解决这个问题后，我们看到 a = 3，b = 0.75。

让我们将 a 和 b 的值应用于上面剩余的 11 个数据点。结果将如下所示:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 月龄 | 重量(千克) | 重量估计 | 估计的平方误差 |
| :-- | :-- | :-- | :-- |
| Zero | three | three | Zero |
| one | Three point seven five | Three point seven five | Zero |
| Two | Four point five | Four point five | Zero |
| three | Five point two five | Five point two five | Zero |
| four | six | six | Zero |
| five | Six point seven five | Six point seven five | Zero |
| six | Seven point five | Seven point five | Zero |
| seven | Eight point two five | Eight point two five | Zero |
| eight | nine | nine | Zero |
| nine | Nine point seven five | Nine point seven five | Zero |
| Ten | Ten point five | Ten point five | Zero |
| Eleven | Eleven point two five | Eleven point two five | Zero |
| Twelve | Twelve | Twelve | Zero |
|   |   | 总平方误差 | Zero |

如您所见，只需解决前两个数据点，就能以最小的错误率解决问题。然而，实际情况可能并非如此，因为大多数真实数据并不像表中所示的那样清晰。

## 求解简单线性回归的更一般的方法

在前面的场景中，我们看到系数是通过仅使用总数据集中的两个数据点获得的，也就是说，我们在得出最佳 a 和 b 时没有考虑大多数观察值。为了避免在建立方程时遗漏大多数数据点，我们可以将目标修改为最小化所有数据点的总体平方误差(普通最小二乘法)。

### 最小化误差平方和

总平方误差定义为所有观测值的实际值和预测值之间的平方差之和。我们考虑平方误差值而不是实际误差值的原因是，我们不希望某些数据点中的正误差抵消其他数据点中的负误差。例如，三个数据点中的+5 误差抵消了其他三个数据点中的–5 误差，导致六个数据点的总误差为 0。平方误差将后三个数据点的–5 误差转换为正数，因此总平方误差变为 6 × 5 <sup>2</sup> = 150。

这就带来了一个问题:我们为什么要最小化整体平方误差？原理如下:

1.  如果每个单独的数据点都预测正确，则总误差最小。
2.  一般来说，5%的高估和 5%的低估一样糟糕，因此我们考虑平方误差。

让我们把这个问题公式化:

<colgroup><col align="left"> <col align="left"> <col align="left"> <col align="left"> <col align="left"></colgroup> 
| 月龄 | 重量(千克) | 公式 | 当 a = 3，b = 0.75 时的重量估计 | 估计的平方误差 |
| :-- | :-- | :-- | :-- | :-- |
| Zero | three | 3 = a + b × (0) | three | Zero |
| one | Three point seven five | 3.75 = a + b × (1) | Three point seven five | Zero |
| Two | Four point five | 4.5 = a + b × (2) | Four point five | Zero |
| three | Five point two five | 5.25 = a + b × (3) | Five point two five | Zero |
| four | six | 6 = a + b × (4) | six | Zero |
| five | Six point seven five | 6.75 = a + b × (5) | Six point seven five | Zero |
| six | Seven point five | 7.5 = a + b × (6) | Seven point five | Zero |
| seven | Eight point two five | 8.25 = a + b × (7) | Eight point two five | Zero |
| eight | nine | 9 = a + b × (8) | nine | Zero |
| nine | Nine point seven five | 9.75 = a + b × (9) | Nine point seven five | Zero |
| Ten | Ten point five | 10.5 = a + b × (10) | Ten point five | Zero |
| Eleven | Eleven point two five | 11.25 = a + b × (11) | Eleven point two five | Zero |
| Twelve | Twelve | 12 = a + b × (12) | Twelve | Zero |
|   |   |   | 总平方误差 | Zero |

线性回归方程在上表的公式列中表示。

将数据集(前两列)转换为公式(第 3 列)后，线性回归是一个求解公式列中 a 和 b 值的过程，以使估计的总平方误差(所有数据点的平方误差之和)最小化。

### 求解公式

求解公式的过程非常简单，只需对 a 和 b 值的多个组合进行迭代，就能尽可能地降低整体误差。请注意，最佳 a 和 b 值的最终组合是通过使用一种称为梯度下降的技术获得的，这将在第 [7](07.html) 章中探讨。

## 简单线性回归的工作细节

求解 a 和 b 可以理解为 Excel 中的目标搜索问题，其中 Excel 帮助确定 a 和 b 的值，使总值最小化。

要了解其工作原理，请查看以下数据集(在 github 中以“线性回归 101.xlsx”的形式提供):

![A463052_1_En_2_Figa_HTML.jpg](A463052_1_En_2_Figa_HTML.jpg)

通过检查数据集中的以下内容，您应该了解以下内容:

1.  H3 和 H4 单元格与 D 列(估计重量)的关系
2.  E 列的公式
3.  单元格 E15，每个数据点的误差平方和
4.  To obtain the optimal values of a and b (in cells H3 and H4)—go to Solver in Excel and add the following constraints:

    ![A463052_1_En_2_Figb_HTML.jpg](A463052_1_En_2_Figb_HTML.jpg)

    1.  最小化单元格 E15 中的值
    2.  通过改变 H3 和 H4 的牢房 

### 简单的线性回归有点复杂

在前面的例子中，我们从一个值完全吻合的场景开始:a = 3，b = 0.75。

零差错率的原因是我们先定义了场景，再定义了方法——即一个婴儿出生时 3 kg，体重每月增加 0.75 kg。然而，实际情况是不同的:“每个婴儿都是不同的。”

让我们通过一个数据集(在 github 中以“婴儿年龄与体重的关系. xlsx”提供)来可视化这个新场景。这里，我们测量了两个不同婴儿的年龄和体重。

年龄-体重关系图现在看起来像图 [2-3](#Fig3) 。

![A463052_1_En_2_Fig3_HTML.jpg](A463052_1_En_2_Fig3_HTML.jpg)

图 2-3

Age-to-weight reltionship

体重的值随着年龄的增长而增加，但并不是从 3 kg 开始，每个月增加 0.75 kg 的确切趋势，如最简单的例子所示。

为了解决这个问题，我们要经历和之前一样的严格过程:

![A463052_1_En_2_Figc_HTML.jpg](A463052_1_En_2_Figc_HTML.jpg)

1.  用 a 和 b 的任意值初始化(例如，每个值都等于 1)。
2.  为预测创建一个新列，值为 a+b×X–列 c。
3.  做一个新的平方误差栏，d 栏。
4.  计算单元格 G7 中的总误差。
5.  调用规划求解，通过更改单元格 a 和 b(即 G3 和 G4)来最小化单元格 G7。

前面场景中的单元连接如下:

![A463052_1_En_2_Figd_HTML.jpg](A463052_1_En_2_Figd_HTML.jpg)

使总误差最小的 G3 和 G4 的单元值是 a 和 b 的最佳值。

### 达到最佳系数值

使用一种称为梯度下降的技术来获得系数的最佳值。第 [7](07.html) 章详细讨论了梯度下降是如何工作的，但现在，让我们通过以下步骤开始理解梯度下降:

1.  随机初始化系数(a 和 b)的值。
2.  计算成本函数，即训练数据集中所有数据点的误差平方和。
3.  稍微改变系数的值，比如说，其值的+1%。
4.  通过稍微改变系数值，检查总体平方误差是减少还是增加。
5.  如果通过将系数的值改变+1%来降低总平方误差，则进一步进行，否则将系数降低 1%。
6.  重复步骤 2-4，直到总平方误差最小。

### 引入均方根误差

到目前为止，我们已经看到，总误差是每个数据点的预测值和实际值之差的平方和。请注意，一般来说，随着数据点数量的增加，总平方误差也会增加。

为了对数据中的观察值数量进行归一化，也就是说，有一个有意义的误差度量，我们将考虑误差的均方根(因为我们在计算误差时已经对差值进行了平方)。均方根误差(RMSE)计算如下(在单元格 G9 中):

![A463052_1_En_2_Fige_HTML.jpg](A463052_1_En_2_Fige_HTML.jpg)

请注意，在前面的数据集中，我们必须求解 a 和 b(单元格 G3 和 G4)的最佳值，以使总误差最小。

## 在 R 中运行简单的线性回归

为了理解前面几节中介绍的实现细节，我们将在 R(作为“简单线性回归”提供)中运行线性回归。github 中的 r”)。

```
# import file
data=read.csv("D:/Pro ML book/linear_reg_example.csv")
# Build model
lm=glm(Weight~Age,data=data)
# summarize model
summary(lm)

```

函数`lm`代表线性模型，一般语法如下:

```
lm(y~x,data=data)

```

其中`y`是因变量，`x`是自变量，`data`是数据集。

`summary(lm)`给出了模型的概要，以及重要的变量和一些自动化测试。让我们一次解析一个:

![A463052_1_En_2_Figf_HTML.jpg](A463052_1_En_2_Figf_HTML.jpg)

### 残差

残差只不过是误差值(实际值和预测值之差)。汇总函数自动给出残差的分布。例如，考虑模型在我们训练的数据集上的残差。

使用模型的残差分布计算如下:

```
#Extracting prediction
data$prediction=predict(lm,data)
# Extracting residuals
data$residual = data$Weight - data$prediction
# summarizing the residuals
summary(data$residual)

```

在前面的代码片段中，`predict`函数将要实现的模型和要处理的数据集作为输入，并生成预测作为输出。

Note

`summary`函数的输出是残差列中的各种四分位数值。

### 系数

输出的系数部分给出了获得的截距和偏差的汇总版本。`(Intercept)`是偏差项(a)，而`Age`是`independent`变量:

*   `Estimate`是 a 和 b 各自的值。
*   如果我们从总人口中随机抽取样本，会给我们一种 a 和 b 值变化的感觉。标准误差与截距的比率越低，模型越稳定。

让我们来看一种可以可视化/计算标准误差值的方法。以下步骤提取标准误差值:

1.  随机抽取总数据集的 50%。
2.  对采样数据拟合一个`lm`模型。
3.  提取对采样数据拟合的模型的自变量系数。
4.  重复整个过程超过 100 次迭代。

在代码中，上述内容将转换如下:

```
# Initialize an object that stores the various coefficient values
samp_coef=c()
# Repeat the experiment 100 times
for(i in 1:100){
  # sample 50% of total data
  samp=sample(nrow(data),0.5*nrow(data))
  data2=data[samp,]
  # fit a model on the sampled data
  lm=lm(Weight~Age,data=data2)
  # extract the coefficient of independent variable and store it
  samp_coef=c(samp_coef,lm$coefficients['Age'])
}
sd(samp_coef)

```

注意，标准差越低，样本数据的系数值越接近原始数据。这表明，无论选择何种样本，系数值都是稳定的。

t 值是除以标准误差的系数。t 值越高，模型稳定性越好。

考虑下面的例子:

![A463052_1_En_2_Figg_HTML.jpg](A463052_1_En_2_Figg_HTML.jpg)

对应于变量`Age`的 t 值将等于 0.47473/0.01435。`(Pr>|t|)`给出了对应于 t 值的 p 值。p 值越低，模型越好。让我们来看看从 t 值推导出 p 值的方法。t 值到 p 值的查找可在此链接获得: [`http://www.socscistatistics.com/pvalues/tdistribution.aspx`](http://www.socscistatistics.com/pvalues/tdistribution.aspx)

在我们的例子中，对于`Age`变量，t 值是 33.09。

自由度=数据集中的行数-(模型中独立变量的数量+1)= 22-(1+1)= 20

注意，前面公式中的`+1`来自于包含截距项。

我们将检查双尾假设，并将 t 的值和自由度输入到查找表中，输出将是相应的 p 值。

根据经验，如果一个变量的 p 值小于 0.05，则它被认为是预测因变量的重要变量。我们来看看原因。

如果 p 值很高，这是因为相应的 t 值很低，这是因为与估计值相比，标准误差很高，这最终意味着从总体中随机抽取的样本没有相似的系数。

在实践中，我们通常将 p 值视为决定是否在模型中包含独立变量的指导指标之一。

### 残差的 SSE(残差偏差)

残差的平方和计算如下:

```
# SSE of residuals
data$prediction = predict(lm,data)
sum((data$prediction-data$Weight)^2)

```

剩余偏差表示建立模型后可以预期的偏差量。理想情况下，剩余偏差应该与零偏差进行比较，也就是说，由于建立了一个模型，偏差减少了多少。

### null deviation(零偏差)

零偏差是指在建立模型时没有使用独立变量时的预期偏差。

当没有自变量时，预测的最佳猜测是因变量本身的平均值。例如，如果我们说，平均每天有 1000 美元的销售额，那么一个人对未来销售额的最佳猜测(当没有提供其他信息时)是 1000 美元。

因此，零偏差可以计算如下:

```
#Null deviance
data$prediction = mean(data$Weight)
sum((data$prediction-data$Weight)^2)

```

请注意，在计算零偏差时，预测只是因变量的平均值。

### r 的平方

r 平方是预测值和实际值之间相关性的度量。其计算方法如下:

1.  找出实际因变量和预测因变量之间的相关性。
2.  对步骤 1 中获得的相关性求平方，即 R 的平方值。

r 的平方也可以这样计算:

![$$ 1-\left( Residual\ deviance/ Null\ deviance\right) $$](A463052_1_En_2_Chapter_Eque.gif)

零偏差——在预测因变量时不使用任何自变量(而是偏差/常数)时的偏差——计算如下:

![$$ null\ deviance=\sum {\left(Y\hbox{--} \widehat{Y}\right)}^2 $$](A463052_1_En_2_Chapter_Equf.gif)

其中是因变量，![$$ \widehat{Y} $$](A463052_1_En_2_Chapter_IEq1.gif)是因变量的平均值。

剩余偏差是我们用自变量预测因变量时的实际偏差。计算如下:

![$$ residual\ deviance=\sum {\left(Y\hbox{--} \overline{y}\right)}^2 $$](A463052_1_En_2_Chapter_Equg.gif)

其中 Y 为实际因变量，![$$ \overline{y} $$](A463052_1_En_2_Chapter_IEq2.gif)为因变量的预测值。

本质上，当剩余偏差比零偏差低得多时，R 的平方是高的。

### f 统计量

f 统计量为我们提供了一个类似于 R 平方的度量。F 统计量的计算方式如下:

![$$ F=\left(\frac{\frac{SSE(N)- SSE(R)}{d{f}_N-d{f}_R}}{\frac{SSE(R)}{d{f}_R}}\right) $$](A463052_1_En_2_Chapter_Equh.gif)

其中 SSE(N)是零偏差，SSE(R)是残差偏差，df <sub>N</sub> 是零偏差的自由度，df <sub>R</sub> 是残差偏差的自由度。F 统计值越高，模型越好。从零偏差到剩余偏差的偏差减少越多，在模型中使用独立变量的可预测性就越高。

## 在 Python 中运行简单的线性回归

可以使用以下代码在 Python 中运行线性回归(在 github 中以“线性回归 Python code.ipynb”的形式提供):

```
# import relevant packages
# pandas package is used to import data
# statsmodels is used to invoke the functions that help in lm
import pandas as pd
import statsmodels.formula.api as smf

# import dataset
data = pd.read_csv('D:/Pro ML book/Linear regression/linear_reg_example.csv')

# run least squares regression
est = smf.ols(formula='Weight~Age',data=data)
est2=est.fit()
print(est2.summary())

```

上述代码的输出如下所示:

![A463052_1_En_2_Figh_HTML.jpg](A463052_1_En_2_Figh_HTML.jpg)

请注意，R 和 Python 的系数部分输出非常相似。然而，这个包给了我们更多的度量来研究默认的预测水平。我们将在后面的部分中更详细地研究这些问题。

## 简单线性回归的常见陷阱

到目前为止，简单的例子是为了说明线性回归的基本工作原理。让我们考虑失败的情况:

*   当因变量和自变量之间始终不是线性相关时:随着婴儿年龄的增长，体重增加，但在某一阶段增加趋于平稳，此后两个值不再线性相关。另一个例子是个人的年龄和身高之间的关系。
*   当自变量的值中有异常值时:假设在婴儿的年龄中有一个极值(人工输入错误)。因为我们的目标是在获得简单线性回归的 a 和 b 值时使总误差最小化，所以独立变量中的极值会对参数产生相当大的影响。通过改变任何年龄值的值并计算使总误差最小化的 a 和 b 的值，可以看到这种工作。在这种情况下，您会注意到，即使给定的 a 和 b 值的总体误差较低，但它会导致大多数其他数据点的误差较高。

为了避免刚刚提到的第一个问题，分析师通常会看到两个变量之间的关系，并确定我们可以应用线性回归的临界值(段)。例如，当根据年龄预测身高时，有不同的时间段:0-1 岁、2-4 岁、5-10 岁、10-15 岁、15-20 岁和 20 岁以上。每个阶段的年龄-身高关系都有不同的斜率。例如，与 2-4 阶段相比，0-1 阶段的身高增长率较高，2-4 阶段比 5-10 阶段好，依此类推。

为了解决提到的第二个问题，分析师通常会执行以下任务之一:

*   将异常值标准化为第 99 百分位值:标准化为第 99 百分位值可以确保异常高的值不会对结果产生很大影响。例如，在前面的示例场景中，如果年龄被错误地输入为 1200 而不是 12，它将被规范化为 12(这是年龄列中的最高值之一)。
*   Normalize，但创建一个标记，指出特定变量已被规范化:有时极值中有很好的信息。例如，在预测信用额度时，让我们考虑这样一个场景:九个人收入为 500，000 美元，第十个人收入为 5，000，000 美元，申请信用卡，每个人的信用额度为 5，000，000 美元。让我们假设给予一个人的信用额度至少是其收入的 10 倍或 500 万美元。对此进行线性回归将导致斜率接近 10，但小于 10，因为一个人获得了 500 万美元的信贷限额，尽管他们的收入是 500 万美元。在这种情况下，如果我们有一个标志，说明收入为 500 万美元的人是一个异常值，斜率将接近 10。

异常值标记是多元回归的一种特殊情况，在这种情况下，我们的数据集中可能有多个独立变量。

## 多元线性回归

多元回归，顾名思义，涉及多个变量。

到目前为止，在一个简单的线性回归中，我们观察到因变量是根据单个自变量预测的。在实践中，多个变量通常会影响一个因变量，这意味着多元变量比简单的线性回归更常见。

在第一部分中提到的相同的冰淇淋销售问题可以转化为如下的多变量问题:

冰淇淋销售额(因变量)取决于以下因素:

*   温度
*   周末与否
*   冰淇淋的价格

这个问题可以通过以下方式转化为数学模型:

![$$ Y=a+{w_1}^{\ast }{X}_1+{w_2}^{\ast }{X}_2 $$](A463052_1_En_2_Chapter_Equi.gif)

在该等式中，w <sub>1</sub> 是与第一自变量相关联的权重，w <sub>2</sub> 是与第二自变量相关联的权重(系数)，a 是偏差项。

a、w <sub>1</sub> 和 w <sub>2</sub> 的值将按照我们在简单线性回归(Excel 中的求解器)中求解 a 和 b 的方式求解。

多元线性回归汇总的结果和解释与我们在前面章节中看到的简单线性回归相同。

对上述场景的示例解释如下:

冰淇淋销量= 2 + 0.1 ×温度+ 0.2 ×周末旗帜-0.5×冰淇淋价格

前面的等式解释如下:如果温度上升 5 度，而其他参数保持不变(即在某一天，价格保持不变)，冰淇淋的销售额增加 0.5 美元。

### 多元线性回归的工作细节

要了解多元线性回归是如何计算的，我们来看下面的例子(在 github 中以“linear_multi_reg_example.xlsx”的形式提供):

![A463052_1_En_2_Figi_HTML.jpg](A463052_1_En_2_Figi_HTML.jpg)

对于前面的数据集，其中 Weight 是因变量，Age、New 是自变量，我们将按如下方式初始化估计值和随机系数:

![A463052_1_En_2_Figj_HTML.jpg](A463052_1_En_2_Figj_HTML.jpg)

在这种情况下，我们将迭代 a、b 和 c 的多个值，即最小化总体平方误差值的像元 H3、H4 和 H5。

### R 中多元线性回归

多元线性回归可以在 R 中如下执行(可作为“多元线性回归”获得。github 中的 r”)。

![A463052_1_En_2_Figk_HTML.jpg](A463052_1_En_2_Figk_HTML.jpg)

```
# import file
data=read.csv("D:/Pro ML book/Linear regression/linear_multi_reg_example.csv")
# Build model
lm=glm(Weight~Age+New,data=data)
# summarize model
summary(lm)

```

注意，我们已经通过在独立变量之间使用`+`符号为回归指定了多个变量。

我们可以在输出中注意到一个有趣的方面，即`New`变量的 p 值大于 0.05，因此是一个无关紧要的变量。

通常，当 p 值较高时，我们测试变量转换或变量封顶是否会导致获得较低的 p 值。如果前面的技术都不起作用，我们最好排除这些变量。

我们在这里可以看到的其他细节的计算方式类似于前面几节中的简单线性回归计算。

### Python 中的多元线性回归

与 R 类似，Python 在公式部分也有一个小的增加，以适应简单线性回归之上的多元线性回归:

```
# import relevant packages
# pandas package is used to import data
# statsmodels is used to inoke the functions that help in lm
import pandas as pd
import statsmodels.formula.api as smf
# import dataset
data = pd.read_csv('D:/Pro ML book/Linear regression/linear_multi_reg_example.csv')
# run least squares regression
est = smf.ols(formula='Weight~Age+New',data=data)
est2=est.fit()
print(est2.summary())

```

### 模型中有不重要变量的问题

当 p 值较高时，变量不重要。与系数值相比，当标准误差较高时，p 值通常较高。当标准误差较高时，表示为多个样本生成的多个系数中存在较高的方差。当我们有一个新数据集(即测试数据集，在构建模型时模型看不到它)时，系数不一定会针对新数据集进行归纳。

当模型中包含非显著变量时，这将导致测试数据集的 RMSE 较高，而当模型构建中不包含非显著变量时，RMSE 通常较低。

### 多重共线性问题

构建多变量模型时要注意的一个主要问题是自变量何时可能彼此相关。这种现象称为多重共线性。例如，在冰淇淋示例中，如果冰淇淋价格在周末上涨 20%，则两个独立变量(价格和周末标志)相互关联。在这种情况下，在解释结果时需要小心——其余变量保持不变的假设不再成立。

例如，我们不能再假设在周末唯一变化的变量是周末标志；我们还必须考虑到价格在周末也会变化。这个问题转化为，在给定的温度下，如果这一天恰好是周末，由于是周末，销售额增加了 0.2 个单位，但是由于价格在周末期间增加了 20%，销售额减少了 0.1 个单位，因此，周末销售额的净效应是+0.1 个单位。

### 多重共线性的数学直觉

为了对自变量之间存在相互关联的变量所涉及的问题有所了解，请考虑以下示例(代码可作为“相关自变量的问题”。github 中的 r”)。

![A463052_1_En_2_Figl_HTML.jpg](A463052_1_En_2_Figl_HTML.jpg)

```
# import dataset
data=read.csv("D:/Pro ML book/linear_reg_example.csv")
# Creating a correlated variable
data$correlated_age = data$Age*0.5 + rnorm(nrow(data))*0.1
cor(data$Age,data$correlated_age)
# Building a linear regression
lm=glm(Weight~Age+correlated_age,data=data)
summary(lm)

```

请注意，尽管在前面的示例中，年龄是预测体重的一个重要变量，但当数据集中存在相关变量时，年龄会变成一个不重要的变量，因为它具有较高的 p 值。

数据样本的年龄和相关年龄系数差异较大的原因是，尽管年龄和相关年龄变量相关，但年龄和相关年龄的组合(当作为单个变量处理时，即两个变量的平均值)的系数差异较小。

假设我们使用两个变量，根据样本的不同，Age 可能具有较高的系数，而 correlated_age 可能具有较低的系数，对于其他一些样本则相反，从而导致所选样本的两个变量的系数差异较大。

### 多元线性回归中需要考虑的其他问题

*   回归系数过高是不可取的:虽然回归在某些情况下可以有很高的系数，但一般来说，系数的高值会导致预测值的巨大摆动，即使自变量变化 1 个单位。例如，如果销售是价格的函数，其中销售= 1，000，000–100，000 x 价格，价格的单位变化会大幅减少销售。在这种情况下，为了避免这个问题，建议通过将销售额改为 log(sales)而不是 sales 来降低销售额，或者归一化销售额变量，或者通过 L1 和 L2 正则化来惩罚具有高权重的模型(更多关于 L1/ L2 正则化的信息，请参见第 [7](07.html) 章)。这样，等式中的 a 和 b 值保持较小。
*   回归应该建立在大量观察的基础上:一般来说，数据点的数量越多，模型越可靠。而且，自变量的数量越多，需要考虑的数据点就越多。如果我们只有两个数据点和两个独立变量，我们总能得出一个适合这两个数据点的方程。但是仅仅建立在两个数据点上的等式的一般化是有问题的。实际上，建议数据点的数量至少是自变量数量的 100 倍。

行数少或列数多，或者两者都有，这就引出了调整后的 R 平方的问题。如前所述，一个方程中的自变量越多，它最接近因变量的可能性就越大，因此 R 的平方越高，即使自变量不显著。因此，应该有一种方法来惩罚在一组固定的数据点上有大量的独立变量。调整后的 R 平方考虑了方程中使用的独立变量的数量，并因拥有更多独立变量而受到惩罚。调整后的 R 平方的公式如下:

![$$ {R}_{adj}^2=1-\left[\frac{\left(1-{R}^2\right)\left(n-1\right)}{n-k-1}\right] $$](A463052_1_En_2_Chapter_Equj.gif)

其中 n 是数据集中数据点的个数，k 是数据集中自变量的个数。

具有最小调整的 R 平方的模型通常是更好的模型。

## 线性回归的假设

线性回归的假设如下:

*   自变量必须与因变量呈线性关系:如果线性水平随区段而变化，则为每个区段建立一个线性模型。
*   独立变量之间的值不应该有任何异常值:如果有异常值，它们应该被限制，或者需要创建一个新的变量来标记异常值的数据点。
*   Error values should be independent of each other: In a typical ordinary least squares method, the error values are distributed on both sides of the fitted line (that is, some predictions will be above actuals and some will be below actuals), as shown in Figure [2-4](#Fig4). A linear regression cannot have errors that are all on the same side, or that follow a pattern where low values of independent variable have error of one sign while high values of independent variable have error of the opposite sign.

    ![A463052_1_En_2_Fig4_HTML.jpg](A463052_1_En_2_Fig4_HTML.jpg)

    图 2-4

    Errors on both sides of the line
*   Homoscedasticity: Errors cannot get larger as the value of an independent variable increases. Error distribution should look more like a cylinder than a cone in linear regression (see Figure [2-5](#Fig5)). In a practical scenario, we can think of the predicted value being on the x-axis and the actual value being on the y-axis.

    ![A463052_1_En_2_Fig5_HTML.jpg](A463052_1_En_2_Fig5_HTML.jpg)

    图 2-5

    Comparing error distributions
*   Errors should be normally distributed: There should be only a few data points that have high error. A majority of data points should have low error, and a few data points should have positive and negative error—that is, errors should be normally distributed (both to the left of overforecasting and to the right of underforecasting), as shown in Figure [2-6](#Fig6).

    ![A463052_1_En_2_Fig6_HTML.jpg](A463052_1_En_2_Fig6_HTML.jpg)

    图 2-6

    Comparing curves

Note

在图 [2-6](#Fig6) 中，如果我们稍微调整了右边图表中的偏差(截距),现在更多的观察值将围绕零误差。

## 摘要

在本章中，我们学习了以下内容:

*   误差平方和(SSE)是计算线性回归系数的优化基础。
*   当多个独立变量相互关联时，多重共线性就是一个问题。
*   p 值是预测因变量时变量显著性的指标。
*   为了使线性回归起作用，应该满足五个假设，即，因变量和自变量之间的线性关系、没有异常值、误差值独立性、同方差性、误差的正态分布。