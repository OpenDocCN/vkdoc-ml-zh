# 15.如何在金融领域实现机器学习

金融业最有前途的三个领域是:

*   **算法炒股**

*   自动化机器人顾问

*   **欺诈检测和防范**

在金融行业，潜力巨大的最重要的项目是将机器学习应用于算法股票交易。虽然算法股票交易是人工股票交易的自动化版本，但我会说它缺乏机器学习可以在预测和预测方面提供的人工智能，而不是以今天的形式进行分析和行动。为了举例说明我的意思，让我们看看一个散户如何在全球股市交易。为了使讨论简单，我说的是现金交易，而不是衍生品期权交易。在任何一个典型的日子里，散户投资者都会研究关于一家公司的新闻，检查其基本面和技术分析头寸，然后做出卖出或买入股票的决定。例如，如果交易者跟踪商品通道指数(CCI)技术指标，那么他们会看 CCI 显示超卖信号开始买入，超买信号开始卖出。

虽然这是一个简单的例子，但有经验的交易者会使用比 CCI 更多的技术指标，比如随机振荡指标和布林线指标。当前形式的算法交易已经成功地自动化了技术指标的使用，并将其整合到自动化交易系统中。然而，这不是人工智能，也不是机器学习。如果我们仔细观察，这里几乎没有任何预测。这只是当市场上涨或下跌时，人类会做的自动化操作，然后根据价格运动采取行动。这里几乎没有任何预测。例如，在该系统中没有构建模型来预测股票在接下来的 1 小时、1 天、1 周、1 个月的价格，或者预测任何其他这样的时间段的股票价格运动。如果围绕这一点建立一个模型，那么将它与算法交易结合起来将会产生奇妙的结果。因此，在股票市场中应用机器学习来预测股票价格或指数增加了一些智能，然后使用前景分析，一个完整的系统给出关于 algo 交易系统要执行的动作的建议，这将使它有足够的智能来利用价格运动。

另一个很有潜力的金融领域是金融和投资顾问。这里有很大的实施空间。让我们举个例子:一个人有一百万美元可以投资。然而，尽管信息普及程度有所提高，但任何投资者都面临的问题是，选择最佳投资选项，让资本随着时间的推移而增值。在这里，投资者利用他们的直觉和朋友网络或人类顾问给他们建议在哪里投资，在哪里不投资。然而，这是一个非常幼稚的大笔投资过程。谁也不知道这些投资中有多少是失败的，有多少是成功的。然而，如果我们使用来自资本市场的关于价格和其他影响它们的因素的现有数据，那么我们可以建立一个预测模型，并确保投资不会随着时间的推移而失败。我指的不是目前的机器人操作，比如亚马逊的 Alexa，它只能告诉你当前的市场情况；我的意思是创造一个机器人顾问，它不仅能创造一个概念上的投资组合，还能根据过去的数据，如股票价格、可收藏的艺术品价格、商品价格、石油价格等，给投资者提供预测。这种机器人财务顾问不仅有能力创建机器学习模型，还能利用它们并对其所有者的投资组合提出建议。根据市场价格的变化，它会提前足够长的时间提醒其所有者对其投资组合的建议变化。

欺诈检测和防范是另一个需要升级的领域。虽然在这一领域有机器学习和人工智能的应用，但它们不足以防止欺诈在全球范围内发生。欺诈检测系统需要通过发现洗钱者和其他黑客滥用系统的模式来使用机器学习。黑客企图存在于网络安全领域；然而，与洗钱和欺诈相关的交易很难被发现。这样做的主要原因是系统将用户识别为合法用户，因此不标记他们。但是，通过使用异常值检测等技术并将其与其他欺诈检测算法相结合，可以构建一个不允许实时进行此类交易的健壮系统。对已经发生的交易发出危险信号，除了对系统用户提出刑事指控(这些用户在欺诈被发现时可能已经逃离)之外，没有任何帮助。然而，通过创建机器学习模型，例如我们在本书第 [14](14.html) 章中看到的模型，我们可以很容易地实时检测和阻止这种交易的发生。

## 在金融领域实现机器学习生命周期

现在，我们将看看机器学习在金融行业的实现。在这里，我将使用监督机器学习技术向您展示如何在会计数据集上实现。本例中使用的数据集是虚构的，但基于任何组织通常使用的电子支付分类账，适合您了解如何在会计领域实施机器学习。

现在让我们看看本章将要用到的数据集，看看数据字典，并在开始处理它之前理解它。这里描述了数据集中的列。

*   **记录号:**这是数字支付登记簿中分类账交易的记录号。

*   **交易号:**这是支付网关在成功完成支付交易时提供的唯一交易号。

*   **财政年度:**这是交易发生的财政年度。

*   **月:**该条目给出交易发生的月份。

*   **部门:**部门栏目包括公司的财务、人力资源、学习与发展、法律、营销、采购、研究、战略规划和运输部门。

*   **账户:**账户列表示公司订单在其分类账中的账号。这是公司内部的事情。

*   **费用类别:**费用类别代表公司在向供应商付款时的费用类型。

*   **供应商 ID:** 由支付或采购系统签署给接受付款的供应商。

*   **付款方式:**这一栏讲述了所使用的付款方式的类型，例如自动票据交换所支票或电汇。

*   **付款状态:**付款状态是对已经对账的付款交易的确认，不仅与银行对账，还与内部采购委员会对账。内部支付意味着付款已经得到银行的批准和确认，并且付款已经得到组织内部采购委员会的批准。

*   **付款日期:**这是交易发生的实际付款日期。

*   **发票 ID:** 这是已经生成的发票的发票 ID，付款交易是根据该发票生成的。

*   **发票日期:**表示发票生成的日期。

*   **金额:**已发生的支付交易的实际金额。

*   **红色标志:**列值 1 表示交易金额低于金额列的第 25 个分位数，支付状态为未对账支付。危险信号交易需要进一步调查。代表交易的值高于第 75 个分位数，并且具有“信任”和“协调”状态。这说明还需要进一步调查。值为 0 表示交易在 1.5 倍的第 25 个分位数内，因此被视为合法。

在这个数据集中，我将使用特定分类器的监督学习技术来对合法的交易进行分类。

该数据集采用平面文件 PaymentsLedger.csv 的形式，可从以下 URL 获得: [`http://www.PuneetMathur.me/Book009/`](http://www.PuneetMathur.me/Book009/) 。

### 启动代码

现在让我们开始在这个数据集上实现无监督学习。

*   **信息:本练习中使用的所有代码都在 Anaconda 环境中测试了与 Python 2.7 的兼容性。它应该也能在 3.x 上工作；但是，还没有经过测试。**

在清单 [15-1](#PC1) 中，导出 Python 库，如 String IO 和 numpy，以便开始工作。

```
#Importing python libraries
import pandas as pd
from io import StringIO
import os
import numpy as np
os.getcwd()

Listing 15-1Importing Basic Python Libraries

```

在清单 [15-2](#PC2) 中，我从平面文件中加载数据集，然后查看加载到数据帧中的数据集。

```
#Reading data set from flat files
fname="C:/PaymentsLedger.csv"
openledger= pd.read_csv(fname, low_memory=False, index_col=False)

#Check the data Loaded into memory
print(openledger.head(1))

   Record No  TransactionNo  Fiscal Year  Month Department  Account  \
0          0        1095300         2018      1    FINANCE    54050

   Expense Category Vendor   ID        Payment Method     Payment Status  \
0  Food And Beverages     59169  Automated Clearing House  Paid-Unreconciled

  Payment Date     Invoice ID Invoice Date  Amount  RedFlag
0   07/21/2017  DDSM100367442   07/19/2017   10.08        1

Listing 15-2Loading and Checking Data Set in Memory

```

接下来，在清单 [15-3](#PC3) 中，我将数据传输到一个标准的 Pandas 数据帧中，然后检查数据集是否正确加载。然后我观察数据集的形状，它告诉我们 7500 行和 13 列。我们还查看列名，这与我们在本节开始时讨论的数据字典相同。我使用 dataframe 的 dtypes 属性来查看已经装载的列的数据类型。

```
dfworking= pd.DataFrame(openledger)
#Look at the first record
print(dfworking.head(1))
#Check the shape size and columns in the dataset

print(dfworking.shape)
print(dfworking.columns)
dfworking.dtypes
   Record No  TransactionNo  Fiscal Year  Month Department  Account  \
0          0        1095300         2018      1    FINANCE    54050

   Expense Category Vendor   ID        Payment Method      Payment Status \
0  Food And Beverages     59169  Automated Clearing House  Paid-Unreconciled

  Payment Date     Invoice ID Invoice Date  Amount  RedFlag
0   07/21/2017  DDSM100367442   07/19/2017   10.08        1
(7293, 15)
Index([u'Record No', u'TransactionNo', u'Fiscal Year', u'Month', u'Department',
u'Account', u'Expense Category', u'Vendor ID', u'Payment Method',
u'Payment Status', u'Payment Date', u'Invoice ID', u'Invoice Date',
u'Amount', u'RedFlag'],
      dtype='object')
Out[20]:
Record No             int64
TransactionNo         int64
Fiscal Year           int64
Month                 int64
Department           object
Account               int64
Expense Category     object
Vendor ID            object
Payment Method       object
Payment Status       object
Payment Date         object
Invoice ID           object
Invoice Date         object
Amount              float64
RedFlag               int64
dtype: object

Listing 15-3Loading Pandas Dataframe and the Column Datatypes

```

接下来，在清单 [15-4](#PC4) 中，我查看我们是否有任何丢失的数据，以了解我们是否需要在开始预处理数据之前处理它。

```
dfworking.isnull().any()
#Counting the Number of Null rows in each Column of the dataframe
dfworking.isnull().sum()
Out[21]:
Record No           False
TransactionNo       False
Fiscal Year         False
Month               False
Department          False
Account             False
Expense Category    False
Vendor ID           False
Payment Method      False
Payment Status      False
Payment Date        False
Invoice ID          False
Invoice Date        False
Amount              False
RedFlag             False
dtype: bool

Out[22]:
Record No           0
TransactionNo       0
Fiscal Year         0
Month               0
Department          0
Account             0
Expense Category    0
Vendor ID           0
Payment Method      0
Payment Status      0
Payment Date        0
Invoice ID          0
Invoice Date        0
Amount              0
RedFlag             0
dtype: int64

Listing 15-4Finding Empty Rows in the Dataframe

```

在检查了数据帧中的空值之后，我们发现没有空列或空列。然而，在现实世界中，情况并非如此，因此您可能需要处理和处理各个列中缺失的值。在我们继续之前，让我向您介绍一下我将在这个机器学习模型构建中使用的一些功能。Python 的第一个功能是分发。我将使用这个函数来创建分布图，以查看和比较它们，并查看算法之间的结果。我还将使用它来建立基准预测算法。让我们看看清单 [15-5](#PC5) 中的代码。

```
Visualization Functions which will be used throughout this model
import matplotlib.pyplot as pl
import matplotlib.patches as mpatches
from time import time
from sklearn.metrics import f1_score, accuracy_score

def distribution(data, transformed = False):
    """
    Visualization code for displaying skewed distributions of features
    """

    # Create figure
    fig = pl.figure(figsize = (11,5));

    # Skewed feature plotting
    for i, feature in enumerate(['Amount','Month', 'Fiscal Year']):
        ax = fig.add_subplot(1, 3, i+1)
        ax.hist(data[feature], bins = 25, color = '#00A0A0')
        ax.set_title("'%s' Feature Distribution"%(feature), fontsize = 14)
        ax.set_xlabel("Value")
        ax.set_ylabel("Number of Records")
        ax.set_ylim((0, 2000))
        ax.set_yticks([0, 500, 1000, 1500, 2000])
        ax.set_yticklabels([0, 500, 1000, 1500, ">2000"])

    # Plot aesthetics

    if transformed:
        fig.suptitle("Log-transformed Distributions of Continuous Census Data Features", \
            fontsize = 16, y = 1.03)
    else:
        fig.suptitle("Skewed Distributions of Continuous Census Data Features", \
            fontsize = 16, y = 1.03)

    fig.tight_layout()
    fig.show()
#End of Distribution Visualization function

Listing 15-5Distribution Function

```

分布函数输入参数相同，通知第二个参数说明是否需要转换所需的数据帧。这是为了以后应用日志转换。在函数的第一行，我使用 Matlab figure 函数创建了一个图形，图形大小为 11 和 5。之后，我在 for 循环中创建了一个特性图，它枚举了 amount month 和 fiscal year 列。请注意，这些列实际上是数字。之后，我将这些列中的每一列作为子图添加到图中，然后创建一个包含 25 部电影的数据特征的直方图，并为特征分布提供一个标题，将 x 标签和 y 标签值设置为白色 x 值和标签中的 x 刻度。在 for 循环完成创建图形之后，下一步检查第二个输入参数。如果需要进行对数转换，则需要进行确认，然后用对数转换后的分布附加一个单独的图形超级标题；否则就是连续普查数据特征的偏态分布。所以我用这个函数来创建一个对数转换分布和一个偏斜分布。完成后，图形与字体布局一起使用，并显示在屏幕上。一个简单的函数确实可以很好地显示转换分布和偏斜分布的专业成绩。

### 特征重要性

接下来，我们看看清单 [15-6](#PC6) 中称为 feature_plot()的特征重要性函数。

![../images/464968_1_En_15_Chapter/464968_1_En_15_Fig1_HTML.jpg](../images/464968_1_En_15_Chapter/464968_1_En_15_Fig1_HTML.jpg)

图 15-1

前五个最具预测性特征的归一化权重

```
# Plotting Feature Importances through this function
def feature_plot(importances, X_train, y_train):

    # Display the five most important features
    indices = np.argsort(importances)[::-1]
    columns = X_train.columns.values[indices[:5]]
    values = importances[indices][:5]

    # Creat the plot
    fig = pl.figure(figsize = (9,5))
    pl.title("Normalized Weights for First Five Most Predictive Features", fontsize = 16)
    pl.bar(np.arange(4), values, width = 0.6, align="center", color = '#00A000', \
          label = "Feature Weight")
    pl.bar(np.arange(4) - 0.3, np.cumsum(values), width = 0.2, align = "center", color = '#00A0A0', \
          label = "Cumulative Feature Weight")
    pl.xticks(np.arange(5), columns)
    pl.xlim((-0.5, 4.5))
    pl.ylabel("Weight", fontsize = 12)
    pl.xlabel("Feature", fontsize = 12)

    pl.legend(loc = 'upper center')

    pl.tight_layout()
    pl.show()
#End of Feature Importances function

Listing 15-6Feature Importances Function

```

`feature_plot`函数接受第一个参数作为特征重要性，第二个参数作为`X_train`和第三个参数`y_train`。首先，我从 importances 变量的末尾创建了一个所有重要特性的排序，然后我将来自`x_train`的列名和来自 importance 变量的值放入 values 变量。之后，我创建了一个条形图，其中包含从 importances 变量中提取的值；这只是五个，因为我们想看看这里的前五个重要特性。

### 观察异常值

我们现在来看看数据集中的异常值。我想提醒来自医疗保健和零售这两个行业的读者，我们已经将异常值定义为小于金额列下分位数的 1.5 倍且大于金额列上分位数的 1.5 倍的值。下四分位数是 0.25，上四分位数是 0.75。

```
fname="C:/DATASETS/data.ct.gov/PaymentsDataset.csv"
openledger= pd.read_csv(fname, low_memory=False, index_col=False)

#Verify the data Loaded into memory
print(openledger.head(1))
   Unnamed: 0  TransactionNo  Fiscal Year  Month Department  Account  \
0           0        1095300         2018      1    FINANCE    54050

     Expense Category Vendor ID            Payment Method     Payment Status  \
0  Food And Beverages     59169  Automated Clearing House  Paid-Unreconciled

  Payment Date     Invoice ID Invoice Date  Amount  RedFlag
0   07/21/2017  DDSM100367442   07/19/2017   10.08        1

data= pd.DataFrame(openledger)

Listing 15-7Load the Data Set

```

为了将支付数据集解码成一个数据框架，然后开始研究离群值并探索它，我们不打算研究数据类型并在面积图中研究它们。我们知道我们的数据集是为了增强无空值。在清单 [15-8](#PC8) 中，我提供了异常值的计算。

```
#Total number of records
n_records = len(data.index)

#Number of records where payments are below 1.5 times of upper Quantile- upper Outlier Limit

l=data[data['RedFlag'] == 2].index
n_greater_quantile = len(l)

#Number of records where payments are above 1.5 times of lower Quantile- lower Outlier limit
l=data[data['RedFlag'] == 1].index
n_lower_quantile = len(l)

#Percentage of Payments above Upper Outlier limit
p=float(n_greater_quantile)/n_records*100.0
greater_percent =p

#Percentage of Payments above Lower Outlier limit
p=float(n_lower_quantile)/n_records*100.0
lower_percent =p

# Print the results
print "Total number of records: {}".format(n_records)
print "High value Payments above 1.5 times of 75th Percentile: {}".format(n_greater_quantile)
print "Low value Payments below 1.5 times of 25th Percentile: {}".format(n_lower_quantile)
print "Percentage of high value Payments: {:.2f}%".format(greater_percent)
print "Percentage of low value Payments: {:.2f}%".format(lower_percent)

Total number of records: 7293
High value Payments above 1.5 times of 75th Percentile: 366
Low value Payments below 1.5 times of 25th Percentile: 748
Percentage of high value Payments: 5.02%
Percentage of low value Payments: 10.26%

Listing 15-8Finding the Outliers in the Payments Data Set

```

在我解释异常值之前，我想告诉您，用于确定欺诈或洗钱交易的异常值只是用于欺诈检测的众多技术之一。我使用非常简单的设置和数据集以及最简单的技术来确定交易是否是欺诈，以便您可以理解我们如何使用和建立机器学习模型来确定此类交易。在 n_ records 变量中，我取数据帧中记录数量的长度。接下来，我设置了离群值的上限和下限。整个想法是列出落在异常值上限或下限内的记录或事务的数量。我们从数据字典描述中知道，红色标记列值为 2 意味着一条记录落在异常值上限内。类似地，红色标志 1 意味着交易量低于穷人最喜欢的变量 n_lower_quantile，该变量用于找出满足下限的行数。类似地，变量 n_lower_quantile 用于确定付款分类帐中有多少付款交易或记录落在异常值下限内。之后，它会计算超出较大异常值限制和较低异常值限制的支付百分比。结果旨在解释使用该变量的语句。您可以看到，有 366 笔高值支付在异常上限附近，740 笔交易在异常下限附近。高于异常值上限的事务约占总事务的 5%，低于异常值下限的事务占事务的 10.26%。

```
# PREPARING DATA
# Split the data into features and target label
payment_raw = pd.DataFrame(data['RedFlag'])
type(payment_raw)
features_raw = data.drop('RedFlag', axis = 1)
#Removing redundant columns from features_raw dataset
features_raw.dtypes
features_raw=features_raw.drop('TransactionNo', axis=1)
features_raw=features_raw.drop('Department', axis=1)
features_raw=features_raw.drop('Account', axis=1)
features_raw=features_raw.drop('Expense Category', axis=1)
features_raw=features_raw.drop('Vendor ID', axis=1)
features_raw=features_raw.drop('Payment Method', axis=1)
features_raw=features_raw.drop('Payment Date', axis=1)
features_raw=features_raw.drop('Invoice ID', axis=1)
features_raw=features_raw.drop('Invoice Date', axis=1)
features_raw=features_raw.drop('Unnamed: 0', axis=1)
features_raw.dtypes
type(features_raw)
Fiscal Year         int64
Month               int64
Payment Status     object
Amount            float64
dtype: object

Listing 15-9Preparing Data

```

### 准备数据集

既然我们已经确定我们的数据集中有很大比例的交易属于欺诈类别或危险信号，我们现在可以通过划分为特征和目标标签来准备我们的数据集。这在图 [15-2](#Fig2) 中完成。您会注意到，为了只保留那些对我们的模型构建很重要的特性，我去掉了相当多无用的特性，比如部门、账户、类别 ID、付款方式、付款日期、发票日期和发票 ID。这些列在我们的模型构建过程中非常重要。大多数时候，模型构建是一个迭代的过程，因此，如果我后来发现一些特性是必需的，可能被错误地选择了，或者没有给出足够的准确性，那么我可以考虑添加一些列，如 department、expense category 等。

在图 [15-2](#Fig2) 中，我展示了三列的分布:金额、月份和财政年度。请注意，所有这些列都是数字。分布函数不适用于非数字列。

![../images/464968_1_En_15_Chapter/464968_1_En_15_Fig2_HTML.jpg](../images/464968_1_En_15_Chapter/464968_1_En_15_Fig2_HTML.jpg)

图 15-2

可视化数字列

```
# Visualize skewed continuous features of original data
distribution(data)

```

在图 [15-2](#Fig2) 中，我使用了分布数据来显示金额、月份和财政年度的分布，我们可以清楚地看到，它们都不在正态分布曲线附近。所以这些是需要被转换的高度偏斜的分布。我将对这些功能应用的第一个转换是对数转换。请记住，与平均值的距离只能发生在数字特征上，因此我们选择了我们在图 [15-2](#Fig2) 的可视化中看到的三列或特征。应用对数变换后，下一步将是对这些要素应用比例，以便必须移除这些要素中每一个要素的巨大变化-例如，如果金额的变化范围从 0 到大于 2000，但是月最多只有 8000 条记录，财政年度也是如此。此外，金额以美元计量，月份从 0 到 12 计量，财政年度从一年到另一年计量。因此，如果我们不应用缩放器，那么我们将比较或使用不一致的特征，这就像比较苹果和桔子一样。在图 [15-3](#Fig3) 中，我们将看到我们的数据转换为对数转换，以及在数值数据集上应用最小最大缩放器，因为数据集中的特征具有非常不同的比例，并且包含一些非常大的异常值。这两个特征导致数据可视化困难，更重要的是，它们会降低许多机器学习算法的预测性能。未缩放的数据也会减缓甚至阻止许多基于梯度的估计器的收敛[ [1](#Par61) ]。

![../images/464968_1_En_15_Chapter/464968_1_En_15_Fig3_HTML.jpg](../images/464968_1_En_15_Chapter/464968_1_En_15_Fig3_HTML.jpg)

图 15-3

在支付数据集上应用对数变换和定标器

```
# Log-transform the skewed features
#Replacing Null values with zero due to software data entry problem
#Known issue in software user screen takes null values there is no check.
import warnings
warnings.filterwarnings("ignore")
features_raw.isnull().sum()

skewed = ['Amount','Month', 'Fiscal Year']
features_raw[skewed] = data[skewed].apply(lambda x: np.log(x + 1))
features_raw.isnull().sum()
features_raw.fillna(0, inplace=True)
features_raw.dtypes

# Visualize the new log distributions
distribution(features_raw, transformed = True)

#Normalizing Numerical Features
# Import sklearn.preprocessing.StandardScaler
from sklearn.preprocessing import MinMaxScaler

# Initialize a scaler, then apply it to the features
scaler = MinMaxScaler()
numerical = [ 'Amount','Month', 'Fiscal Year']
features_raw[numerical] = scaler.fit_transform(data[numerical])

```

如果你比较图 [15-2](#Fig2) 和图 [15-3](#Fig3) ，那么这两个图清楚地显示了对数变换和缩放的效果，其中在图 [15-3](#Fig3) 中，在应用变换之后，金额特征几乎被标准化，月份特征也有一点改变，但是在财政年度特征中可以看到非常小的改变。

### 编码列

接下来，我将对一个支付状态列进行编码，这样做的原因是 Python scikit 学习库也需要分类变量的数值。在付款状态列中，我们只有两个值:一个是“已付-已对账”，另一个是“已付-未对账”。在清单 [15-10](#PC12) 中，我将“已支付-已协调”热编码为值 0，将“已支付-未协调”热编码为值 1。然后我看看编码完成后我们有多少功能。

```
# Encoding the 'Non-Numeric' data to numerical values
#Payment Status column
d={"Paid-Reconciled": 0, "Paid-Unreconciled": 1}
features_raw['Payment Status'] = features_raw['Payment Status'].map(d)

# Printing the number of features after one-hot encoding
encoded = list(features_raw.columns)
print "{} total features after one-hot encoding.".format(len(encoded))

4 total features after one-hot encoding.

Listing 15-10Applying Hot Encoding

```

### 将数据分割成要素

在代码的下一部分(在清单 [15-11](#PC13) 中给出)，我将使用 SKlearn 包中的 train test split 函数将数据分割成特性和红旗。

```
# Importing train_test_split
from sklearn.cross_validation import train_test_split
payment_raw.columns
# Splitting the 'features' and 'RedFlags' data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_raw, payment_raw, test_size = 0.2, random_state = 0)
X_train.columns
X_test.columns
y_train.columns
y_test.columns
# Showing the results of the split
print "Training set has {} samples.".format(X_train.shape[0])
print "Testing set has {} samples.".format(X_test.shape[0])
Training set has 5834 samples

.
The testing set has 1459 samples.

Listing 15-11Dividing the Data into a Training Set and Test Set

```

### 评估模型性能

在此分割之后，我现在将执行模型性能评估，首先使用一个基准分类器 nave-Bayes，然后计算其准确度分数并导入五个其他分类器，比较它们之间的结果。您可以在清单 [15-12](#PC14) 和 [15-13](#PC15) 中看到结果。

```
#Evaluating Model Performance
#Establishing Benchmark performance indicator Naive Bayes
#Naive Predictor Performace
from sklearn.naive_bayes import GaussianNB
#from sklearn.metrics import accuracy_score, fbeta_score
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix
NB = GaussianNB()
NB.fit(X_train,y_train)
pred = NB.predict(X_test)

#Calculating Accuracy Score
#Calculating Beta Score
accuracy_score(y_test,pred)
print(f1_score(y_test,pred, average="macro"))
print(precision_score(y_test, pred, average="macro"))
print(recall_score(y_test, pred, average="macro"))
0.8064675486756182
0.7243305924520346
0.9656357388316151

Listing 15-12Scoring the Benchmark Predictor

```

我们看到基准预测器的 F1 分数、精度分数和召回分数都很好，分别为 0.80、0.72 和 0.96。现在我们有了基准预测器或分类器 nave Bayes 的分数，让我们将它应用于其他分类器，如清单 [15-13](#PC15) 中给出的代码中的决策树、逻辑回归、SGD(梯度下降)、Extratrees 分类器和随机森林分类器。

![../images/464968_1_En_15_Chapter/464968_1_En_15_Fig4_HTML.jpg](../images/464968_1_En_15_Chapter/464968_1_En_15_Fig4_HTML.jpg)

图 15-4

比较来自分类器的结果

```
# I AM GOING TO DO INITIAL MODEL EVALUATION NOW
# Importing the three supervised learning models from sklearn
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize the three models
clf_A = GaussianNB()
clf_B = DecisionTreeClassifier(max_features=0.2, max_depth=2, min_samples_split=2,random_state=0)
clf_C = LogisticRegression(random_state=0)
clf_D = SGDClassifier(loss="hinge", penalty="l2")
clf_E = ExtraTreesClassifier(n_estimators=2, max_depth=2,min_samples_split=2, random_state=0)
clf_F = RandomForestClassifier(max_depth=2)

# Calculate the number of samples for 1%, 10%, and 100% of the training data
#Defining function since percent is required 3 times

# Collect results on the learners
learners=["Naive Bayes","Decision Tree","Logistic Regression","SGD Classifier","ExtaTrees Classifier","RandomFores Classifier"]
cnt=0
columns=['learner','train_time','pred_time','acc_train','acc_test','f1_score']
learningresults= pd.DataFrame(columns=columns)
results = {}

for learner in [clf_A, clf_B, clf_C,clf_D,clf_E,clf_F]:
    #print(learners[cnt])
    results['learner']=learners[cnt]
    #Fitting the learner to the training data using slicing with 'sample_size'
    start = time() # Get start time
    learner.fit(X_train, y_train)
    #Calculating the total prediction time
    end = time() # Get end time
    results['train_time'] =  end - start

    start = time() # Get start time
    predictions_test = learner.predict(X_test)
    predictions_train = learner.predict(X_train)
    end = time() # Get end time
    results['pred_time'] = end - start
    results['acc_train'] = accuracy_score(y_train, predictions_train)
    results['acc_test'] = accuracy_score(y_test, predictions_test)
    beta=0.5
    results['f1_score'] = f1_score(y_test,pred, average="macro")
    print(results)
    learningresults.loc[cnt]=results
    cnt=cnt+1

#Looking at the plots to determine the best Classifier for our Dataset
print(learningresults)
learningresults.columns
learningresults.plot(kind='bar', x="learner", legend="reverse",  title='Classifier Algorithms Compared- Accounts Payment Dataset',figsize=(10,10), fontsize=20)

Listing 15-13Initial Model Evaluation

```

在我们进一步进行模型实现之前，我想讨论学习者数据集的结果，该数据集存储了用于训练时间的 train_time 和用于预测时间的 pred_time、用于训练期间准确性的 acc_train、用于测试期间准确性的 acc_test 以及用于 f1 分数的 f1_score。这些在表 [15-1](#Tab1) 中给出。

有多种方法可以为您的模型选择最佳的学习器或分类器。不同的数据集和应用模型的不同商业情境可以决定哪个学习者最适合你。例如，仅使用一揽子参数进行模型选择(如最低训练时间)不一定是好的，因为它可能是在测试或 f1 分数期间准确度较低的学习者。此外，如果您为将应用于批量离线流程(如月度报告或其他此类夜间流程)的模型选择学习者，那么您的目标可能不是最低的训练和预测时间。如果业务人员希望在最短的预测时间内获得更高的准确性，那么在我们的例子中，您可以选择 SGD 分类器。应用模型的环境更为重要，它决定了模型的选择标准。

表 15-1

比较学习者或分类器

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
| 

学习者

 | 

火车时间

 | 

pred_time

 | 

acc_train

 | 

acc _ 测试

 | 

f1 _ 分数

 |
| --- | --- | --- | --- | --- | --- |
| 决策图表 | Zero point zero zero one | Zero point zero zero one | 0.857045 | 0.880055 | 0.806468 |
| 纳伊夫拜厄斯 | Zero point zero zero two | Zero point zero zero two | 0.910525 | 0.910898 | 0.806468 |
| SGD 分类器 | Zero point zero zero nine | Zero | 0.947377 | 0.946539 | 0.806468 |
| 树外分类器 | Zero point zero one | Zero point zero zero two | 0.858759 | 0.867032 | 0.806468 |
| 逻辑回归 | Zero point zero two eight | Zero point zero zero one | 0.96349 | 0.971213 | 0.806468 |
| 随机森林分类器 | Zero point one four six | Zero point zero one one | 0.948234 | 0.956134 | 0.806468 |

在表 [15-1](#Tab1) 中，所有分类器的 f1 分数都是 0.8064，因此不能考虑进行选择。我使用了逻辑回归，就测试数据集的最少训练时间和最高准确度而言，它是最好的分类器。在清单 [15-14](#PC16) 中，让我们使用这个模型通过 GridSearch 交叉验证进行超参数调优。

```
# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries
from sklearn.metrics import make_scorer
from sklearn.model_selection import GridSearchCV
from IPython.display import display
import pickle, os.path
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import fbeta_score

def getscore(y_true, y_predict):
    return fbeta_score(y_true, y_predict, beta)

best_clf = None
beta=0.5

#Initialize the classifier

clf_C = LogisticRegression(random_state=0)

# Create the parameters list you wish to tune
#parameters = {'n_estimators':range(10,20),'criterion':['gini','entropy'],'max_depth':range(1,5)}
parameters = {'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],'C':range(1,10),'max_iter':range(50,100)}

# Make an fbeta_score scoring object
scorer = make_scorer(getscore)

# Perform grid search on the classifier using 'scorer' as the scoring method
#grid_obj = GridSearchCV(clf_C, parameters, scoring=scorer)

#do something
grid_obj = GridSearchCV(clf_C, parameters)

#grid_obj = GridSearchCV(clf_C, parameters)

# Fit the grid search object to the training data and find the optimal parameters
from datetime import datetime
startTime = datetime.now()

grid_fit = grid_obj.fit(X_train, y_train)
CV_lr = GridSearchCV(estimator=clf_C, param_grid=parameters, cv= 5)
CV_lr.fit(X_train, y_train)
print(datetime.now() - startTime)

    # Get the estimator
best_clf = grid_fit.best_estimator_

# Make predictions using the unoptimized and model
predictions = (clf_C.fit(X_train, y_train)).predict(X_test)
best_predictions = best_clf.predict(X_test)

# Report the before-and-afterscores
print "Unoptimized model\n------"
print "Accuracy score on testing data: {:.4f}".format(accuracy_score(y_test, predictions))
print "F-score on testing data: {:.4f}".format(fbeta_score(y_test, predictions, beta = 0.5,average='micro'))
print "\nOptimized Model\n------"
print "Final accuracy score on the testing data: {:.4f}".format(accuracy_score(y_test, best_predictions))
print "Final F-score on the testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, beta = 0.5,average='micro'))

0:39:21.351000

Unoptimized model
------
Accuracy score on testing data: 0.9712
F-score on testing data: 0.9712

Optimized Model
------
Final accuracy score on the testing data: 0.9925
Final F-score on the testing data: 0.9925

Listing 15-14Tuning the Model Using GridSearch Cross-Validation Method

```

如你所见，我对最佳选择的分类器——逻辑回归——实施了 GridSearch 交叉验证，它花了大约 39 分 21 秒得出结果。优化模型的结果非常好，精确度分数和 F 分数为 0.9925，与未优化模型的精确度分数 0.9712 和 F 分数 0.9712 相比有了显著提高。在清单 [15-15](#PC17) 中，我现在打印最终调整的参数，这些参数将是我们用于预测模型的参数。

```
# Print the final parameters
df = pd.DataFrame(grid_fit.grid_scores_).sort_values('mean_validation_score').tail()
display(df)
print "Parameters for the optimal model: {}".format(clf.get_params())
                                             parameters  \
2098      {u'C': 9, u'max_iter': 69, u'solver': u'sag'}
2099     {u'C': 9, u'max_iter': 69, u'solver': u'saga'}
2100  {u'C': 9, u'max_iter': 70, u'solver': u'newton...
2086    {u'C': 9, u'max_iter': 67, u'solver': u'lbfgs'}
2249     {u'C': 9, u'max_iter': 99, u'solver': u'saga'}

    mean_validation_score                             cv_validation_scores
2098            0.985087  [0.9881808838643371, 0.9845758354755784, 0.982...
2099            0.985087  [0.9881808838643371, 0.9845758354755784, 0.982...
2100            0.985087  [0.9881808838643371, 0.9845758354755784, 0.982...
2086            0.985087  [0.9881808838643371, 0.9845758354755784, 0.982...
2249            0.985087  [0.9881808838643371, 0.9845758354755784, 0.982...
Parameters for the optimal model: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'min_impurity_decrease': 0.0, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 10, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'gini', 'random_state': None, 'min_impurity_split': None, 'max_features': 'auto', 'max_depth': 2, 'class_weight': None}

Listing 15-15Tuned Parameter for Logistic Regression Prediction Model

```

我们可以看到，相关的最佳模型参数是 bootstrap:True，这意味着 GridSearch 交叉验证能够在模型上应用 bootstrap，以便获得最佳结果。有关自举如何帮助模型调优的更多信息，请阅读 IEEE 的研究论文( [`https://ieeexplore.ieee.org/document/6396613/`](https://ieeexplore.ieee.org/document/6396613/) )。其他重要的参数是 n 估计值:10，最大深度:2。

### 确定特征

在找出逻辑回归预测模型的调整参数后，我现在将进入前五个功能的重要性，这将帮助我们确定我们的功能如何在预测模型中帮助我们。您可以使用 feature_plot()函数在清单 [15-16](#PC18) 中看到这段代码，我们在前面的代码中已经看到过。

![../images/464968_1_En_15_Chapter/464968_1_En_15_Fig5_HTML.jpg](../images/464968_1_En_15_Chapter/464968_1_En_15_Fig5_HTML.jpg)

图 15-5

使用分类器的特征选择

```
# Now Extracting Feature Importances
# importing a supervised learning model that has 'feature_importances_'
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import fbeta_score

# Training the supervised model on the training set
model = ExtraTreesClassifier()
model.fit(X_train, y_train)

# TODO: Extract the feature importances

importances = model.feature_importances_

# Plot
feature_plot(importances, X_train, y_train)

Listing 15-16Extracting Feature Importances

```

正如所料，金额是我们模型中最重要的特征，其次是支付状态。请记住，我们在这个数据集中的欺诈是基于红旗列中的这两列。大于第 75 百分位的 1.5 倍且支付状态为未对账的异常值被视为潜在欺诈或洗钱交易。同样，低于 1.5 倍第 25 百分位的下限和支付状态为未对账的交易被视为潜在欺诈或洗钱支付交易。从图中我们可以看到，财政年度和月份对我们的预测模型没有任何重大影响，并且具有较低的特征权重。现在我们已经看到了最好的两个特性，让我们选择这些特性，以便我们的模型可以最终完成。这是在清单 [15-17](#PC19) 中完成的。

```
# Feature Selection
# Import functionality for cloning a model
from sklearn.base import clone
best_clf= clf_F
# Reduce the feature space
X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]
X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]

# Train on the "best" model found from grid search earlier
clf = (clone(best_clf)).fit(X_train_reduced, y_train)
best_predictions = best_clf.predict(X_test)
# Make new predictions
reduced_predictions = clf.predict(X_test_reduced)

# Report scores from the final model using both versions of data
print "Final Model trained on full data\n------"
print "Accuracy on testing data: {:.4f}".format(accuracy_score(y_test, best_predictions))
print "F-score on testing data: {:.4f}".format(fbeta_score(y_test, best_predictions, average="macro", beta = 0.5))
print "\nFinal Model trained on reduced data\n------"
print "Accuracy on testing data: {:.4f}".format(accuracy_score(y_test, reduced_predictions))
print "F-score on testing data: {:.4f}".format(fbeta_score(y_test, reduced_predictions, beta = 0.5, average="macro"))

Final Model trained on full data

------
Accuracy on testing data: 0.9561
F-score on testing data: 0.6537

Final Model trained on reduced data
------
Accuracy on testing data: 0.9548
F-score on testing data: 0.6523

Listing 15-17Feature Selection and Model Implementation

```

在最终模型和包括所有四个特征的完整数据之间，以及与简化数据相比，我们没有如此大的结果。因此，在我看来，我们可以保留完整的数据集，没有必要丢弃其他的特性，比如月份和财政年度。

### 最终参数

我们模型的最终参数在清单 [15-18](#PC20) 中给出。

```
# Print the final parameters

print "Parameters for the optimal model: {}".format(clf.get_params())
Parameters for the optimal model: {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'min_impurity_decrease': 0.0, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 10, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'criterion': 'gini', 'random_state': None, 'min_impurity_split': None, 'max_features': 'auto', 'max_depth': 2, 'class_weight': None}

Listing 15-18Parameters for the Optimized Model

```

至此，我们完成了机器学习在支付数据集中的实现。该实现非常类似于您在本书的医疗保健和零售部分看到的方法；但是，我们在这里使用了支付交易数据集，这要求我们调整定义洗钱交易的一些方式。请记住，这是一种非常幼稚和简单的欺诈检测方法。在现实世界中，模型和数据变得更加复杂，例如增加国籍、ip 地址或位置等的复杂性。

## 尾注

1.  需要应用缩放: [`http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html`](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html)