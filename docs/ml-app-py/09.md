# 9.如何在零售业实现机器学习

零售业最有前途的三个领域是:

*   **零售消费者购买模式**

*   **客户管理**

*   **供应链管理**

在零售行业，最具潜力的项目是应用机器学习来理解零售消费者的购买模式。虽然在这个领域已经有了一些进步，比如推荐引擎的创建，但这只是冰山一角。消费者购买模式是一种人类行为，需要机器更好地理解，以便从数据中获得任何好处。购买者出于内在需求购买产品，对产品类型进行某种研究，评估各种购买选择，然后做出购买决定。虽然这看起来很简单，但是想想你最近不得不购买某样东西的时候，以及你是如何购买该产品的。事实上，你认识到需要购买该产品只是一个起点。你可以谷歌一下这个产品，或者问问你可能用过这个产品的朋友和家人的意见。如果你在搜索一部电影，那么你会在网上或许多移动应用程序中查看各种电影评论。一旦你做了调查，并确信你应该购买这种产品，你就可以看看你可以购买这种产品的各种选择，包括网上销售或实体店。你购买产品的时间也是一个重要的决策标准。例如，如果你没有时间去实体店，你会选择在网上购买产品。在对各种选择进行评估之后，你就可以做出购买决定了。为了理解这种消费者购买模式，机器需要学习人类如何做出购买决定。机器学习可以用来实现营销个性化。这包括了解客户的购买需求，然后为目标客户定制促销方案。机器学习的另一种应用方式是通过学习消费者过去的购买行为，然后向他们提供建议。像 Amazon.com 这样的电子商务商店已经成功地做到了这一点。使用机器学习算法将产品与消费者偏好相匹配，在确定品牌对正确消费者的正确定位方面可以发挥至关重要的作用。这是一个需要做更多研究和建立商业应用的领域。通过识别客户的行为模式，有两种销售策略可用于向客户提供优惠:追加销售和交叉销售。追加销售是指向客户提供比客户最初选择的产品更高档、更优质的产品。交叉销售是从顾客选择的品牌中给顾客一个现有品牌的替代品。这两种策略都可以包含在推荐引擎中，以便测试和创建良好的促销要约推荐系统。

在客户管理方面，组织正寻求实施一项战略，从获得新客户的阶段，到在首次购买时创造或促进品牌忠诚度，再到实施重复购买的战略。使用机器学习来了解客户新的和不断变化的购买需求，查看竞争产品及其供应，然后给出比竞争产品更好的促销优惠，可以通过对竞争产品实施在线情感分析和对导致购买行为的那些情感进行数据挖掘来完成。例如，在对竞争产品进行情感分析时，我们可能会发现竞争对手组织利用某个特定名人开展的促销活动促进了该公司的销售。现在为了取得成功，我们公司也必须做一些类似的事情来获得新的客户。机器学习的这种智能使用可以极大地帮助营销智能策略。对首次购买者的数据进行深入分析，可以在首次购买者如何以及为什么对特定公司做出购买决定方面产生很多好处。在这样的数据集上使用机器学习可以给你隐藏的购买模式，这种模式迄今为止还没有被探索过。忠诚客户数据集来自那些从特定组织重复购买的客户。我们可以通过数据挖掘来了解和挖掘关键的购买模式。在本章中，我们将看到一个实施数据挖掘的例子，在这个例子中，我们将使用无监督学习中的聚类来挖掘消费者购买模式的隐藏维度。

使用机器学习来提供有效的促销优惠是机器学习可以用于促进销售的另一个方面。如果一个组织基于消费者的背景创建了一个过去促销优惠和消费者行为的数据库，它可以帮助机器学习应用程序挖掘出那些构成成功促销优惠的模式。例如，上班族喜欢使用该公司的应用程序购买产品的优惠。

供应链管理中的机器学习很难，因为这个过程涉及到商品和服务的物理移动。在实现机器学习以改进所提供的参数时，我们需要大量的数据，如供应商计算最优库存的提前期等。数据源可能来自不同的系统，这是在供应链管理中实现机器学习的主要挑战，并且增加了这种复杂性的事实是，关于供应品的数据在具有非常不同的格式和过程的数据库系统中使用。例如，供应商数据可能来自采购管理系统，而运输数据可能来自运输管理系统。在某些情况下，数据可能会导致外部系统不与公司的供应链系统直接对话。如此多样的数据面临的另一个挑战是数据来源，数据的质量和准确性都成问题。为供应链管理实现机器学习的最大挑战是创建一个拥有最新数据并经过适当清理的单一存储库。如果你无法在机器学习中创建一个供应链管理应用程序，你将不得不设计一个机器学习解决方案，该解决方案具有中间件数据库管理功能以及在几分钟内集成各种数据源的能力。

## 在零售业中实现机器学习生命周期

现在，我将带您了解零售行业中机器学习的一个实现。在这里，我将使用无监督学习技术向您展示如何对零售数据集进行数据挖掘。本例中使用的数据集是虚构的，适合您理解如何使用。我们知道无监督学习技术，我们心中有一个解决方案架构；然而，还没有找到，因为我们不知道我们将在数据准备的步骤中构建什么模型，这在第 [3 章](03.html)中给出。机器学习生命周期。在数据准备(如清理和预处理数据)后的这个阶段，我们停留在探索性数据分析这一步，以便在零售数据集中找到任何模式。我想让你明白，在这个阶段，我们不是在寻找建立一个解决方案或模型，而是在询问在可用的数据集中是否有任何模式。我们正在挖掘数据，从中找出任何意义。这里的重点是机器找出数据集中的任何模式或维度，而不是像我们在监督学习技术中所做的那样，为机器重新定义任何预定义的学习。我将只向你展示如何用零售数据集建立一个无监督的模型；但是，在本练习结束时，您将获得可进一步用于开发监督学习模型的维度(如果有)。通常，我们使用无监督学习技术来挖掘购买模式，然后基于使用无监督学习发现的维度来创建预测模型。我将帮助你使用监督学习，并创建一个类似于我在第 [3](03.html) 章中所做的模型，使用的数据是通过应用无监督学习技术开发的。一旦您开始处理实时零售数据集，这将有助于您在实时生产环境中实施。该数据集是平面文件的形式，可从以下 URL 获得: [`http://www.PuneetMathur.me/Book009/`](http://www.PuneetMathur.me/Book009/) 。现在让我们开始在这个数据集上实现无监督学习。

### 信息

本练习中使用的所有代码都在 Anaconda 环境中测试了与 Python 2.7 的兼容性。它应该也能在 3.x 上工作；但是，还没有经过测试。

### 无监督学习

在清单 [9-1](#PC1) 中，我首先取消了一些关于从 matplotlib 导入库的基本警告，否则大量的警告会导致无法解释结果。在我们继续之前，让我解释一下代码的结构。我将这段代码分为两部分:第一部分是我们进行数据可视化和处理所需的所有功能，如 PCA 输出、Y 绘图仪输出和通道输出；代码的第二部分是无监督学习技术的实际实现。在第二部分中，我使用前面定义的函数的逐步求和，并使用力和特征来找出这个零售数据集中隐藏的维度模式。

```py
import warnings
warnings.filterwarnings("ignore", category = UserWarning, module = "matplotlib")
warnings.filterwarnings("ignore", category = UserWarning, module = "cross_validation")
# Importing libraries
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import pandas as pd
import numpy as np
import pandas.plotting

Listing 9-1Importing Necessary Libraries

```

现在让我来解释我在清单 [9-2](#PC2) 给出的代码的第一部分中定义的四个函数。

```py
def pcaOutput(good_data, pca):
      "'
      Visualizing the PCA results and calculating explained variance
      "'

      # I am doing Dimension indexing through pca components
      dims = dims = ['Dimension {}'.format(i) for i in range(1,len(pca.comps_)+1)]

      # Creating the PCA components pandas dataframe from the dimensions
      comps = pd.DataFrame(np.round(pca.comps_, 4), columns = good_data.keys())
      comps.index = dims

      # Calculating PCA explained variance for each component
      ratios = pca.explained_variance_ratio_.reshape(len(pca.comps_), 1)
      variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])
      variance_ratios.index = dims

      # Creating a bar plot visualization for better understanding
      fig, ax = plt.subplots(figsize = (14,8))

      # Plotting the feature weights as a function of the components
      comps.plot(ax = ax, kind = 'bar');
      ax.set_ylabel("Feature Weights")
      ax.set_xticklabels(dims, rotation=0)

      # Displaying the explained variance ratios
      for i, ev in enumerate(pca.explained_variance_ratio_):
            ax.text(i-0.40, ax.get_ylim()[1] + 0.05, "Explained Variance\n            %.4f"%(ev))

      # Returning back a concatenated DataFrame
      return pd.concat([variance_ratios, comps], axis = 1)

Listing 9-2Function to Output PCA Results

```

PCA 输出函数给出了在该函数中对零售状态运行的主成分分析的结果。我首先创建维度，然后通过运行 for 循环将所有 PCA 组件复制到其中。然后我在熊猫数据框架中从维度创建组件，之后我计算每个组件的 PCA 解释的方差。解释的方差将被用作选择那些彼此独立的特征的度量。完成后，我通过绘制作为组件函数的特征权重来创建一个柱状图可视化。此函数返回串联的 Pandas 数据框，其中包含解释过的方差比率和分量。

现在让我们看看清单 [9-3](#PC3) 中的下一个函数 clusterOutput。

```py
def clusterOutput(redData, preds, centers, pca_samples):
      "'
      Visualizes the PCA-reduced cluster data in two dimensions
      Adds points for cluster centers for-selected sample data
      "'

      preds = pd.DataFrame(preds, columns = ['Cluster'])
      plot_data = pd.concat([preds, redData], axis = 1)

      # I am Generating the cluster plot
      fig, ax = plt.subplots(figsize = (14,8))

      # Creating the Color map to distinguish between clusters
      cmap = cm.get_cmap('gist_rainbow')

      # Coloring the points based on assigned cluster
      for i, cluster in plot_data.groupby('Cluster'):
          cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \
                       color = cmap((i)*1.0/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);

      # Plotting the centers with indicators
      for i, c in enumerate(centers):
          ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \
                     alpha = 1, linewidth = 2, marker = 'o', s=200);
          ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);

      # Plotting transformed sample points
      ax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \
                 s = 150, linewidth = 4, color = 'black', marker = 'x');

      # Plot title
      ax.set_title("Cluster Learning on PCA-Reduced Data - Centroids with Numerical markingr\nTick marks denote Transformed Sample Data");

Listing 9-3Cluster Output Results

```

在 clusterOutput 函数中，我使用简化数据的输入、质心中心的预测和 PCA 样本来可视化给定样本的聚类中心并向其添加点。在第一组语句中，我首先用名为 plaster 的预测列创建了一个 Pandas 数据帧，然后创建了一个包含预测和简化数据的串联 Pandas 数据帧。我现在生成一个包含 14 乘 18 大小的支线剧情的集群剧情，并使用彩虹色图。然后，我将所有的点按聚类分组，并迭代数据。我还使用 for 循环来绘制中心，用黑色指示器十字标记来表示采样数据。这样，您将能够可视化样本所属的聚类图。

### 可视化和绘图

接下来，在代码中，我使用了绘图仪函数，它采用干净的数据来减少数据，并将 PCA 作为代码清单 [9-4](#PC4) 中的输入。

```py
def biPlotter(cleanData, redData, pca):
    "'
    Building a biplot for PCA of the reduced data and the projections of the original features.
    Variable cleanData: original data, before transformation.
    Creating a pandas dataframe with valid column names
    redData: the reduced data (the first two dimensions are plotted)
    pca: pca object that contains the components_ attribute

    This function returns: a matplotlib AxesSubplot object (for any additional customization)
    This function is inspired by the script by Teddy Roland on Biplot in Python:
    https://github.com/teddyroland/python-biplot
    "'

    fig, ax = plt.subplots(figsize = (14,8))
    # scatterplot of the reduced data
    ax.scatter(x=redData.loc[:, 'Dimension 1'], y=redData.loc[:, 'Dimension 2'],
        facecolors='b', edgecolors="b", s=70, alpha=0.5)

    feature_vectors = pca.components_.T

    # we use scaling factors to make the arrows easier to see
    arrow_size, text_pos = 7.0, 8.0,

    # projections of the original features
    for i, v in enumerate(feature_vectors):
        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1],
                  head_width=0.2, head_length=0.2, linewidth=2, color="red")
        ax.text(v[0]*text_pos, v[1]*text_pos, cleanData.columns[i], color="black",
                 ha='center', va="center", fontsize=18)

    ax.set_xlabel("Dimension 1", fontsize=14)
    ax.set_ylabel("Dimension 2", fontsize=14)
    ax.set_title("Principal Component plane with original feature projections.", fontsize=16);
    return ax

Listing 9-4Biplots for Sample Data

```

在清单 [9-4](#PC4) 中，我为简化的 PCA 数据构建了一个双标图，并在原始特征上进行投影。Y 绘图仪功能的数据通过三个参数发送:干净数据、读取数据和最后一个可佩戴的 PCA。定义此双绘图函数的目的是创建双绘图，然后将其返回到颜色。你会注意到我创建了一个尺寸为 14 乘 8 的图形作为支线图，并使用散点图打印维度 1 和维度 2，因为我们使用的是二维平面图。我使用了箭头大小和文本位置来允许从质心投射箭头。在下一个 for 循环中，我使用了原始特征的投影，遍历特征向量，并用红色填充箭头的大小、长度和线宽。下一个文本出现在文本位置，颜色为黑色。我在这个 biplot 中使用了 18 的字体大小。对于维度 1 和 2 的 x 坐标和 y 坐标，我使用了 14 的字体大小；但是，您可以根据自己的要求进行调整。标题使用 16 的字体大小，然后函数将图形返回给调用者。

在清单 [9-5](#PC5) 中，我们看到一个函数，它通过在二维空间中可视化 PCA 简化的聚类数据来给出通道输出结果。这里的通道只是数据点，显示所选的样本数据。

```py
def channelOutput(redData, outliers, pca_samples):
      "'
      Here we are Visualizing the PCA-reduced cluster data in two dimensions using the full dataset
      Data is labeled by "Channel" points added for selected sample data
      "'

      # Check that the dataset is loadable
      try:
          full_data = pd.read_csv("retail.csv")
      except:
          print "Dataset could not be loaded. Is the file missing?"
          return False

      # Create the Channel DataFrame
      chl = pd.DataFrame(full_data['Channel'], columns = ['Channel'])
      chl = chl.drop(chl.index[outliers]).reset_index(drop = True)
      labeled = pd.concat([redData, chl], axis = 1)

      # Generate the cluster plot
      fig, ax = plt.subplots(figsize = (14,8))

      # Color map
      cmap = cm.get_cmap('gist_rainbow')

      # Color the points based on assigned Channel
      labels = ['Segment 1/Segment 2/Segment3', 'Retail Customer']
      grouped = labeled.groupby('Channel')
      for i, chl in grouped:
          chl.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \
                     color = cmap((i-1)*1.0/2), label = labels[i-1], s=30);

      # Plot transformed sample points
      for i, sample in enumerate(pca_samples):
            ax.scatter(x = sample[0], y = sample[1], \
                 s = 200, linewidth = 3, color = 'black', marker = 'o', facecolors = 'none');
            ax.scatter(x = sample[0]+0.25, y = sample[1]+0.3, marker='$%d$'%(i), alpha = 1, s=125);

      # Set plot title
      ax.set_title("PCA-Reduced Data Labeled by 'Channel'\nTransformed Sample Data Circled");

Listing 9-5Channel Output Results or Sample Data Projection on the Graph

```

在通道输出功能的第一部分，我们尝试再次加载数据集，然后构建包含完整数据的通道数据帧。然后我们删除离群值列，因为不再需要它，然后我将减少的数据与通道数据帧连接起来。之后，像我们之前做的那样，用彩虹色地图创建一个 14 乘 8 大小的子图，然后我将这些标签分配给零售客户的三个部分。在 for 循环中，我使用每个通道数据遍历分组的通道数据，以便分别在维度 1 和维度 2 上绘制它们。在下一组 for 循环中，我通过 PCI 样本对其进行了枚举，以便创建一个黑色散点图，餐饮服务提供者函数获取每个样本的输入，然后创建输出。在最后一行，我定义了情节标题。现在我们已经理解了这些函数，让我向你解释我将如何使用它们来执行或实现无监督学习技术。

### 加载数据集

首先，我们将导入库，然后导入数据集。之后，我们将通过描述来看看数据。现在，因为我们正在进行探索性数据分析，我将从数据集中选择三个样本。

样本的选择已经完成，这样我们就可以探索数据，看看是否有任何模式出现。在选择了三个索引之后，我现在开始按照样本的名称创建这些样本的数据框架。我删除了其余的列，因为它们不是必需的，所以 samples 数据集只包含我将在 sample 数据集中用来计算平均值和中值偏差的样本数据。为了了解特定特征中是否存在任何巨大的异常值，这是必需的。在此之后，我继续创建一个决策树回归评分使用 R2 分数为每个功能。这一步允许我们过滤掉那些不独立的特征。然后，我使用一种自然算法来缩放样本数据，然后生成一个散布矩阵。在此之后，我们查看离群值，为此，有一个函数可以列举给定特性的所有离群值。在去除异常值后，我们留下了好的数据，然后用于主成分分析(PCA)。在拟合好的数据之后，我使用我们之前看到的 PCA 输出函数来获得 PCA 结果，然后我调用 biplotter 函数来创建一个 biplot。然后，我随后调用集群输出函数来显示集群的实现。

从清单 [9-6](#PC6) 中，在实现了代码之后，我们可以看到 retail.csv 文件已经被加载，并且有 440 个样本，每个样本有 30 个特性。

```py
# This is Necessary for a newer version of matplotlib
import warnings
warnings.filterwarnings("ignore", category = UserWarning, module = "matplotlib")
warnings.filterwarnings("ignore", category = UserWarning, module = "cross_validationb")
warnings.filterwarnings("ignore", category=DeprecationWarning)
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import pandas as pd
import numpy as np

# Load the retail customers data set
try:
    data = pd.read_csv("retail.csv")
    data.drop(['Region', 'Channel'], axis = 1, inplace = True)
    print "Retail customers data set has {} samples with {} features each.".format(*data.shape)
except:
    print "Data set could not be loaded. Is the dataset missing?"

Retail customers data set has 440 samples with 30 features each.

Listing 9-6Loading the Data Set

```

简单来说，有 440 行 30 列。现在让我们看看表 [9-1](#Tab1) 中的详细数据集。

表 9-1

描述数据集

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"> <col class="tcol7 align-left"></colgroup> 
|   | 

面包

 | 

牛奶

 | 

沐浴皂

 | 

冷冻食品

 | 

清洁剂

 |   |
| --- | --- | --- | --- | --- | --- | --- |
| **计数** | 440.000000 | 440.000000 | 440.000000 | 440.000000 | 440.000000 |   |
| **表示** | 12000.297727 | 5796.265909 | 7951.277273 | 3071.931818 | 2881.493182 |   |
| **标准** | 12647.328865 | 7380.377175 | 9503.162829 | 4854.673333 | 4767.854448 |   |
| **分钟** | 3.000000 | 55.000000 | 3.000000 | 25.000000 | 3.000000 |   |
| **25%** | 3127.750000 | 1533.000000 | 2153.000000 | 742.250000 | 256.750000 |   |
| **50%** | 8504.000000 | 3627.000000 | 4755.500000 | 1526.000000 | 816.500000 |   |
| **75%** | 16933.750000 | 7190.250000 | 10655.750000 | 3554.250000 | 3922.000000 |   |
| **最大** | 112151.000000 | 73498.000000 | 92780.000000 | 60869.000000 | 40827.000000 |   |
|   | **巧克力** | **洗发水** | **再吃** | **led 灯泡** | **包装水** | **...** |
| **计数** | 440.000000 | 440.000000 | 440.000000 | 440.000000 | 440.000000 | ... |
| **表示** | 1524.870455 | 1172.447727 | 639.102273 | 1838.736364 | 26792.865909 | ... |
| **标准** | 2820.105937 | 527.238879 | 369.311261 | 396.316449 | 9186.632655 | ... |
| **分钟** | 3.000000 | 192.000000 | 17.000000 | 1164.000000 | 10811.000000 | ... |
| **25%** | 408.250000 | 751.750000 | 329.750000 | 1495.500000 | 19412.500000 | ... |
| **50%** | 965.500000 | 1202.500000 | 647.500000 | 1808.000000 | 26530.000000 | ... |
| **75%** | 1820.250000 | 1625.500000 | 951.250000 | 2169.500000 | 34425.750000 | ... |
| **最大** | 47943.000000 | 2020.000000 | 1240.000000 | 2562.000000 | 43085.000000 | ... |
|   | **配件设备** | **鼻中隔雷** | **清洁剂。1** | **果汁** | **黄油** |   |
| **计数** | 440.000000 | 440.000000 | 440.000000 | 440.000000 | 440.000000 |   |
| **表示** | 9.875000 | 4457.306818 | 223.106818 | 1288.706818 | 18678.575000 |   |
| **标准** | 5.547501 | 979.522344 | 99.645453 | 754.998453 | 10419.614651 |   |
| **分钟** | 1.000000 | 2851.000000 | 52.000000 | 38.000000 | 202.000000 |   |
| **25%** | 5.000000 | 3522.250000 | 138.000000 | 651.000000 | 9690.250000 |   |
| **50%** | 10.000000 | 4486.000000 | 222.000000 | 1252.000000 | 19584.000000 |   |
| **75%** | 15.000000 | 5326.500000 | 304.250000 | 1931.500000 | 27397.750000 |   |
| **最大** | 19.000000 | 6125.000000 | 539.000000 | 2589.000000 | 35969.000000 |   |
|   | **果冻** | **尿布** | **烤面包机** | **电影** | **爆米花** |   |
| **计数** | 440.000000 | 440.000000 | 440.000000 | 440.000000 | 440.000000 |   |
| **表示** | 219.047727 | 31805.109091 | 120.011364 | 22.984091 | 2300.545455 |   |
| **标准** | 93.574304 | 38012.651316 | 126.492916 | 12.707583 | 1273.548161 |   |
| **分钟** | 59.000000 | 12.000000 | 0.000000 | 1.000000 | 58.000000 |   |
| **25%** | 137.750000 | 8612.000000 | 31.000000 | 12.000000 | 1207.500000 |   |
| **50%** | 219.000000 | 19022.000000 | 85.000000 | 23.000000 | 2316.500000 |   |
| **75%** | 299.000000 | 42623.000000 | 169.000000 | 34.000000 | 3419.000000 |   |
| **最大** | 381.000000 | 371120.000000 | 1122.000000 | 45.000000 | 4512.000000 |   |

这个零售客户的数据集有 440 个样本，每个样本有 30 个特征。

```py
8 rows × 30 columns

```

我们可以看到，代码输出给出了每个特性或列的平均标准偏差、最小值和最大值。第一列(面包)的平均值为 12，000.29，标准偏差为 12，647，最小值为 3，最大值为 112，151。这意味着，对于面包，该零售连锁店的顾客平均每年订购 12，000 块面包，他们订购了最少 3 块面包，最多 112，151 块面包。由 25%、50%和 75%表示的行代表面包数据的第 25 个百分点、第 50 个百分点和第 75 个百分点。你可以看看从牛奶到爆米花的其余栏目，了解这 30 种商品对这家零售连锁店意味着什么。该数据属于一个特定的零售连锁店一年的数据，它显示了每种商品以美元计算的销售额。这是一个全球数据集，并不构成个人购买交易。它在频道之间被划分。值 2 表示实体商店，值 1 表示渠道是在线零售商。同样，该数据有一个列 region，用于表示销售发生的地区。

现在我转到清单 [9-7](#PC8) 中的下一步，它显示了我选择的三个索引，以及 samples Pandas 数据帧的创建和 samples 数据帧的输出。

```py
#   Selecting three indices of my choice from the data set
indices = [225,182,400]

# Creating a DataFrame of the chosen samples
samples = pd.DataFrame(data.loc[indices], columns = data.keys()).reset_index(drop = True)
print "Chosen samples of Retail customers dataset:"
display(samples)
display(samples)
Chosen samples of Retail customers dataset:
   Bread  Milk  BathSoaps  FrozenFood  Detergents  Chocolates  Shampoo  \
0  12680  3243       4157         660         761         786     2008
1    694  8533      10518         443        6907         156      842
2   4446   906       1238        3576         153        1014     1933

   ReadytoEat  LEDBulbs  PackagedWater    ...     FitnessEquipments  \
0         600      1694          36590    ...                     3
1         230      2298          18582    ...                    17
2        1232      1603          26071    ...                     8

   NasalSpray  Detergents.1  Juices  Butter  Jelly  Diapers  Toasters  Movies  \
0        2948           294    1613   33305    349    16628       127      23
1        4236           138    1406   32028    322    42072         7      35
2        5868           270    1134    9964     90     4952        44      23

   Popcorns
0      2291
1      3490
2      2256

[3 rows x 30 columns]

Listing 9-7Selecting Samples

```

选择三个样本的原因是为了了解数据的情况。如果数据集较大，您可以选择比这更多的样本。但是如果加了五个以上的样本，情节就会变得复杂，难以理解。

### 可视化样本数据集

在图 [9-1](#Fig1) 中，我们看到了可视化样本数据集的代码。

![../images/464968_1_En_9_Chapter/464968_1_En_9_Fig1_HTML.jpg](../images/464968_1_En_9_Chapter/464968_1_En_9_Fig1_HTML.jpg)

图 9-1

可视化零售客户样本数据集

```py
pd.DataFrame(samples).transpose().plot(kind='bar', stacked=False, figsize=(8,8), title='Retail customers dataset')
Out[46]: <matplotlib.axes._subplots.AxesSubplot at 0xa1a59860>

```

我们可以看到，第三个样本对蛋糕和面条的排序较高，对牙膏的排序第三高。样本一在面条、黄油、LED 灯泡和婴儿麦片中含量最高。在记录了三个样本的各种收入后，我们可以继续检查三个样本的平均值和中值。这可以在图 [9-2](#Fig2) 中看到。

![../images/464968_1_En_9_Chapter/464968_1_En_9_Fig2_HTML.jpg](../images/464968_1_En_9_Chapter/464968_1_En_9_Fig2_HTML.jpg)

图 9-2

平均值和中值的样本比较

```py
# Calculating deviations from Mean and Median
#to check how much the 3 samples deviate from the Center.
delta_mean = samples - data.mean().round()
delta_median = samples - data.median().round()

#display(delta_mean, delta_median)
delta_mean.plot.bar(figsize=(10,4), title='Compared to MEAN', grid=True)
delta_median.plot.bar(figsize=(10,4), title='Compared to MEDIAN', grid=True);

```

在代码中，我们看到，在第一个样本中，与平均值相比，订购数量最多的商品与洗涤剂有关，这比一般零售店订购的数量要高。同样，黄油也是该样本中订购最多的。从平均值来看，鼻喷剂是该样本零售店订单中最低的一项。同样，尿布也是一种远离平均水平的东西，订购量远低于一般零售店。在第二个样本中，平均值最高的订单是黄油和尿布。最低的订单是面包、蛋糕和包装水。在第三个样本数据中，我们看到了蛋糕、面条和牙膏的最高订单，它们远离平均值。同样，纸尿裤和黄油也是如此，它们的订购量远远低于普通零售店的订购量。既然我们已经了解了比较的含义，那么我们也来看看与中位数的比较。平均值和中位数的趋势没有显著差异。然而，我们确实在中间值图表中发现了一些差异，在样本一中，面包的订购量高于中间值商店。在第二个数据集的情况下，大多数其他项目保持不变；然而，最大的区别是洗发水，它的订单高于商店的中位数。在第三个样本数据集的情况下，平均值和中值几乎相同。

### 特征工程和选择

现在我们进入下一步，查看每个特征的 R2 分数，以确定其重要性。为此，我们需要一个称为 SCORER()函数的小函数，它将使用决策树回归器来确定特定功能的 R2 分数。我们的要素存储在数据框中，我们将使用 for 循环和 scorer 函数遍历数据框的每一列，以计算其 R2 分数值。

在清单 [9-8](#PC11) 中，您可以看到在 dataframe 上运行这个 scorer 函数的输出。

```py
#Importing Libraries
from sklearn.cross_validation import train_test_split
from sklearn.tree import DecisionTreeRegressor

#Broke down the logic into a scoring function and a For loop
def scorer(feature):
    #   Make a copy of the DataFrame, using the 'drop' function to drop the given feature relevant_data = data.drop([feature], axis=1)

    #   Split the data into training and testing sets using the given feature as the target X_train, X_test, y_train, y_test = train_test_split(relevant_data, data[feature], test_size=0.25, random_state=45)

    #   Create a decision tree regressor and fit it to the training set
    regressor = DecisionTreeRegressor(random_state=45).fit(X_train, y_train)

    #   Report the score of the prediction using the testing set
    score = regressor.score(X_test, y_test)

    print("The R2 score for feature {:16} is {:+.5f}".format(feature, score))

for feature in data.columns.values:

    scorer(feature)
The R2 score for feature Bread            is +0.95214
The R2 score for feature Milk             is -0.65329
The R2 score for feature BathSoaps        is +0.93408
The R2 score for feature FrozenFood       is -1.44912
The R2 score for feature Detergents       is +0.73558
The R2 score for feature Chocolates       is -12.74963
The R2 score for feature Shampoo          is -1.06275
The R2 score for feature ReadytoEat       is -0.57883
The R2 score for feature LEDBulbs         is -0.75093
The R2 score for feature PackagedWater    is -1.34695
The R2 score for feature Vitamins         is -0.98001
The R2 score for feature Toothpastes      is -1.26507
The R2 score for feature PainRelievers    is -0.74436
The R2 score for feature FirstAid         is -1.02262
The R2 score for feature Noodles          is -0.73669
The R2 score for feature BabyCereals      is -1.49739
The R2 score for feature Cakes            is -1.12935
The R2 score for feature Sweets           is -1.23246
The R2 score for feature HardwareTools    is -1.44181
The R2 score for feature CannedFoods      is -1.44300
The R2 score for feature FitnessEquipments is -1.51559
The R2 score for feature NasalSpray       is -0.53323
The R2 score for feature Detergents.1     is -1.34752
The R2 score for feature Juices           is -1.14858
The R2 score for feature Butter           is -1.16000
The R2 score for feature Jelly            is -1.04652
The R2 score for feature Diapers          is +0.98407
The R2 score for feature Toasters         is +0.95279
The R2 score for feature Movies           is +0.99948
The R2 score for feature Popcorns         is +0.99792
In [52]:

Listing 9-8Scoring the Features

```

清单中有两个分数。在清单 [9-8](#PC11) 中，我们可以看到面包、香皂、洗涤剂和维生素的 R2 分数为正。其他积极的特征包括尿布、烤面包机、电影和爆米花。负面得分是牛奶、冷冻食品、巧克力(最高为-12)、洗发水、包装水、牙膏、急救用品、面条、婴儿麦片、蛋糕、糖果、五金工具、罐头食品、健身器材、洗涤剂、果汁、黄油和果冻——仅举几例。我不建议我们将具有正分数的特征用于无监督学习的目的，因为这些特征显示了一个特征与另一个正特征之间的某种关系。此外，这些特征提供的信息似乎在很大程度上包含在其他特征中，因此删除它们是合适的，因为它们对于我们的无监督模型来说是多余的。对于负面评分特征，我建议保留这些特征，因为它们不显示与其他特征的关系，因此可以独立于它们来识别客户特定的行为。

在清单 [9-9](#PC12) 中，您可以看到删除冗余特性所需的代码。

```py
#Now selecting Only relevant features from the R2 score of each feature
#print(feature)
selected_features=[]
cntftr=0
for feature in data.columns.values:
    scorer(feature)
    if scorer(feature)<=0:
        print("This Feature is fit for selection ", feature)
        print("Counter Feature is: ",cntftr)
        #selected_features[cntftr]=feature
        selected_features.insert(cntftr,feature)
        cntftr=cntftr+1
# Create a data set with only the features with a negative R2 score from above for loop
display(selected_features)
data=data[selected_features]

Listing 9-9Code for Removing the Redundant Features

```

我在这里没有实现代码来删除这些多余的特性；然而，我肯定会向您展示如何删除冗余特性的代码。这是一个 for 循环，它遍历数据帧的列值，并根据 if 条件创建所有选定要素的列表。if 条件在这里很重要，因为我们使用的是 square 函数，我已经在清单 [9-1](#PC1) 中对其进行了概述，我们正在检查该特定特性的值是否小于或等于 0。如果该值为负，那么我们认为该特征适合我们所选的选择。否则，如果该特征的分数大于 0，if 条件将忽略该特征。顺便说一下，你可以根据 R2 分数实现特征选择。

### 可视化特征关系

接下来，我将使用 Pandas 绘图包为数据中的每对要素绘制散布矩阵。这可以在图 [9-3](#Fig3) 中详细看到。

![../images/464968_1_En_9_Chapter/464968_1_En_9_Fig3_HTML.jpg](../images/464968_1_En_9_Chapter/464968_1_En_9_Fig3_HTML.jpg)

图 9-3

所有特征的散点图

```py
pd.plotting.scatter_matrix(data, alpha = 0.3, figsize = (24,12), diagonal = 'kde');

```

在图 [9-3](#Fig3) 中，虽然特征的散布矩阵很拥挤，但在最左边的图和最上面的图中，我们确实看到了一些具有某些特征的图案，如面包、牛奶、沐浴皂、冷冻食品、洗涤剂、巧克力和洗发水。

```py
data.corr()

```

为了进一步查看各种特性之间是否存在关系，我在整个数据集上使用了 correlation(输出对于本书来说太长了，所以如果您想详细检查它，可以自己运行命令)。相关表显示，有一些项目或功能之间的相关性很高，如面包和烤面包机、尿布和沐浴露、洗涤剂和尿布、沐浴露和尿布、电影和爆米花。所有这些特征都与相关系数大于 0.9 的值正相关。这意味着人们在这些零售店经常一起购买这些商品。

### 样本转换

```py
display(log_samples)
      Bread      Milk  BathSoaps  FrozenFood  Detergents  Chocolates  \
0  9.447781  8.084254   8.332549     6.49224    6.634633    6.666957
1  6.542472  9.051696   9.260843     6.09357    8.840291    5.049856
2  8.399760  6.809039   7.121252     8.18200    5.030438    6.921658

    Shampoo  ReadytoEat  LEDBulbs  PackagedWater  ...  FitnessEquipments  \
0  7.604894    6.396930  7.434848      10.507530  ...           1.098612
1  6.735780    5.438079  7.739794       9.829949  ...           2.833213
2  7.566828    7.116394  7.379632      10.168579  ...           2.079442

   NasalSpray  Detergents.1    Juices     Butter     Jelly    Diapers  \
0    7.988882      5.683580  7.385851  10.413463  5.855072   9.718843
1    8.351375      4.927254  7.248504  10.374366  5.774552  10.647138
2    8.677269      5.598422  7.033506   9.206734  4.499810   8.507547

   Toasters    Movies  Popcorns
0  4.844187  3.135494  7.736744
1  1.945910  3.555348  8.157657
2  3.784190  3.135494  7.721349

[3 rows x 30 columns]

Listing 9-10Converting Data to Natural Log

```

在清单 [9-10](#PC15) 中，我使用自然算法缩放数据帧，输出显示在那里。我们这样做是为了进行特征缩放。如果数据不是正态分布，并且平均值和中值变化很大，那么我们可以对搜索数据应用非线性缩放。这里，我没有使用 box Cox 变换测试进行缩放；我使用一种更简单的自然算法。如果您想使用 Python 的 scipy 包，可以对这些数据应用 box Cox 测试。

在图 [9-2](#Fig2) 中，我们看到了应用自然对数变换后的样本数据的相关矩阵。我们可以看到，就特征之间的相关性而言，有一个显著的变化，我们看到了显著的负相关特征，例如，LED 灯泡与面包的负相关性为-0.87，健身设备与面包的负相关性为-0.9252。这表明我们的日志转换已经成功，我们已经准备好进入下一步识别异常值。尽管为了保持简洁，整个表的输出被截断，但在表 [9-2](#Tab2) 中，您可以运行图 [9-2](#Fig2) 中的命令来运行相关表的完整输出。

表 9-2

自然对数变换后的样本相关性

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"> <col class="tcol5 align-left"> <col class="tcol6 align-left"></colgroup> 
|   | 

面包

 | 

牛奶

 | 

烤箱

 | 

电影

 | 

爆米花

 |
| --- | --- | --- | --- | --- | --- |
| **面包** | one | -0.567927 | 0.999984 | -0.934429 | -0.922904 |
| **led 灯泡** | -0.874226 | 0.896117 | -0.871473 | 0.98982 | 0.993766 |
| **包装水** | 0.987271 | -0.429787 | 0.988152 | -0.865889 | -0.849917 |
| **维生素** | -0.473998 | -0.455546 | -0.478957 | 0.129318 | 0.098425 |
| **果汁** | 0.235358 | 0.666292 | 0.240837 | 0.12622 | 0.157002 |
| **黄油** | -0.130568 | 0.890186 | -0.124972 | 0.475107 | 0.502236 |
| **果冻** | -0.106274 | 0.878774 | -0.100663 | 0.453439 | 0.480931 |
| **尿布** | -0.570271 | 0.999996 | -0.565627 | 0.825439 | 0.842591 |
| **烤面包机** | 0.999984 | -0.563275 | one | -0.932405 | -0.920717 |
| **电影** | -0.934429 | 0.823827 | -0.932405 | one | 0.999517 |
| **爆米花** | -0.922904 | 0.841053 | -0.920717 | 0.999517 | one |

### 异常检测和过滤

清单 [9-11](#PC16) 显示了异常值检测的代码，然后将其从数据集中过滤出来。

```py
#Identifying Outliers
#Using Counters
from collections import Counter
outliers_counter = Counter()

# For each feature finding out the data points with extremely high or low values - Outliers
outliers_scores = None

for feature in log_data.keys():

    #   Calculate Q1 (25th percentile of the data) for the given feature
    Q1 = np.percentile(log_data[feature], 25)

    #   Calculate Q3 (75th percentile of the data) for the given feature
    Q3 = np.percentile(log_data[feature], 75)

    #   Use the interquartile range to calculate an outlier step (1.5 times the interquartile range) step = 1.5 * (Q3 - Q1)

    empty = np.zeros(len(log_data[feature]))
    aboveQ3 = log_data[feature].values - Q3 - step
    belowQ3 = log_data[feature].values - Q1 + step
    current_outliers_scores = np.array(np.maximum(empty, aboveQ3) - np.minimum(empty, belowQ3)).reshape([-1,1])
    outliers_scores = current_outliers_scores if outliers_scores is None else np.hstack([outliers_scores, current_outliers_scores])

    # Display the outliers
    print("Data points considered outliers for the feature '{}':".format(feature))
    current_outliers = log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))]
    display(current_outliers)

    outliers_counter.update(current_outliers.index.values)

Data points considered outliers for the feature 'Juices':
        Bread       Milk  BathSoaps  FrozenFood  Detergents  Chocolates  \
64   8.468003   7.112327   8.086410    8.222554    7.128496    7.043160
73   9.898425   8.581482   9.072571    9.003562    6.638568    6.473891
85   9.687630  10.740670  11.437986    6.933423   10.617099    7.987524
123  9.320987   9.284427   9.084097    7.693482    7.588830    4.962845
131  7.650169   6.378426   5.749393    5.846439    4.248495    5.736572
133  9.152075   6.948897   7.118826    6.077642    5.545177    5.981414
134  8.907612   7.540090   7.684324    6.579251    3.850148    6.285998
186  8.009363   6.255750   6.749931    8.151910    6.855409    6.588926
190  9.737197   8.740337   7.591357    8.900004    4.770685    4.158883
193  5.192957   8.156223   9.917982    6.865891    8.633731    6.501290
219  8.332068   5.905362   7.237059    7.743270    4.454347    4.867534
266  6.349139   9.186355  10.007036    7.705713    8.493310    7.848934
267  9.947169   7.108244   7.853993    8.287780    6.602588    8.196437
321  9.122055   7.548029   8.550048    7.906179    5.429346    7.014814
349  8.696343   9.591581   9.929204    7.603399    9.410666    6.098074
371  9.908724   7.542744   7.569928    9.007490    6.573680    6.672033
375  8.672657   7.279319   7.057898    6.391917    6.165418    4.248495
433  7.591862   8.076515   7.308543    7.340187    5.874931    7.278629
...

Listing 9-11Finding Outliers

```

在清单 [9-11](#PC16) 和 [9-12](#PC17) 中，我现在开始识别数据集中的异常值。我将使用 collections 包中的计数器库，然后使用 for 循环在每个特性的键之间传递，以确定数据是在四分位数范围内还是在第 75 个百分点到第 25 个百分点之间。Q1(第 25 百分位)和 Q3(第 75 百分位)的 1.5 倍被视为异常值。我们在第 3 章[中也使用了这种方法，所以读者会记得我们使用了同样的方法来寻找异常值。接下来，在确定特征之后，每个特征用它自己的离群值集合来标识，并且结果被显示出来。接下来，我打印出数据点，多个特征被认为是异常值。](03.html)

```py
min_outliers_count = 2
outliers = [x[0] for x in outliers_counter.items() if x[1] >= min_outliers_count]
print("Data points considered outlier for more than 1 feature: {}".format(outliers))
Data points considered outlier for more than 1 feature: [29, 35, 38, 50, 64, 65, 66, 75, 81, 83, 85, 86, 87, 95, 96, 104, 109, 127, 128, 134, 135, 137, 145, 147, 154, 162, 163, 171, 179, 180, 183, 184, 187, 190, 193, 196, 199, 210, 218, 228, 229, 231, 232, 250, 267, 276, 278, 299, 304, 305, 315, 321, 322, 327, 331, 338, 353, 355, 357, 412, 420, 431, 433]

Listing 9-12Index Values with Outliers in Multiple Features

```

我们在清单 [9-12](#PC17) 中看到，大约有 63 个数据点在不止一个特性中有异常值。在生产环境或商业设置中，您现在需要决定如何处理这些异常值。您可以做出三个决定:一个是不做任何事情，理由是我们将丢失总共 440 行中的 63 行，因此保留它们；第二，你可以删除它们以提高你的模型的准确性；或者最后，您可以用平均值或中值替换这些异常值。在我的例子中，我将把它们从清单 [9-13](#PC18) 给出的代码的数据集中删除。

```py
good_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)

Listing 9-13Getting Good Data by Dropping the Outliers

```

### 主成分分析

我现在将应用 PCA，这是一种用于减少特征维数的技术。清单 [9-14](#PC19) 给出了应用 PCA 并查看其尺寸值的输出代码。

```py
#   Apply PCA by fitting the good data with the same number of dimensions as features from sklearn.decomposition import PCA
pca = PCA(n_components=len(good_data.columns)).fit(good_data)

#   Transform log_samples using the PCA fit above
pca_samples =pca.transform(log_samples)

# Generate PCA results plot
pcaOutput = pcaOutput(good_data, pca)
# show results
display(pcaOutput)
# Cumulative explained variance should add to 1
display(pcaOutput['Explained Variance'].cumsum())

Dimension 1     0.2925
Dimension 2     0.4495
Dimension 3     0.5200
Dimension 4     0.5760
Dimension 5     0.6255
Dimension 6     0.6706
Dimension 7     0.7132
Dimension 8     0.7528
Dimension 9     0.7900
Dimension 10    0.8237
Dimension 11    0.8566
Dimension 12    0.8853
Dimension 13    0.9108
Dimension 14    0.9266
Dimension 15    0.9416
Dimension 16    0.9546
Dimension 17    0.9662
Dimension 18    0.9739
Dimension 19    0.9811
Dimension 20    0.9867
Dimension 21    0.9897
Dimension 22    0.9924
Dimension 23    0.9947
Dimension 24    0.9969
Dimension 25    0.9980
Dimension 26    0.9989
Dimension 27    0.9998
Dimension 28    0.9998
Dimension 29    0.9998
Dimension 30    0.9998
Name: Explained Variance, dtype: float64

Listing 9-14Applying Explained Variance on PCA

```

现在，我将通过拟合具有相同维数的良好数据作为特征来应用 PCA，这里我将使用 PCA 输出函数。我从分解库包 sklearn 中导入 PCA，最终结果是 30 个维度中的每一个都有一个解释的方差。如图 [9-4](#Fig4) 所示。我们可以清楚地看到，解释方差随着维度的增加而增加，维度 1 解释方差最低，维度 30 解释方差最高。

### 聚类和双标图可视化实现

图 [9-4](#Fig4) 提供了实现双绘图的代码。

![../images/464968_1_En_9_Chapter/464968_1_En_9_Fig4_HTML.jpg](../images/464968_1_En_9_Chapter/464968_1_En_9_Fig4_HTML.jpg)

图 9-4

尺寸和特征的双标图

在图 [9-4](#Fig4) 中，我们看到了尺寸和特征的双标图。双标图告诉我们，烤面包机和面包之间有一种关系，可能会经常被这个零售连锁店的顾客一起购买。类似地，面包和牛奶之间也可能有关系，它们一起出现在第 2 维附近。

在代码的最后一部分，我现在在 PCA 上实现聚类，这是一个简化的数据，它用刻度线显示我们选择的样本的输出。我使用 GMM 聚类和轮廓评分库从一个标量矩阵。在代码清单 [9-15](#PC20) 中，我们看到了集群的实现。

```py
#Import GMM and silhouette score library
from sklearn.mixture import GMM
from sklearn.metrics import silhouette_score

#Divided the logic into Clustering and Scoring functions
#Then Iterated through the clusters in the identified range

#This function creates Cluster using GMM
def clusterCreator(data, n_clusters):
    clusterer = GMM(n_components=n_clusters, covariance_type="full", random_state=45).fit(data)
    preds = clusterer.predict(data)
    centers = clusterer.means_
    sample_preds = clusterer.predict(pca_samples)
    return preds, centers, sample_preds

#Scoring after creating Clusters
def scorer(data, n_clusters):
    preds, _, _  = clusterCreator(data, n_clusters)
    score = silhouette_score(data, preds)
    return score

Listing 9-15Implementing Clustering

```

这里我使用了集群创建函数，它初始化 GMM 或者高斯混合模型来创建集群。聚类项目基于来自零售领域的数据集，并使用 PCA 样本在学校函数中进行样本预测。我在通过调用分类创建函数并返回数据和预测的轮廓分数来创建分类后进行评分。在 for 循环中，我在 2 到 10 维的范围内运行集群，并通过创建集群输出对它们进行评分。在这之后，我现在用减少的数据拟合高斯混合模型，然后进行预测并找出每个维度的中心。有了数据后，我将简化的数据传递给聚类输出函数，以生成预测中心和 PCA 样本，这是我们之前看到的，目的是打印聚类维度数据。现在我们来看看基于图 [9-5](#Fig5) 中“渠道”数据的聚类结果。

![../images/464968_1_En_9_Chapter/464968_1_En_9_Fig5_HTML.jpg](../images/464968_1_En_9_Chapter/464968_1_En_9_Fig5_HTML.jpg)

图 9-5

打印群维度

```py
#Iterate in the clusters and Print silhouette score
for n_clusters in range(2,10):
    score = scorer(reduced_data, n_clusters)
    print "For n_clusters = {}. The average silhouette_score is : {}".format(n_clusters, score)

n_clusters=0

# Resetting as due to loop run before we need to reset values again with 2 cluster components
clusterer = GMM(n_components=2).fit(reduced_data)
predictions = clusterer.predict(reduced_data)
centers = clusterer.means_
sample_preds = clusterer.predict(pca_samples)
# Display the results of the clustering from implementation
clusterOutput(reduced_data, predictions, centers, pca_samples)

```

现在我们需要开发通道输出，通过调用 channel output 函数清楚地了解数据是如何转换的。在代码清单 [9-6](#PC6) 中，我们可以看到基于“通道”数据的聚类结果。

![../images/464968_1_En_9_Chapter/464968_1_En_9_Fig6_HTML.jpg](../images/464968_1_En_9_Chapter/464968_1_En_9_Fig6_HTML.jpg)

图 9-6

基于“渠道”数据的聚类结果

```py
log_centers = pca.inverse_transform(centers)

#   Exponentiate the centers
true_centers = np.exp(log_centers)

# Display the true centers
segments = ['Segment {}'.format(i) for i in range(0,len(centers))]
true_centers = pd.DataFrame(np.round(true_centers), columns = data.keys())
true_centers.index = segments
display(true_centers)
#Compare the True Centers from the Central values Mean and Median
#display(true_centers - data.mean().round())
delta_mean=true_centers - data.mean().round()
#display(true_centers - data.median().round())
delta_median=true_centers - data.median().round()
delta_mean.plot.bar(figsize=(10,4), title='Compared to MEAN', grid=True)
delta_median.plot.bar(figsize=(10,4), title='Compared to MEDIAN', grid=True);

# Display the predictions
for i, pred in enumerate(sample_preds):
    print "Sample point", i, "predicted to be in Cluster", pred
samples

# Display the clustering results based on 'Channel' data
redData=reduced_data
channelOutput(redData, outliers, pca_samples)

```

我在这个例子中使用了 GMM，但是我也可以使用 K-Means 算法。与 K-Means 相比，GMM 有许多可以调整的参数。这是一种期望最大化算法。与 K-Means 相比，这是一种快速算法，可以用于小样本数据集。它使用马氏距离来计算聚类[ [2](#Par54) ]。选择 GMM 作为样本数据集并不大，GMM 会工作得很好[ [2](#Par54) ]。您是否注意到图 [9-6](#Fig6) 中的红点表示可能与另一个零售客户链相关的各个细分市场？你能想出他们之间有什么不同的方式吗，比如细分市场 1、细分市场 2 或细分市场 3 的客户是什么样的？它们会是基于地理位置的差异吗？这是你需要与客户管理部门一起坐下来研究的事情，看看这些客户群对企业意味着什么。至此，我结束了机器学习在零售业中的示例实现。在下一章中，我们将看到机器学习在零售运营中的两个案例研究实现。

## 尾注

1.  泰迪·罗兰[`https://github.com/teddyroland/python-biplot`](https://github.com/teddyroland/python-biplot)GitHub 上的脚本中的双标

2.  高斯混合， [`http://scikit-learn.org/stable/modules/mixture.html#mixture`](http://scikit-learn.org/stable/modules/mixture.html#mixture)