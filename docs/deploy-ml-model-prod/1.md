# 一、机器学习导论

在第一章中，我们将讨论机器学习和深度学习的一些基础知识。我们还将关注正在通过使用机器学习进行转型的不同商业垂直领域。最后，在进入下一组关于生产化的章节之前，我们将回顾在云平台(Databricks)上训练和建立一个相当简单的机器学习模型和深度学习模型的传统步骤。如果您知道这些概念，并且对自己在机器学习方面的专业水平感到满意，我鼓励您跳过接下来的两个部分，继续阅读最后一部分，在最后一部分，我提到了开发环境，并给出了本书附带的代码库和数据下载信息的链接，以便您能够适当地设置环境。本章分为三节。第一部分介绍机器学习的基础知识。第二部分深入研究深度学习的基础知识和广泛使用的深度神经网络的细节。前面的每一节后面都有在云平台上构建模型的代码。最后一部分是关于本书剩余章节的需求和环境设置。

## 历史

机器学习/深度学习并不新鲜；事实上，这可以追溯到 20 世纪 40 年代，当时人们第一次试图建造某种具有一定内置智能的东西。第二次世界大战期间，伟大的艾伦·图灵致力于建造这台能够解密德国密码的独特机器。这是机器智能时代的开端，几年内，许多国家的研究人员开始详细探索这一领域。在当时，ML/DL 被认为在改变世界方面非常强大，大量的资金被给予以使其具有生命力。几乎每个人都非常乐观。到 20 世纪 60 年代末，人们已经在研究机器视觉学习，开发具有机器智能的机器人。

虽然表面上看起来一切都很好，但仍有一些严重的挑战阻碍着这一领域的进展。研究人员发现在机器中创造智能极其困难。主要是由于几个原因。其中之一是当年计算机的处理能力不足以处理和加工大量数据，原因是相关数据本身的可获得性。尽管有政府的支持和充足的资金，但从 20 世纪 60 年代末到 90 年代初，人工智能研究遇到了障碍。这一段时间在社区成员中也被称为“艾温特斯”。

20 世纪 90 年代末，企业再次对人工智能产生了兴趣。日本政府公布了开发第五代计算机以推进机器学习的计划。人工智能爱好者认为，很快计算机将能够像人一样进行对话、翻译语言、解释图片和推理。1997 年，IBM 的“深蓝”成为第一台击败国际象棋世界冠军加里·卡斯帕罗夫的计算机。21 世纪初网络泡沫破裂时，一些人工智能资金枯竭了。然而，机器学习继续前进，这在很大程度上要归功于计算机硬件的改进。

### 过去十年

不可否认的是，在过去十年左右的时间里，世界在机器学习和人工智能应用方面取得了重大进展。事实上，如果要与任何其他技术相比较，ML/AI 在许多方面都是开创性的。亚马逊、谷歌和脸书等企业在人工智能的这些进步中蓬勃发展，也对此负有部分责任。像这样的组织的研发部门正在推动极限，并在将人工智能带给每个人方面取得令人难以置信的进展。不仅是像这样的大公司，还有成千上万家专门从事人工智能产品和服务的初创公司。在我写这一章的时候，这个数字只会继续增长。如前所述，在过去十年左右的时间里，各种业务对 ML 和 AI 的采用呈指数增长，这种行为的主要原因是多方面的。

*   数据的增长

*   提高计算效率

*   改进的最大似然算法

*   数据科学家的可用性

### 数据的增长

这一趋势的第一个最突出的原因是过去几十年中数据生成的大幅增长。数据总是存在的，但是理解这些大量数据背后的确切原因是必要的。在早期，数据是由特定组织的雇员或工人生成的，因为他们会将数据保存到系统中，但是只有有限的数据点保存几个变量。随后出现了革命性的互联网，使用互联网的几乎每个人都可以访问通用信息。有了互联网，用户可以控制输入和生成自己的数据。这是一个巨大的转变，因为世界上互联网用户的总数以爆炸式的速度增长，这些用户创建的数据量以更高的速度增长。所有这些数据——捕获用户详细信息的登录/注册表单、上传到各种社交平台的照片和视频以及其他在线活动——导致了术语“大数据”的产生。因此，ML 和 AI 研究人员在早期由于缺乏数据点而面临的挑战被完全消除，这被证明是 ML 和 AI 采用的主要促成因素。

最后，从数据的角度来看，我们已经达到了下一个水平，因为机器正在生成和积累数据。我们周围的每一个设备都在捕捉数据，比如汽车、建筑、手机、手表和飞机引擎。它们嵌入了多个监控传感器，每秒钟都在记录数据。这种数据在数量上甚至高于用户生成的数据，通常称为物联网(IoT)数据。

### 提高计算效率

我们必须理解这样一个事实，ML 和 AI 最终只是处理一大堆放在一起的数字，并从中找出意义。为了应用人工智能或人工智能，对强大的处理系统有着强烈的需求，我们已经见证了计算能力以极快的速度显著提高。仅仅观察一下我们在最近十年左右所看到的变化，移动设备的尺寸已经急剧减小，速度也有了很大程度的提高。这不仅仅是指微处理器芯片中使用 GPU 和 TPU 进行更快处理的物理变化，还包括 Spark 等数据处理框架的出现。在过去的十年中，处理能力的进步和使用 Spark 的内存计算的结合使得许多 ML 算法能够成功运行。

### 改进的最大似然算法

在过去的几年中，在新的和升级的算法的可用性方面已经取得了巨大的进步，这些算法不仅提高了预测的准确性，而且解决了传统 ML 面临的多重挑战。在第一阶段，这是一个基于规则的系统，人们必须首先定义所有的规则，然后在这些规则集内设计系统。由于环境过于动态，控制和更新规则的数量变得越来越困难。因此，传统的 ML 取代了基于规则的系统。这种方法面临的挑战是，数据科学家必须花费大量时间来手工设计构建模型的功能(称为*功能工程*),并且在预测准确性方面有一个上限，无论输入数据大小是否增加，这些模型都不能超过该上限。第三阶段是引入深度神经网络，其中网络将自己找出最重要的特征，并且也优于其他 ML 算法。此外，在过去的几年中，其他一些方法也引起了很多关注，如下所示:

*   元学习

*   迁移学习(纳米网络)

*   胶囊网络

*   深度强化学习

*   生成对抗网络

### 数据科学家的可用性

ML/AI 是一个专业领域，因为能够做到这一点所需的技能确实是多个学科的结合。为了能够建立和应用最大似然模型，你需要对数学和统计学的基础知识有一个很好的了解。与此同时，对机器学习算法和各种优化技术的深入理解对于采取正确的方法来使用 ML 和 AI 解决商业问题至关重要。下一个重要的技能是非常擅长编码，最后一个是成为特定领域(金融、零售、汽车、医疗保健等)的专家。)或拥有多个领域的深厚知识。数据科学家的角色在就业市场上非常令人兴奋，世界各地都有大量的数据科学家需求，尤其是在美国、英国和印度等国家。

## 机器学习

现在我们知道了一点机器学习的历史，我们可以复习机器学习的基础知识。我们可以把 ML 分解成四个部分，如图 [1-1](#Fig1) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig1_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig1_HTML.jpg)

图 1-1

机器学习类别(来源:en.proft.me)

*   监督机器学习

*   无监督机器学习

*   半监督机器学习

*   强化机器学习

### 监督机器学习

监督机器学习是机器学习的主要类别，它为企业带来了大量应用和价值。在这种类型的学习中，模型是根据我们已经有了正确标签或输出的数据来训练的。简而言之，我们试图映射输入数据和输出数据之间的关系，这样它也可以很好地概括看不见的数据，如图 [1-2](#Fig2) 所示。模型的训练通过将实际输出与预测输出进行比较，然后优化函数以减少实际输出和预测输出之间的总误差来进行。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig2_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig2_HTML.jpg)

图 1-2

一般化

这种类型的学习主要用于历史数据可用并且需要对未来数据进行预测的情况。监督学习的进一步分类基于用于预测的标签类型，如图 [1-3](#Fig3) 所示。如果输出变量的性质是数字，它属于回归，而如果是分类，它属于分类类别。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig3_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig3_HTML.jpg)

图 1-3

回归与分类

分类是指输出变量是离散值或本质上是分类的情况。分类有两种类型。

*   二元分类

*   多经典小说

当目标类为两类时，称为*二元*，当目标类多于两类时，称为多类，如图 [1-4](#Fig4) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig4_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig4_HTML.jpg)

图 1-4

二元对多元

监督学习的另一个特性是可以评估模型的性能。基于模型的类型(分类或回归)，可以应用评估度量，并且可以测量性能结果。这主要通过将训练数据分成两组(训练集和验证集)并在训练集上训练模型并在验证集上测试其性能来实现，因为我们已经知道验证集的正确标签/结果。

### 无监督学习

无监督学习是在商业应用中大量使用的另一类机器学习。就输出标签而言，它不同于监督学习。在无监督学习中，除了数据集不包含任何标签或结果列之外，我们在与监督学习类似的数据类别上建立模型。本质上，我们在没有任何正确答案的情况下对数据应用模型。在无监督学习中，机器试图在数据中找到隐藏的模式和有用的信号，这些数据可以在以后用于其他应用。主要目标是探测数据，并在数据集中找出隐藏的模式和相似性结构，如图 [1-5](#Fig5) 所示。用例之一是在客户数据中寻找模式，并将客户分组到不同的集群中。它还可以识别区分任何两个组的属性。从验证的角度来看，没有对无监督学习的准确性进行测量。基于用于建立模型的参数，由人 A 完成的聚类可能与人 B 完成的聚类完全不同。有不同类型的无监督学习。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig5_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig5_HTML.jpg)

图 1-5

使聚集

*   k 均值聚类

*   最近邻映射

### 半监督学习

顾名思义，半监督学习介于监督学习和无监督学习之间。事实上，它使用了这两种技术。这种类型的学习主要与我们处理混合类型的数据集的场景相关，这种数据集包含有标签和无标签的数据。有时它只是完全未标记的数据，但我们手动标记了其中的一部分。半监督学习的整个思想是使用这一小部分已标记的数据来训练模型，然后使用它来标记其他剩余部分的数据，这些数据可以用于其他目的。这也被称为*伪标记*，因为它使用监督模型做出的预测来标记未标记的数据。举一个简单的例子，假设我们有很多来自社交媒体的不同品牌的图片，其中大部分都没有标签。现在使用半监督学习，我们可以手动标记这些图像中的一些，然后在标记的图像上训练我们的模型。然后，我们使用模型预测来标记剩余的图像，以将未标记的数据完全转换为标记的数据。

半监督学习的下一步是在整个标记数据集上重新训练模型。它提供的优势是模型在更大的数据集上进行训练，这在早期是不存在的，现在更健壮，更擅长预测。另一个优势是半监督学习节省了大量人工标记数据的精力和时间。这样做的另一面是，很难获得伪标记的高性能，因为它使用一小部分标记数据来进行预测。然而，这仍然是比手动标记数据更好的选择，手动标记数据既昂贵又耗时。这就是半监督学习如何使用监督和非监督学习来生成标记数据。面临标签化培训过程相关成本挑战的企业通常会选择半监督学习。

### 强化学习

强化学习是第四种学习，在数据使用和预测方面没有什么不同。强化学习本身就是一个很大的研究领域，可以就此写一整本书。其他类型的学习和强化学习的主要区别在于，我们需要数据，主要是历史数据来训练模型，而强化学习是在一个奖励系统上工作的，如图 [1-6](#Fig6) 所示。它主要是基于代理为改变其状态而采取的某些行动的决策，同时试图最大化回报。让我们使用可视化将它分解为单个元素。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig6_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig6_HTML.jpg)

图 1-6

强化学习

*   *自主代理*:这是整个学习中负责采取行动的主角。如果是一个游戏，代理采取行动来完成或达到最终目标。

*   *Actions* :这是一组可能的步骤，代理可以采取这些步骤来推进任务。每个动作都会对代理的状态产生一些影响，并可能导致奖励或惩罚。例如，在网球比赛中，动作可能是发球、回球、向左或向右移动等。

*   *奖励*:这是强化学习取得进步的关键。奖励使代理能够根据它们是积极的奖励还是惩罚来采取行动。它是一种即时反馈机制，区别于传统的监督和非监督学习技术。

*   环境(Environment):这是代理可以参与的领域。环境决定了行动者采取的行动是奖励还是惩罚。

*   *状态*:代理在任何给定时间点所处的位置定义了代理的状态。为了向前推进或达到最终目标，代理人必须不断地朝积极的方向改变状态，以使回报最大化。

强化学习的独特之处在于，它有一个即时反馈机制，基于一个奖励系统来驱动代理的下一个行为。大多数使用强化学习的应用是在导航、机器人和游戏中。然而，它也可以用来建立推荐系统。

现在让我们回顾一下机器学习中的一些重要概念，因为在进入生产中的机器学习之前，很好地理解这些方面是至关重要的。

### 梯度下降

在一天结束时，机器学习模型就像它在预测中能够最小化的损失一样好。对于特定类别的问题，有不同类型的损失函数，通常在典型的分类或回归任务中，我们试图在训练和交叉验证期间最小化均方误差和对数损失。如果我们把损耗想成一条曲线，如图 [1-7](#Fig7) 所示，梯度下降帮助我们到达损耗值最小的点。我们基于模型中的初始权重或参数开始一个随机点，并向它开始减少的方向移动。这里值得记住的一点是，当梯度下降远离实际最小值时，它会采取很大的步长，而一旦它达到一个附近的值，步长就会变得很小，不会错过最小值。

为了向最小值点移动，它从对参数/系数(在神经网络的情况下是权重)取误差的导数开始，并试图找到该误差曲线的斜率等于零的点。梯度下降中的一个重要组成部分是学习率，因为它决定了下降到最低误差值的快慢。如果学习率参数被设置为较高的值，那么很可能它会跳过最低值，相反，如果学习率太小，它将需要很长时间才能收敛。因此，学习率成为整个梯度下降过程中的重要部分。

梯度下降的总体目标是基于训练数据达到反映最小误差的输入系数的相应组合。因此，在某种程度上，我们试图改变这些系数值，使损失最小。这是通过从旧系数值中减去学习速率和斜率(误差相对于系数的导数)的乘积的过程来实现的。系数值的这种改变持续发生，直到模型的系数/权重不再有变化，因为它表示梯度下降已经达到损失曲线中的最小值点。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig7_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig7_HTML.jpg)

图 1-7

梯度下降

另一种类型的梯度下降技术是随机梯度下降(SGD ),它涉及一种类似的方法，用于使误差向零方向最小化，但是使用多组数据点，而不是一次性考虑所有数据。它从输入数据中提取样本数据，并应用梯度下降来找到误差最低的点。

### 偏差与方差

偏差方差权衡是数据科学家最常关注的问题。高偏差是指机器学习模型没有从输入数据中学习足够多的信号，并导致最终预测性能不佳的情况。在这种情况下，模型过于简单，无法根据给定的输入来近似输出。另一方面，高方差指的是过度拟合(在训练数据上学习太多)。在高方差的情况下，由于模型过于复杂，对训练数据的模型学习会影响对未知或测试数据的泛化性能。你需要平衡偏差和方差，因为两者是相反的。换句话说，如果我们增加 bias，方差就会下降，反之亦然，如图 [1-8](#Fig8) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig8_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig8_HTML.jpg)

图 1-8

偏差与方差

### 交叉验证和超参数

对于大多数机器学习算法来说，有一组超参数可以相应地调整，以使模型具有最佳性能。超参数的著名类比是收音机/晶体管中的调谐旋钮，以匹配无线电台的精确频率，从而正确听到声音。同样，超参数为给定训练数据的模型性能提供了最佳可能组合。以下是机器学习模型(如随机森林)中超参数的几个示例:

*   树的数量

*   最大功能数量

*   树的最大深度

对于先前超参数的不同值，模型将学习给定输入数据的不同参数，并且预测性能将相应地变化。大多数库为模型的普通版本提供了这些参数的默认值，数据科学家有责任找出在特定情况下工作的最佳超参数。我们还必须小心不要过度拟合数据。现在，超参数和交叉验证齐头并进。交叉验证是一种技术，在这种技术中，我们以这样的方式分割训练数据，即训练集中的大部分记录用于训练模型，剩余的记录集(较小的记录集)用于测试模型的性能。根据交叉验证的类型(有重复或无重复)，训练数据被相应地拆分，如图 [1-9](#Fig9) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig9_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig9_HTML.jpg)

图 1-9

交互效度分析

### 性能指标

根据所用算法的性质，有不同的方法可以评估机器学习模型的性能。如前所述，大体上有两类模型:回归和分类。对于预测连续目标的模型，如 R-square，可以使用均方根误差(RMSE ),而对于后者，精度度量是标准度量。然而，在存在类别不平衡并且业务需要只关注正面或负面类别中的一个类别的情况下，可以使用诸如精度和召回之类的度量。

既然我们已经复习了机器学习中的基础知识和重要概念，那么是时候在云平台上建立一个简单的机器学习模型了，即 Databricks。

Databricks 是一种简单方便的方式，可以开始使用云基础设施来构建和运行机器学习模型(单线程和分布式)。我在我之前的几本书里已经对 Databricks 平台做了深入的介绍(*使用 PySpark 的机器学习*和*学习 PySpark* )。本章这一节的目的是让你了解如何通过注册任何一个主要的云服务提供商(Google、Amazon、Microsoft、Databricks)来在云上使用 ML。这些平台中的大多数允许用户简单地注册并使用 ML 服务(在某些情况下具有有限的能力)一段预定的时间或者直到用尽免费信用点的程度。Databricks 允许您使用其平台的社区版，该平台提供高达 6 GB 的集群大小。我们将使用 community edition 在假货币数据集上构建并理解决策树模型。该数据集包含纸币的四个属性，可用于检测纸币是真的还是假的。因为我们使用的是社区版，所以数据集的大小是有限制的，因此为了演示的目的，它被保持得相对较小。

Note

注册 Databricks 社区版来运行这段代码。

第一步是用默认设置启动一个新的集群，因为我们在这里不是构建一个复杂的模型。一旦集群启动并运行，我们只需将数据从本地系统上传到 Databricks。下一步是创建一个新的笔记本，并将其连接到我们之前创建的集群。下一步是导入所有必需的库，并确认数据已成功上传。

```py
[In]: import pandas as pd
[In]: import numpy as np
[In]: from sklearn.model_selection import train_test_split
[In]: from sklearn.tree import DecisionTreeClassifier
[In]: from sklearn.metrics import classification_report

```

以下命令行将显示从本地系统上传的表(数据集):

```py
[In]: display(dbutils.fs.ls("/FileStore/tables/"))

```

下一步是从表中创建 Spark 数据帧，然后将其转换为 pandas 数据帧来构建模型。

```py
[In:sparkDF=spark.read.csv('/FileStore/tables/currency_note_data.csv', header="true", inferSchema="true")

[In]: df=sparkDF.toPandas()

```

我们可以使用 pandas head 函数来查看数据帧的前五行。这确认了我们总共有五列，包括目标列(Class)。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figa_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figa_HTML.jpg)

```py
[In]: df.head(5)

[Out]:

```

如前所述，数据大小相对较小，我们可以看到它总共只包含 1，372 条记录，但是目标类似乎很平衡，因此我们没有处理不平衡的类。

```py
[In]: df.shape
[Out]: (1372, 5)

[In]: df.Class.value_counts()

[Out]:
0 762
1 610

```

我们还可以通过使用 info 函数来检查数据帧中是否有任何丢失的值。数据帧似乎不包含这样的缺失值。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figb_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figb_HTML.jpg)

```py
[In]: df.info()

[Out]:

```

下一步是使用训练测试拆分功能将数据拆分为训练集和测试集

```py
[In]: X = df.drop('Class', axis=1)
[In]: y = df['Class']

[In]:X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=30)

```

既然我们已经分离出了训练集，我们可以用默认的超参数构建一个决策树来简化事情。请记住，构建此模型的目的只是为了介绍在云平台上训练模型的过程。如果您想要训练一个更复杂的模型，请随意添加您自己的步骤，例如增强的特征工程、超参数调整、基线模型、可视化等等。我们将建立更复杂的模型，包括本书后面章节中的所有步骤。

```py
[In]: dec_tree=DecisionTreeClassifier().fit(X_train,y_train)

[In]: dec_tree.score(X_test,y_test)

[Out]: 0.9854227405247813

```

我们可以看到，决策树似乎在测试数据上做得非常好。除了准确性之外，我们还可以使用分类报告功能查看其他性能指标。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figc_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figc_HTML.jpg)

```py
[In]: y_preds = dec_tree.predict(X_test)
[In]: print(classification_report(y_test,y_preds))

[Out]:

```

## 深度学习

在本章的这一节，我们将回顾深度学习的基础及其底层操作原理。深度学习已经引人注目好几年了，在解决各种商业挑战方面正在突飞猛进。从图像字幕到语言翻译，再到无人驾驶汽车，深度学习已经成为更大范围内的重要组成部分。举个例子，谷歌的产品，如 Gmail、YouTube、Search、Maps 和 Assistance，都在后台以某种方式使用深度学习，因为它具有令人难以置信的能力，可以提供比其他一些传统机器学习算法好得多的结果。

但是深度学习到底是什么？嗯，在进入深度学习之前，我们必须了解什么是神经网络。深度学习实际上是神经网络的一种扩展。正如本章前面提到的，神经网络不是新的，但由于各种限制，它们没有起飞。这些限制不再存在，企业和研究团体现在能够利用神经网络的真正力量。

在监督学习设置中，有特定的输入和相应的输出。机器学习算法的目标是使用这些数据并近似输入和输出变量之间的关系。在某些情况下，这种关系是显而易见的，很容易捕捉，但在现实情况下，输入和输出变量之间的关系是复杂的，本质上是非线性的。举个例子，对于自动驾驶汽车，输入变量可能如下:

*   地带

*   离最近物体的距离

*   红绿灯

*   标志板

输出需要是转弯、开快或开慢、刹车等。正如你可能想到的，输入变量和输出变量之间的关系在本质上是相当复杂的。因此，传统的机器学习算法发现很难映射这种关系。在这种情况下，深度学习优于机器学习算法，因为它也能够学习那些非线性特征。

### 人脑神经元与人工神经元

如上所述，深度学习只是神经网络的扩展，也称为*深度神经网络*。在学习方面，神经网络与其他机器学习算法相比略有不同。神经网络是由人脑中的神经元松散地激发出来的。神经网络由人工神经元组成。虽然我并不自称是神经科学或大脑功能的专家，但让我试着给你一个“人脑如何运作”的高层次概述。你可能已经意识到了，人类的大脑是由数十亿个神经元以及它们之间数量惊人的连接组成的。每个神经元都与多个其他神经元相连，它们反复交换信息(信号)。我们在身体上或精神上所做的每一项活动都会激活我们大脑中的某一组神经元。现在，每一个神经元都由三个基本部分组成。

*   树突

*   细胞体

*   终端

正如我们在图 [1-10](#Fig10) 中看到的，树突负责接收来自其他神经元的信号。树突充当特定神经元的接收器，并将信息传递给细胞体，在细胞体中处理该特定信息。现在，根据信息的级别，它要么激活(启动),要么不触发。这种活动取决于神经元的特定阈值。如果输入信号值低于该阈值，则不会触发；否则，它会激活。最后，第三个组成部分是与其他神经元的树突连接的末端。终端负责将特定神经元的输出传递给其他相关的连接。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig10_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig10_HTML.jpg)

图 1-10

神经元

现在，我们来看看人工神经元，它是神经网络的基本构件。单个人工神经元主要由两部分组成；一个是求和，一个是激活，如图 [1-11](#Fig11) 所示。这也被称为*感知器*。求和是指将所有输入信号相加，激活是指根据阈值决定神经元是否触发。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig11_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig11_HTML.jpg)

图 1-11

人工神经元

假设我们有两个二进制输入(X1，X2)和它们各自连接的权重(W1，W2)。权重可以被认为类似于传统机器学习中输入变量的系数。这些权重表示特定输入要素在模型中的重要性。求和函数计算输入的总和。然后，激活功能使用该总累加值并给出一定的输出，如图 [1-12](#Fig12) 所示。激活是一种决策功能。基于所使用的激活功能的类型，它相应地给出一个输出。有不同类型的激活函数可用于神经网络层。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig12_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig12_HTML.jpg)

图 1-12

神经元计算

### 激活功能

激活函数在神经网络中起着至关重要的作用，因为输出基于所使用的激活函数的类型而变化。通常有四种广泛使用的主要激活功能。我们将在本节中简要介绍这些内容。

#### Sigmoid 激活函数

第一类激活函数是 sigmoid 函数。该激活功能确保无论输入如何，输出始终在 0 和 1 之间，如图 [1-13](#Fig13) 所示。这就是为什么它也被用于逻辑回归来预测事件的概率。【T2![$$ f(x)=\frac{1}{1+{e}^{-x}} $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equa.png)

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig13_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig13_HTML.jpg)

图 1-13

乙状结肠的

#### 双曲正切

另一个激活函数被称为*双曲正切激活函数*，或 tanh。该功能确保数值保持在-1 到 1 之间，与输出无关，如图 [1-14](#Fig14) 所示。tanh 激活函数的公式如下:

![$$ f(x)=\frac{e^{2x}-1}{e^{2x}+1\ } $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equb.png)

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig14_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig14_HTML.jpg)

图 1-14

双曲正切

#### 整流器线性单元

整流线性单元(relu)在过去几年里非常成功，已经成为激活功能的默认选择。它功能强大，因为它产生 0 到∞之间的值。如果输入为 0 或小于 0，则输出将始终为 0，但对于任何大于 0 的值，输出与输入相似，如图 [1-15](#Fig15) 所示。relu 的公式如下:

f（x）= 最大值（0，x）

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig15_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig15_HTML.jpg)

图 1-15

线性单元

### 神经元计算示例

既然我们对不同的激活函数有了基本的了解，那么让我们看一个例子来了解实际的输出是如何在神经元内部计算的。假设我们有两个输入，X1 和 X2，值分别为 0.2 和 0.7，权重分别为 0.05 和 0.03，如图 [1-16](#Fig16) 所示。求和功能计算输入信号的总和，如图 [1-17](#Fig17) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig16_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig16_HTML.jpg)

图 1-16

神经元输入

下面是求和:

![$$ sum=X1\ast W1+X2\ast W2 $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equc.png)

![$$ sum=0.2\ast 0.05+0.7\ast 0.03 $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equd.png)

![$$ sum=0.01+0.021 $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Eque.png)

![$$ sum=0.031 $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equf.png)

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig17_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig17_HTML.jpg)

图 1-17

总和

下一步是通过激活函数传递这个总和。让我们考虑使用一个 sigmoid 函数，不管输入是什么，它都返回 0 到 1 之间的值。sigmoid 函数将计算值，如下图所示和图[1-18](#Fig18):

![$$ f(x)=\frac{1}{\left(1+{e}^{-x}\right)} $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equg.png)

![$$ f(sum)=\frac{1}{\left(1+{e}^{- sum}\right)} $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equh.png)

![$$ f(0.031)=\frac{1}{\left(1+{e}^{-0.031}\right)} $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equi.png)

![$$ f(0.031)=0.5077 $$](img/493063_1_En_1_Chapter/493063_1_En_1_Chapter_TeX_Equj.png)

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig18_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig18_HTML.jpg)

图 1-18

激活

所以，这个单个神经元的输出等于 0.5077。

### 神经网络

当我们组合多个神经元时，我们最终会得到一个神经网络。最简单和最基本的神经网络可以仅使用输入和输出神经元来构建，如图 [1-19](#Fig19) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig19_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig19_HTML.jpg)

图 1-19

简单网络

像这样使用神经网络的挑战是，它只能学习线性关系，在输入和输出之间的关系是非线性的情况下，它不能很好地执行。正如我们已经看到的，在现实世界中，这种关系很难是简单的和线性的。因此，我们需要在输入层和输出层之间引入一个额外的神经元层，以增加它学习不同类型的非线性关系的能力。这个额外的神经元层被称为*隐藏层*，如图 [1-20](#Fig20) 所示。它负责将非线性引入网络的学习过程。神经网络也被称为*通用逼近器*，因为它们具有逼近输入和输出变量之间的任何关系的能力，无论这种关系有多复杂和非线性。很大程度上取决于网络中隐藏层的数量以及每个隐藏层中神经元的总数。如果有足够多的隐藏层，它可以非常好地映射这种关系。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig20_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig20_HTML.jpg)

图 1-20

带隐含层的神经网络

### 培训过程

神经网络就是各种连接(红线)和与这些连接相关的不同权重。神经网络的训练主要包括调整这些权重，以使模型能够以更高的准确度进行预测。为了理解神经网络是如何训练的，我们来分解一下网络训练的步骤。

**第一步:**取如图 [1-21](#Fig21) 所示的输入值，计算输出值传递给隐藏神经元。用于求和计算的第一次迭代的权重是随机生成的。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig21_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig21_HTML.jpg)

图 1-21

隐蔽层

传递的另一个分量是偏置神经元输入，如图 [1-22](#Fig22) 所示。这主要用于当你想有一些非零输出，甚至零输入值(你会学到更多的偏见在本章稍后)。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig22_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig22_HTML.jpg)

图 1-22

偏差分量

**第二步:**将网络预测输出与实际输出进行比较，如图 [1-23](#Fig23) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig23_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig23_HTML.jpg)

图 1-23

输出比较

**第三步:**误差反向传播到网络，如图 [1-24](#Fig24) 。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig24_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig24_HTML.jpg)

图 1-24

错误传播

**第四步:**根据输出重新调整权重，使误差最小，如图 [1-25](#Fig25) 。

**步骤 5:** 基于更新的权重计算新的输出值。

重复步骤 2，直到权重不可能再有变化。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig25_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig25_HTML.jpg)

图 1-25

重量调整

### 偏差在神经网络中的作用

人们的一个常见问题是，为什么我们要在神经网络中加入偏差？偏差的作用对于模型的正确学习是至关重要的，因为它直接关系到模型的性能。为了理解偏差在神经网络中的作用，我们需要回到线性回归，并揭示截距在回归线中的作用。我们知道一个事实，截距的值改变线的位置向上或向下，而斜率改变线的角度，如图 [1-26](#Fig26) 所示。如果斜率小于输入，变量对最终预测的影响较小，因为对于输入值的变化，小斜率对应的输出变化较小，而如果斜率值较大，则输出对输入值的最小变化更敏感，如图 [1-26](#Fig26) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig26_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig26_HTML.jpg)

图 1-26

回归斜率

因此，斜率值决定线条存在的角度，而截距值决定线条存在的位置(低或高)。

同样，如果我们不对网络使用偏差，简单的计算将是权重的组合输出和激活函数的输入。由于输入是固定的，只有权重可以改变，我们只能改变激活曲线的陡度/角度，这只是完成了一半的工作(尽管有些情况下它是可行的)，如图 [1-27](#Fig27) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig27_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig27_HTML.jpg)

图 1-27

没有偏见

理想情况下，我们还希望水平移动曲线(从左到右)，以获得激活函数的特定输出，从而进行适当的学习，如图 [1-28](#Fig28) 所示。这就是网络中偏差的确切目的，因为它允许我们将曲线从左向右移动，就像回归中的截距一样。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig28_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig28_HTML.jpg)

图 1-28

有偏见

现在我们已经很好地理解了深度学习是如何工作的，我们可以深入到广泛使用的特定神经网络中。深度学习模型有许多变体，但我们将只关注两种类型的深度学习模型。

*   卷积神经网络

*   递归神经网络

### 美国有线新闻网；卷积神经网络

当 CNN 首次用于基于图像的任务时，出现了重大突破。他们提供的准确性超过了以前用于图像识别的所有其他算法。从那时起，有多种 CNN 变体被用于解决特定的基于图像的任务，如人脸识别、计算机视觉、无人驾驶汽车等。细胞神经网络能够从图像中提取高级特征，这些特征通过称为*卷积*的过程捕获图像最重要的方面进行识别，如图 [1-29](#Fig29) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig29_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig29_HTML.jpg)

图 1-29

图像分类

卷积是一个容易理解的过程，因为我们在图像像素值上滚动滤波器(也称为*内核*)来提取卷积特征，如图 [1-30](#Fig30) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig30_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig30_HTML.jpg)

图 1-30

图像卷积

在图像上滚动过滤器表示我们用过滤器取图像的特定区域的值的点积。一旦我们有了复杂的特征地图，我们就使用池化来减少它。共有不同版本的池(最大池、最小池和平均池)。这样做是为了确保通过网络减少图像数据的空间大小。根据数据集和其他指标，我们可以在 CNN 的不同阶段建立池层。图 [1-31](#Fig31) 显示了最大池和图像池的示例。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig31_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig31_HTML.jpg)

图 1-31

联营

我们可以在一个网络中多次重复前面的步骤(卷积和池化)来学习图像的主要特征，最后传递到最后的全连通层来进行分类。

### RNN

一般的前馈神经网络和 CNN 不适合时间序列类型的数据集，因为这些网络没有它们自己的任何记忆。递归神经网络带来了独特的能力，能够在一段时间的训练中记住重要的东西。这使得它们非常适合自然语言翻译、语音识别和图像字幕等任务。这些网络具有在时间线上定义的状态，并在当前输入中使用先前状态的输出，如图 [1-32](#Fig32) 所示。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fig32_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fig32_HTML.jpg)

图 1-32

RNN

虽然 RNNs 已被证明在时序类应用中确实有效，但由于其架构，它在性能方面确实遇到了一些严重的限制。它与所谓的*消失梯度问题*进行斗争，该问题是由于网络试图使用时间线早期阶段的数据点时网络权重没有更新或更新很少而发生的。因此，用简单的话来说，它的记忆是有限的。为了解决这个问题，有几个其他的 RNNs 变种。

*   长短期记忆(LSTM)

*   梯度重复单位(GRU)

*   注意力网络(编码器-解码器模型)

现在，我们将使用一个小数据集，并建立一个深度学习模型来预测用户评论的情绪。我们将利用 TensorFlow 和 Keras 来建立这个模型。在 Databricks 中训练这个模型之前，我们需要做几个步骤。我们首先需要转到群集并单击“Libraries”。在 Libraries 选项卡上，我们需要选择 Pypi 选项并提到 Keras 来安装它。同样，一旦安装了 Keras，我们还需要提到 TensorFlow。

一旦我们上传了评论数据集，我们就可以像在前面的例子中那样创建一个熊猫数据框架。

```py
[In]: from tensorflow.keras.models import Sequential
[In]: from tensorflow.keras.layers import LSTM,Embedding
[In]: from tensorflow.keras.layers import Dense
[In]: from tensorflow.keras.preprocessing.text import Tokenizer
[In]: from tensorflow.keras.preprocessing.sequence import pad_sequences
[In]:sparkDF= spark.read.csv('/FileStore/tables/text_summary.csv', header="true", inferSchema="true")

[In]: df=sparkDF.toPandas()

[In]: df.columns
[Out]: Index(['Sentiment', 'Summary'], dtype="object")

```

如我们所见，数据帧中只有两列。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figd_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figd_HTML.jpg)

```py
[In]: df.head(10)

[Out]:

```

```py
[In]: df.Sentiment.value_counts()

[Out]:
1 1000
0 1000

```

我们还可以通过计算目标列的值来确认类平衡。数据似乎很平衡。在我们继续构建模型之前，因为我们正在处理文本数据，所以我们需要稍微清理一下，以确保在训练时不会抛出不必要的错误。因此，我们使用正则表达式编写了一个小的助手函数。

![img/493063_1_En_1_Chapter/493063_1_En_1_Fige_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Fige_HTML.jpg)

```py
[In]:
import re
def clean_reviews(text):
    text=re.sub("[^a-zA-Z]"," ",str(text))
    return re.sub("^\d+\s|\s\d+\s|\s\d+$", " ", text)

[In]: df['Summary']=df.Summary.apply(clean_reviews)

[In]: df.head(10)

[Out]:

```

下一步是分离输入和输出数据。由于数据已经很小，我们不打算把它分成训练集和测试集；相反，我们将根据所有数据训练模型。

```py
[In]: X=df.Summary
[In]: y=df.Sentiment

```

我们现在用 10，000 个 vocab 单词创建 tokenizer 对象，并为模型接触到的不属于训练的不可见单词提到了一个词汇外(oov)标记。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figf_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figf_HTML.jpg)

```py
[In]: tokenizer=Tokenizer(num_words=10000,oov_token='xxxxxxx')

[In]: tokenizer.fit_on_texts(X)

[In]: X_dict=tokenizer.word_index

[In]: len(X_dict)

[Out]: 2018

[In]: X_dict.items()

[Out]:

```

我们可以看到，在训练数据中有 2018 个唯一的单词。现在，我们根据使用 tokenizer 完成的标记映射，将每个评论转换成一个数字向量。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figg_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figg_HTML.jpg)

```py
[In]: X_seq=tokenizer.texts_to_sequences(X)

[In]: X_seq[:10]

[Out]:

```

尽管 text-to-sequence 函数将每个评论转换成了一个向量，但是有一个小问题，因为每个向量的长度根据原始评论的长度而不同。为了解决这个问题，我们利用

填充函数。它确保每个向量符合固定的长度(我们根据使用的填充类型(前置或后置)在末尾或开头添加一组 0)。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figh_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figh_HTML.jpg)

```py
[In]: X_padded_seq=pad_sequences(X_seq,padding='post',maxlen=100)

[In]: X_padded_seq[:3]

[Out]:

```

```py
[In]: X_padded_seq.shape

[Out]: (2000, 100)

```

正如我们所看到的，我们现在已经将每个评论转换为固定大小的向量。下一步，我们展平目标变量，并声明网络的一些全局参数。您可以选择自己的参数值。

```py
[In]: y = np.array(y)
[In]: y=y.flatten()

[In]: max_length = 100
[In]: vocab_size = 10000
[In]: embedding_dims = 50

```

现在，我们构建一个本质上是顺序的模型，并利用 relu 激活函数。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figi_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figi_HTML.jpg)

```py
[In]: model = tf.keras.Sequential([
tf.keras.layers.Embedding(input_length=100,input_dim=10000,output_dim=50),
tf.keras.layers.Flatten(),
tf.keras.layers.Dense(50, activation="relu"),
tf.keras.layers.Dense(1, activation="sigmoid")
])
[In]:model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
[In]: model.summary()

[Out]:

```

```py
[In]: num_epochs = 10
[In]: model.fit(X_padded_seq,y, epochs=num_epochs)

```

该模型似乎学习得很好，但也有可能过度拟合数据。我们将在本书后面的章节中讨论网络的过度拟合和其他设置。

![img/493063_1_En_1_Chapter/493063_1_En_1_Figk_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figk_HTML.jpg)

![img/493063_1_En_1_Chapter/493063_1_En_1_Figj_HTML.jpg](img/493063_1_En_1_Chapter/493063_1_En_1_Figj_HTML.jpg)

现在，我们已经对机器学习和深度学习基础知识有了一些了解，并对如何在云中构建模型有了一些了解，我们可以看看 ML/DL 在世界各地企业中的不同应用，以及随之而来的一些挑战。

## 工业应用和挑战

在本章的最后一节，我们将介绍人工智能和人工智能的一些实际应用。全球各地的企业都在大力投资 ML 和 AI，并建立标准程序来利用 ML 和 AI 的能力建立自己的竞争优势。目前有多个领域正在应用 ML 和 AI，并为企业提供巨大的价值。我们将看看人工智能和人工智能正在改变的几个主要领域。

### 零售

令人难以置信地使用 ML 和 AI 的商业垂直领域之一是零售业。由于零售业务产生大量的客户数据，这为应用 ML 和 AI 提供了一个完美的平台。零售行业一直面临着多种挑战，例如缺货情况、次优定价、有限的交叉销售或追加销售以及个性化不足。ML 和 AI 已经能够应对这些挑战，并在零售领域产生令人难以置信的影响。在过去的十年中，在零售领域已经构建了许多由 ML 和 AL 驱动的应用程序，并且数量还在继续增长。最突出的应用是推荐系统。在线零售业务在推荐系统上蓬勃发展，因为它们可以大大增加收入。此外，零售使用 ML 和 AI 功能进行库存优化，以控制库存水平并降低成本。动态定价是人工智能和人工智能被广泛用于获得最大回报的另一个领域。使用 ML 也可以进行客户细分，因为它不仅使用客户的人口统计信息，还使用交易数据，并在揭示客户群中的不同群体之前考虑多个其他变量。产品分类也是使用 ML 完成的，因为它节省了大量的手工劳动，并提高了产品标签的准确性。需求预测和库存优化使用 ML 和 AI 来解决，以节省成本。在过去的几年里，路线规划也由 ML 和 AI 处理，因为它使企业能够以更有效的方式履行订单。由于 ML 和 AI 在零售业中的应用，成本节约有所改善，企业能够做出明智的决策，整体客户满意度也有所提高。

### 卫生保健

另一个深受 ML 和 AI 影响的垂直业务是医疗保健。使用 ML 和 AI 基于图像数据的诊断正在医疗保健领域迅速得到采用。主要原因是人工智能和人工智能提供的准确性水平以及从过去几十年的数据中学习的能力。医疗保健领域的 X 射线、MRI 扫描和各种其他图像的 ML 和 AI 算法被大量用于检测任何异常。虚拟助手和聊天机器人也作为应用程序的一部分被部署，以帮助解释实验室报告。最后，保险验证也在医疗保健中使用 ML 模型来避免任何不一致。

### 金融

金融领域总是有大量的数据。与任何其他领域相比，金融一直是数据丰富的领域。因此，在过去的十年中，有许多基于人工智能和人工智能的应用程序正在被构建。最突出的是欺诈检测系统，它在后台使用异常检测算法。其他领域是投资组合管理和算法交易。ML 和 AI 有能力扫描 100 多年的过去数据，并学习隐藏的模式，以建议投资组合的最佳校准。复杂的人工智能系统正被用来做出极快的交易决策，以实现收益最大化。ML 和 AI 也用于风险缓解和贷款保险承保。此外，推荐系统正被各种机构用于追加销售和交叉销售各种金融产品。他们还使用推荐系统来预测客户群的流失，以便制定策略来留住可能会停止使用特定产品或服务的客户。ML 和 AI 在金融领域的另一个重要用途是，根据模型做出的预测，检查是否应该向各种申请人发放贷款。此外，基于 ML 模型预测，ML 被用于验证保险索赔是真实的还是欺诈的。

### 旅行和招待

就像零售一样，旅游和酒店领域正在基于人工智能和人工智能的应用程序上蓬勃发展。举几个例子，推荐系统、价格预测和虚拟助理都是基于人工智能和人工智能的应用，正在旅游和酒店业得到利用。从推荐最佳交易到替代旅行日期，推荐系统对于推动这一领域的客户行为至关重要。它还根据用户的偏好推荐新的旅行目的地，这些都是在后台使用 ML 高度定制的。人工智能还被用来根据各种因素预测未来的价格变动，从而及时向客户发送警报。如今，虚拟助理是每个旅游网站的一部分，因为客户不想等待获取相关信息。最重要的是，与这些虚拟助手的交互非常像人类，因为自然语言智能已经在很大程度上嵌入到这些聊天机器人中，以便理解简单的问题并以类似的方式回答。

### 媒体和营销

每个企业或多或少都依赖营销来获得更多的客户，而接触正确的客户一直是一个巨大的挑战。由于 ML 和 AI，这个问题现在得到了更好的处理，因为它可以在很大程度上预测客户行为。基于 ML 和 AI 的应用程序正被用来区分更有可能购买或订阅该产品或服务的潜在客户和临时候选人。他们还被用来提供一个绝对个性化的报价，以转换或保留客户。再次大量使用流失预测来识别可能停止使用任何特定产品或服务的消费者群体。超目标的高级客户细分正在使用 ML 和 AI 完成。最后，许多营销内容正在使用 ML 和 AI 人工生成，以发送出表现最好的内容。

### 制造业和汽车

制造领域也没能逃脱 ML 和 AI 的浪潮。最主要的用途是预测性维护，因为基于 ML 和 AI 的应用程序可以根据早期数据预测维护需求，从而有助于防止潜在的损坏。汽车公司正在使用远程信息处理数据来学习客户的驾驶模式，并更迅速地采取行动，以多种方式帮助他们。他们还使用网络数据来更好地了解他们的客户，以尝试个性化体验，实现在线旅程中的无缝导航。

### 社会化媒体

大多数人(尤其是年轻一代)在社交媒体上花费了大量时间，却没有意识到许多应用程序正在使用 ML 和 AI。脸书、YouTube、LinkedIn、Twitter 和其他类似的应用大量使用 ML 来提供这种体验。从照片自动标记建议到好友推荐，一切都由 ML 和 AI 驱动。它们还用于为 YouTube 等各种平台生成字幕和语言翻译。各种搜索引擎和语音助手在其中使用了大量的 ML 实现。

### 其他人

还有很多其他的应用都用到 ML 和 AI。例如，垃圾邮件过滤器使用 ML 而不是基于规则的系统。与传统的基于规则的系统相比，ML 方法提供的一个优势是前者可以根据新邮件自动更新和升级自己，以进行区分。另一个领域是石油和天然气行业，ML 和 AI 帮助分析地下矿物和寻找替代能源。ML 和 AI 也被用于交通运输，因为它们可以预测可能的交通状况并提前警告你。

### 挑战

到目前为止，我们已经讨论了 ML 和 AI 对这个世界的能力和影响。然而，为了实现人工智能和人工智能的真正潜力，仍然存在许多差距。首先，熟练人才的短缺是人工智能和人工智能发展的主要障碍。我们已经讨论过，人们需要综合多种技能才能在这个领域出类拔萃，这使得寻找这些资源变得更加困难。

> *“找一个数据科学家很难。找到了解数据科学家的人同样困难。”*
> 
> —克日什托夫扎瓦日基

下一个挑战是获得更高的计算能力。虽然我们有高性能的处理单元，如 GPU 和 TPU，但由于成本因素和训练大模型所需的时间，它仅限于一组人而不是每个人。因此，如果情况需要大量数据处理和模型训练，这仍然是一个挑战。当使用 ML 和 AI 时，安全性是最关键的方面，因为它们使用大量数据进行训练，以便给出更好的预测。然而，使用个人和敏感数据来构建模型会危及用户数据的安全性和保密性。归根结底，机器学习和人工智能并不是解决所有问题的灵丹妙药。与 ML 和 AI 相关的另一个挑战是可解释性部分，因为很难解释模型预测背后的基本原理。事实上，我们可以称之为黑盒，因为机器学习映射功能的解释有时会变得非常复杂，对其他利益相关者来说可能没有太多意义。在某些领域，人工智能和人工智能无法应用，因此许多计划和应用注定会失败。

## 要求

接下来的章节使用 Docker 来构建和部署容器，因此您的系统应该已经安装了 Docker 并正常工作。您还需要有管理员权限来安装一些依赖项。您还应该在系统上安装一个虚拟机器。要使用云服务部署应用程序，您应该拥有一个 Google Cloud 帐户。

## 结论

在这一章中，我们复习了机器学习和深度学习的基础知识。我们还看到了在 Databricks 上构建模型的过程。我们讨论了机器学习和深度学习的不同应用以及它们现有的挑战。