

# 二、查找部分文本

查找文本的各个部分涉及到将文本分解成称为记号的单个单元，并可选地对这些记号执行额外的处理。这种额外的处理可以包括词干化、词汇化、停用词移除、同义词扩展以及将文本转换为小写。

我们将展示在标准 Java 发行版中发现的几种标记化技术。这些都包括在内，因为有时这是你做这项工作所需要的。在这种情况下，可能不需要导入 NLP 库。然而，这些技术是有限的。接下来讨论 NLP APIs 支持的特定标记化器或标记化方法。这些例子将为如何使用标记器以及它们产生的输出类型提供参考。接下来简单比较了这两种方法之间的差异。

有许多专门的记号赋予者。例如，Apache Lucene 项目支持各种语言和专门文档的标记器。`WikipediaTokenizer`类是一个标记器，处理特定于维基百科的文档，而`ArabicAnalyzer`类处理阿拉伯文本。不可能在这里说明所有这些不同的方法。

我们还将研究如何训练某些记号赋予者来处理专门的文本。当遇到不同形式的文本时，这很有用。它通常可以消除编写新的专用记号赋予器的需要。

接下来，我们将说明如何使用这些记号赋予器来支持特定的操作，比如词干化、词汇化和停用词移除。词性也可以被认为是部分文本的特殊实例。不过这个话题在第五章、*检测词性*中有所考察。

因此，我们将在本章中讨论以下主题:

*   什么是标记化？
*   标记化器的使用
*   NLP 标记器 API
*   理解标准化



# 理解文本的各个部分

对文本的各个部分进行分类有多种方法。例如，我们可能关注字符级的问题，如标点符号，可能需要忽略或扩展缩写。在单词级别，我们可能需要执行不同的操作，例如:

*   使用词干化和/或词汇化识别词素
*   扩展缩写和首字母缩略词
*   隔离数字单位

我们不能总是用标点符号来拆分单词，因为标点符号有时会被认为是单词的一部分，比如单词*不能*。我们也可能关心将多个单词组合成有意义的短语。句子检测也是一个因素。我们不一定要将跨越句子边界的单词组合在一起。

在这一章中，我们主要关注记号化过程和一些专门的技术，比如词干。我们不会试图展示它们在其他 NLP 任务中是如何使用的。这些工作留待后面的章节讨论。



# 什么是标记化？

标记化是将文本分解成更简单单元的过程。对于大多数文本，我们关心的是孤立词。令牌根据一组分隔符进行拆分。这些分隔符通常是空白字符。Java 中的空白由`Character`类的`isWhitespace`方法定义。下表列出了这些字符。但是，有时可能需要使用一组不同的分隔符。例如，当空白分隔符模糊了文本分隔符(如段落边界)时，不同的分隔符会很有用，检测这些文本分隔符很重要:

| **字符** | **意为** |
| --- | --- |
| Unicode 空格字符 | (空格 _ 分隔符、行 _ 分隔符或段落 _ 分隔符) |
| `\t` | U+0009 水平制表 |
| `\n` | U+000A 馈线 |
| `\u000B` | U+000B 垂直制表 |
| `\f` | U+000C 换页 |
| `\r` | U+000D 回车 |
| `\u001C` | U+001C 文件分隔符 |
| `\u001D` | U+001D 组分隔符 |
| `\u001E` | U+001E 记录分隔符 |
| `\u001F` | U+001F 单元分离器 |

令牌化过程因大量因素而变得复杂，例如:

*   语言:不同的语言带来不同的挑战。空白是一种常用的分隔符，但是如果我们需要使用中文，它是不够的，因为中文不使用空白。
*   **文本格式**:文本通常以不同的格式存储或呈现。相对于 HTML 或其他标记技术，简单文本的处理方式将使标记化过程变得复杂。
*   **停用词**:常用词对于一些自然语言处理任务来说可能不重要，比如一般的搜索。这些常用词被称为停用词。当停用词对手头的 NLP 任务没有帮助时，它们有时会被移除。这些可以包括诸如 *a* 、*和*以及*她*之类的词。
*   **文本扩展**:对于首字母缩写词和缩写词，有时需要
    来扩展它们，以便后处理可以产生更高质量的结果。
    例如，如果搜索者对单词*机器*感兴趣，知道 IBM 代表国际商业机器可能是有用的。
*   **大小写**:单词的大小写(大写或小写)在某些情况下可能很重要。例如，单词的大小写可以帮助识别专有名词。识别文本的各个部分时，转换为相同的大小写有助于简化搜索。
*   **词干化和词汇化**:这些过程将改变单词以获得它们的*词根*。

删除停用词可以节省索引空间，加快索引过程。但是，有些引擎不删除停用字词，因为它们对某些查询可能很有用。例如，在执行精确匹配时，删除停用词会导致未命中。此外，NER 任务通常依赖于停用词的包含。认识到*罗密欧与朱丽叶*是一部戏剧取决于*和*这个词的包含。

There are many lists that define stopwords. Sometimes, what constitutes a stopword is dependent on the problem domain. A list of stopwords can be found at [`www.ranks.nl/stopwords`](http://www.ranks.nl/stopwords). It lists a few categories of English stopwords and stopwords for languages other than English. At [`www.textfixer.com/resources/common-english-words.txt`](http://www.textfixer.com/resources/common-english-words.txt), you will find a comma-separated formatted list of English stopwords.

改编自斯坦福的十大停用词([`library . Stanford . edu/blogs/digital-library-blog/2011/12/stop words-search works-be-or-not-be`](http://library.stanford.edu/blogs/digital-library-blog/2011/12/stopwords-searchworks-be-or-not-be))可以在下表中找到:

| **停止字** | **事件** |
| 这 | Seven thousand five hundred and seventy-eight |
| 关于 | Six thousand five hundred and eighty-two |
| 和 | Four thousand one hundred and six |
| 在 | Two thousand two hundred and ninety-eight |
| a | One thousand one hundred and thirty-seven |
| 到 | One thousand and thirty-three |
| 为 | Six hundred and ninety-five |
| 在 | Six hundred and eighty-five |
| 一；一个 | Two hundred and eighty-nine |
| 随着 | Two hundred and thirty-one |

我们将重点关注用于标记英语文本的技术。这通常涉及到使用空白或其他分隔符来返回一个令牌列表。

Parsing is closely related to tokenization. They are both concerned with identifying parts of text, but parsing is also concerned with identifying the parts of speech and their relationship to each other.

# 标记化器的使用

标记化的输出可以用于简单的任务，比如拼写检查和处理简单的搜索。它对于各种下游 NLP 任务也很有用，比如识别词性、句子检测和分类。接下来的大部分章节都会涉及到需要标记化的任务。

通常，令牌化过程只是更大任务序列中的一步。这些步骤涉及到管道的使用，我们将在*使用管道*一节中说明。这突出了为下游任务产生高质量结果的记号赋予器的需要。如果分词器做得不好，下游的任务将受到不利影响。

Java 中有许多不同的记号赋予器和记号化技术。有几个核心 Java 类被设计成支持标记化。其中一些现在已经过时了。还有许多 NLP APIs 被设计用来解决简单和复杂的令牌化问题。接下来的两节将研究这些方法。首先，我们将看到 Java 核心类必须提供什么，然后我们将演示一些 NLP API 标记化库。



# 简单的 Java 标记化器

有几个 Java 类支持简单的标记化；其中一些如下:

*   扫描仪
*   线
*   break 迭代器
*   StreamTokenizer
*   字符串标记器

尽管这些类提供了有限的支持，但了解它们的使用方法还是很有用的。对于某些任务，这些类就足够了。当核心 Java 类可以完成这项工作时，为什么要使用更难理解、效率更低的方法呢？我们将讨论这些类中的每一个，因为它们支持令牌化过程。

`StreamTokenizer`和`StringTokenizer`类不应用于新的开发。相反，?? 方法通常是更好的选择。它们被包含在这里，以防你碰到它们，想知道它们是否应该被使用。



# 使用 Scanner 类

`Scanner`类用于从文本源读取数据。这可能是标准输入，也可能来自文件。它提供了一种简单易用的技术来支持令牌化。

`Scanner`类使用空白作为默认分隔符。可以使用许多不同的构造函数来创建`Scanner`类的实例。
以下序列中的构造函数使用了一个简单的字符串。`next`方法从输入流中检索下一个令牌。令牌从字符串中分离出来，存储到字符串列表中，然后显示出来:

```java
Scanner scanner = new Scanner("Let's pause, and then "
    + " reflect."); 
List<String> list = new ArrayList<>(); 
while(scanner.hasNext()) { 
    String token = scanner.next(); 
    list.add(token); 
} 
for(String token : list) { 
    System.out.println(token); 
} 
```

执行时，我们得到以下输出:

```java
Let's
pause,
and
then
reflect.
```

这个简单的实现有几个缺点。如果我们需要我们的收缩被识别并可能被分割，正如第一个令牌所演示的，这个实现没有做到。此外，句子的最后一个单词被返回并附加了一个句点。



# 指定分隔符

如果我们对默认分隔符不满意，有几种方法可以用来改变它的行为。下表中总结了其中几种方法[`docs . Oracle . com/javase/7/docs/API/Java/util/scanner . html`](https://docs.oracle.com/javase/7/docs/api/java/util/Scanner.html)。提供这个列表是为了让您了解什么是可能的:

| **方法** | **效果** |
| `useLocale` | 使用区域设置来设置默认分隔符匹配 |
| `useDelimiter` | 基于字符串或模式设置分隔符 |
| `useRadix` | 指定处理数字时要使用的基数 |
| `skip` | 跳过输入匹配模式并忽略分隔符 |
| `findInLine` | 忽略分隔符，查找模式的下一个匹配项 |

这里，我们将演示`useDelimiter`方法的使用。如果我们在前面部分的示例中的`while`语句之前使用下面的语句，将使用的唯一分隔符将是空格、撇号和句点:

```java
scanner.useDelimiter("[ ,.]"); 
```

执行时，将显示以下内容。空行反映了逗号分隔符的使用。在本例中，它具有返回空字符串作为令牌的不良效果:

```java
Let's
pause

and
then
reflect  
```

此方法使用字符串中定义的模式。左括号和右括号用于创建一类字符。这是一个匹配这三个字符的正则表达式。关于 Java 模式的解释可以在 http://docs.oracle.com/javase/8/docs/api/找到。可以使用`reset`方法将分隔符列表重置为空白。



# 使用拆分方法

我们在第一章、*NLP 简介*中演示了`String`class’`split`方法。
为方便起见，此处重复:

```java
String text = "Mr. Smith went to 123 Washington avenue."; 
String tokens[] = text.split("\\s+"); 
for (String token : tokens) { 
    System.out.println(token); 
} 
```

输出如下所示:

```java
Mr.
Smith
went
to
123
Washington
avenue.
```

`split`方法也使用正则表达式。如果我们用上一节中使用的相同字符串(`"Let's pause, and then reflect."`)替换文本，我们将得到相同的输出。

`split`方法有一个重载版本，它使用一个整数来指定正则表达式模式应用于目标文本的次数。使用此参数可以在达到指定的匹配次数后停止操作。

`Pattern`类也有一个`split`方法。它将根据用于创建`Pattern`对象的模式来分割它的参数。



# 使用 BreakIterator 类

标记化的另一种方法是使用`BreakIterator`类。这个类支持不同文本单元的整数边界的位置。在这一节中，我们将说明如何使用它来查找单词。

该类有一个受保护的默认构造函数。我们将使用静态的`getWordInstance`方法来获取该类的一个实例。这个方法使用一个`Locale`对象重载了一个版本。该类拥有几种访问边界的方法，如下表所示。它有一个字段`DONE`，用于指示已经找到最后一个边界:

| **方法** | **用途** |
| `first` | 返回文本的第一个边界 |
| `next` | 返回当前边界之后的下一个边界 |
| `previous` | 返回当前边界之前的边界 |
| `setText` | 将字符串与`BreakIterator`实例相关联 |

为了演示这个类，我们声明了一个`BreakIterator`类的实例和一个与之一起使用的字符串:

```java
BreakIterator wordIterator = BreakIterator.getWordInstance(); 
String text = "Let's pause, and then reflect."; 
```

然后将文本指定给实例，并确定第一条边界:

```java
wordIterator.setText(text); 
int boundary = wordIterator.first();
```

接下来的循环将使用`begin`和`end`变量存储断词的开始和结束边界索引。边界值是整数。将显示每个边界对及其相关文本。

当找到最后一个边界时，循环终止:

```java
while (boundary != BreakIterator.DONE) { 
    int begin = boundary; 
    System.out.print(boundary + "-"); 
    boundary = wordIterator.next(); 
    int end = boundary; 
    if(end == BreakIterator.DONE) break; 
    System.out.println(boundary + " [" 
    + text.substring(begin, end) + "]"); 
} 
```

输出如下，括号用于清楚地描述文本:

```java
0-5 [Let's]
5-6 [ ]
6-11 [pause]
11-12 [,]
12-13 [ ]
13-16 [and]
16-17 [ ]
17-21 [then]
21-22 [ ]
22-29 [reflect]
29-30 [.]  
```

这种技术在识别基本令牌方面做得相当好。



# 使用 StreamTokenizer 类

在`java.io`包中找到的`StreamTokenizer`类被设计用来标记输入流。它是一个较老的类，不如在使用 StringTokenizer 类一节中讨论的`StringTokenizer`类灵活。类的实例通常基于文件创建，并将对文件中的文本进行标记。它可以使用字符串来构造。

该类使用一个`nextToken`方法来返回流中的下一个令牌。返回的令牌是一个整数。整数值反映了返回的令牌类型。根据令牌类型，可以用不同的方式处理令牌。

`StreamTokenizer`类字段如下表所示:

| **字段** | **数据类型** | **意为** |
| `nval` | `double` | 如果当前令牌是数字，则包含一个数字 |
| `sval` | `String` | 如果当前标记是单词标记，则包含该标记 |
| `TT_EOF` | `static int` | 流结尾的常数 |
| `TT_EOL` | `static int` | 行尾的常数 |
| `TT_NUMBER` | `static int` | 读取的令牌数 |
| `TT_WORD` | `static int` | 指示单词标记的常数 |
| `ttype` | `int` | 令牌读取的类型 |

在这个例子中，创建了一个标记器，然后声明了用于终止循环的变量`isEOF`。`nextToken`方法返回令牌类型。根据令牌类型，将显示数字令牌和字符串令牌:

```java
try { 
    StreamTokenizer tokenizer = new StreamTokenizer( 
          newStringReader("Let's pause, and then reflect.")); 
    boolean isEOF = false; 
    while (!isEOF) { 
        int token = tokenizer.nextToken(); 
        switch (token) { 
            case StreamTokenizer.TT_EOF: 
                isEOF = true; 
                break; 
            case StreamTokenizer.TT_EOL: 
                break; 
            case StreamTokenizer.TT_WORD: 
                System.out.println(tokenizer.sval); 
                break; 
            case StreamTokenizer.TT_NUMBER: 
                System.out.println(tokenizer.nval); 
                break; 
            default: 
                System.out.println((char) token); 
        } 
    } 
} catch (IOException ex) { 
    // Handle the exception 
} 
```

执行时，我们得到以下输出:

```java
Let
'  
```

这不是我们通常所期望的。问题是标记器使用撇号(单引号字符)和双引号来表示引用的文本。因为没有对应的匹配，所以它消耗字符串的剩余部分。

我们可以使用`ordinaryChar`方法来指定哪些字符应该被视为通用字符。单引号和逗号字符在这里被指定为普通字符:

```java
tokenizer.ordinaryChar('\''); 
tokenizer.ordinaryChar(','); 
```

当这些语句被添加到前面的代码中并被执行时，我们得到以下输出:

```java
Let
'
s
pause
,
and
then
reflect.  
```

撇号现在不是问题了。这两个字符被视为分隔符，并作为标记返回。还有一个`whitespaceChars`方法可以指定哪些字符将被视为空白。



# 使用 StringTokenizer 类

在`java.util`包中可以找到`StringTokenizer`类。它比`StreamTokenizer`类提供了更多的灵活性，并且被设计用来处理来自任何来源的字符串。类的构造函数接受要标记的字符串作为它的参数，并使用`nextToken`方法返回标记。如果输入流中存在更多的标记，`hasMoreTokens`方法将返回`true`。这按以下顺序进行了说明:

```java
StringTokenizerst = new StringTokenizer("Let's pause, and "
     + "then reflect."); 
while (st.hasMoreTokens()) { 
    System.out.println(st.nextToken()); 
}
```

执行时，我们得到以下输出:

```java
Let's
pause,
and
then
reflect.
```

构造函数被重载，允许指定分隔符以及分隔符是否应作为标记返回。



# Java 核心令牌化的性能考虑

当使用这些核心 Java 标记化方法时，有必要简要讨论一下它们的性能。由于影响代码执行的各种因素，衡量性能有时会很棘手。也就是说，在这里可以找到几种 Java 核心令牌化技术的性能的有趣比较:[`stack overflow . com/questions/5965767/performance-of-string tokenizer-class-vs-split-method-in-Java`](http://stackoverflow.com/questions/5965767/performance-of-stringtokenizer-class-vs-split-method-in-java)。对于他们正在解决的问题来说，`indexOf`方法是最快的。



# NLP 标记器 API

在本节中，我们将使用 OpenNLP、Stanford 和 LingPipe APIs 演示几种不同的标记化技术。尽管还有许多其他可用的 API，但我们只对这些 API 进行了演示。这些例子会让你知道哪些技术是可用的。

我们将使用一个名为`paragraph`的字符串来说明这些技术。该字符串包含一个新的换行符，该换行符可能出现在真实文本中的意外位置。其定义如下:

```java
private String paragraph = "Let's pause, \nand then +
     + "reflect.";
```



# 使用 OpenNLPTokenizer 类

OpenNLP 拥有一个由三个类实现的`Tokenizer`接口:`SimpleTokenizer`、`TokenizerME`和`WhitespaceTokenizer`。该接口支持两种方法:

*   `tokenize`:向其传递一个字符串以进行标记化，并以字符串形式返回一个由
    个标记组成的数组。
*   `tokenizePos`:传递一个字符串，返回一个`Span`
    对象的数组。`Span`类用于指定标记的开始和结束
    偏移量。

这些类中的每一个都将在下面的部分中进行演示。



# 使用 SimpleTokenizer 类

顾名思义，`SimpleTokenizer`类执行简单的文本标记化。`INSTANCE`字段用于实例化该类，如下面的代码序列所示。对`paragraph`变量执行`tokenize`方法，然后显示令牌:

```java
SimpleTokenizer simpleTokenizer = SimpleTokenizer.INSTANCE; 
String tokens[] = simpleTokenizer.tokenize(paragraph); 
for(String token : tokens) { 
    System.out.println(token); 
} 
```

执行时，我们得到以下输出:

```java
    Let
    '
    s
    pause
    ,
    and
    then
    reflect
    .  
```

使用这个标记器，标点符号作为单独的标记返回。



# 使用 WhitespaceTokenizer 类

顾名思义，这个类使用空格作为分隔符。在下面的代码序列中，创建了一个记号赋予器的实例，并使用`paragraph`作为输入对其执行`tokenize`方法。然后，for 语句显示标记:

```java
String tokens[] = 
 WhitespaceTokenizer.INSTANCE.tokenize(paragraph); 
for (String token : tokens) { 
    System.out.println(token); 
} 
```

输出如下所示:

```java
    Let's
    pause,
    and
    then
    reflect.  
```

虽然这并不能区分缩写和相似的文本单元，但对于某些应用程序来说，这是很有用的。该类还拥有一个返回令牌边界的`tokizePos`方法。



# 使用 TokenizerME 类

`TokenizerME`类使用用**最大熵** ( **MaxEnt** )和一个统计模型创建的模型来执行标记化。MaxEnt 模型用于确定数据之间的关系——在我们的例子中是文本。一些文本来源，如各种社交媒体，格式不规范，使用大量俚语和特殊符号，如表情符号。统计记号赋予器，例如 MaxEnt 模型，提高了记号化过程的质量。

A detailed discussion of this model is not possible here due to its complexity. A good starting point for an interested reader can be found at [`en.wikipedia.org/w/index.php?title=Multinomial_logistic_regression&redirect=no`](http://en.wikipedia.org/w/index.php?title=Multinomial_logistic_regression&redirect=no).

一个`TokenizerModel`类隐藏了模型，并用于实例化记号赋予器。该模型必须先前已经被训练过。在下面的例子中，使用在`en-token.bin`文件中找到的模型实例化了记号赋予器。这个模型已经被训练为处理普通的英语文本。

模型文件的位置由`getModelDir`方法返回，您需要实现这个方法。返回值取决于模型在系统中的存储位置。许多这些模型可以在 http://opennlp.sourceforge.net/models-1.5/的[找到。](http://opennlp.sourceforge.net/models-1.5/)

在创建了一个`FileInputStream`类的实例之后，输入流被用作`TokenizerModel`构造函数的参数。`tokenize`方法将生成一个字符串数组。接下来是显示令牌的代码:

```java
try { 
    InputStream modelInputStream = new FileInputStream( 
        new File(getModelDir(), "en-token.bin")); 
    TokenizerModel model = new 
         TokenizerModel(modelInputStream); 
    Tokenizer tokenizer = new TokenizerME(model); 
    String tokens[] = tokenizer.tokenize(paragraph); 
    for (String token : tokens) { 
        System.out.println(token); 
    } 
} catch (IOException ex) { 
    // Handle the exception 
} 
```

输出如下所示:

```java
Let
's
pause
,
and
then
reflect
.  
```



# 使用斯坦福记号赋予器

几个斯坦福 NLP API 类支持记号化；其中几个是
如下:

*   `PTBTokenizer`类
*   `DocumentPreprocessor`类
*   作为管道的`StanfordCoreNLP`类

每个例子都将使用前面定义的`paragraph`字符串。



# 使用 PTBTokenizer 类

这个分词器模仿了**佩恩树库 3** ( **PTB** )分词器([`www.cis.upenn.edu/~treebank/`](http://www.cis.upenn.edu/~treebank/))。它在选项和对 Unicode 的支持方面不同于 PTB。`PTBTokenizer`类支持几个旧的构造函数；但是，建议使用三参数构造函数。这个构造函数使用一个`Reader`对象、一个`LexedTokenFactory<T>`参数和一个字符串来指定使用几个选项中的哪一个。

`LexedTokenFactory`接口是由`CoreLabelTokenFactory`和`WordTokenFactory`类实现的。前一个类支持保留标记的开始和结束字符位置，而后一个类只是将标记作为不带任何位置信息的字符串返回。默认情况下使用`WordTokenFactory`类。我们将演示这两个类的用法。

在下面的例子中使用了`CoreLabelTokenFactory`类。使用`paragraph`创建一个`StringReader`实例。最后一个参数用于选项，在本例中是`null`。`Iterator`接口由`PTBTokenizer`类实现，允许我们使用`hasNext`和`next`方法来显示令牌:

```java
PTBTokenizer ptb = new PTBTokenizer( 
    new StringReader(paragraph), new 
 CoreLabelTokenFactory(),null); 
while (ptb.hasNext()) { 
    System.out.println(ptb.next()); 
} 
```

输出如下所示:

```java
Let
's
pause
,
and
then
reflect
.  
```

使用`WordTokenFactory`类可以获得相同的输出，如下所示:

```java
PTBTokenizerptb = new PTBTokenizer( 
    new StringReader(paragraph), new WordTokenFactory(), null);
```

`CoreLabelTokenFactory`类的功能是通过`PTBTokenizer`构造函数的 options 参数实现的。这些选项提供了控制记号赋予器行为的方法。选项包括如何处理引号、如何映射省略号，以及它应该处理英式英语拼写还是美式英语拼写。选项列表可以在[`NLP . Stanford . edu/NLP/javadoc/javanlp/edu/Stanford/NLP/process/ptbtokenizer . html`](http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/PTBTokenizer.html)找到。

在下面的代码序列中，`PTBTokenizer`对象是使用`CoreLabelTokenFactory`变量`ctf`和选项`"invertible=true"`创建的。这个选项允许我们获得并使用一个`CoreLabel`对象，它将给出每个令牌的开始和结束位置:

```java
CoreLabelTokenFactory ctf = new CoreLabelTokenFactory(); 
PTBTokenizer ptb = new PTBTokenizer( 
    new StringReader(paragraph),ctf,"invertible=true"); 
while (ptb.hasNext()) { 
    CoreLabel cl = (CoreLabel)ptb.next(); 
    System.out.println(cl.originalText() + " (" +  
        cl.beginPosition() + "-" + cl.endPosition() + ")"); 
} 
```

这个序列的输出如下。括号中的数字表示标记的开始和结束位置:

```java
Let (0-3)
's (3-5)
pause (6-11)
, (11-12)
and (14-17)
then (18-22)
reflect (23-30)
. (30-31)  
```



# 使用 document 预处理程序类

`DocumentPreprocessor`类标记来自输入流的输入。此外，它实现了`Iterable`接口，使得遍历标记化序列变得容易。记号赋予器支持简单文本和 XML 数据的记号化。

为了说明这个过程，我们将使用一个`StringReader`类的实例，它使用`paragraph`字符串，定义如下:

```java
Reader reader = new StringReader(paragraph);
```

然后实例化`DocumentPreprocessor`类的一个实例:

```java
DocumentPreprocessor documentPreprocessor = 
      new DocumentPreprocessor(reader); 
```

`DocumentPreprocessor`类实现了`Iterable<java.util.List<HasWord>>`接口。`HasWord`接口包含两个处理文字的方法:`setWord`和`word`。后一种方法将单词作为字符串返回。在下面的代码序列中，`DocumentPreprocessor`类将输入文本分割成句子，并存储为`List<HasWord>`。一个`Iterator`对象用于提取一个句子，然后一个 for-each 语句将显示标记:

```java
Iterator<List<HasWord>> it = documentPreprocessor.iterator(); 
while (it.hasNext()) { 
    List<HasWord> sentence = it.next(); 
    for (HasWord token : sentence) { 
        System.out.println(token); 
    } 
} 
```

执行时，我们得到以下输出:

```java
Let
's
pause
,
and
then
reflect
.  
```



# 使用管道

这里，我们将使用`StanfordCoreNLP`类，如第一章、*NLP 介绍*中所示。然而，我们使用一个更简单的注释器字符串来标记段落。如下面的代码所示，创建了一个`Properties`对象，并为其分配了`tokenize`和`ssplit`标注器。

`tokenize`注释器指定将发生标记化，而`ssplit`注释导致句子被拆分:

```java
Properties properties = new Properties(); 
properties.put("annotators", "tokenize, ssplit");
```

接下来创建`StanfordCoreNLP`类和`Annotation`类:

```java
StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); 
Annotation annotation = new Annotation(paragraph); 
```

执行`annotate`方法来标记文本，然后`prettyPrint`方法将显示标记:

```java
pipeline.annotate(annotation); 
pipeline.prettyPrint(annotation, System.out); 
```

显示各种统计信息，后面是输出中用位置信息标记的标记，如下所示:

```java
    Sentence #1 (8 tokens):
    Let's pause, 
    and then reflect.
    [Text=Let CharacterOffsetBegin=0 CharacterOffsetEnd=3] [Text='s CharacterOffsetBegin=3 CharacterOffsetEnd=5] [Text=pause CharacterOffsetBegin=6 CharacterOffsetEnd=11] [Text=, CharacterOffsetBegin=11 CharacterOffsetEnd=12] [Text=and CharacterOffsetBegin=14 CharacterOffsetEnd=17] [Text=then CharacterOffsetBegin=18 CharacterOffsetEnd=22] [Text=reflect CharacterOffsetBegin=23 CharacterOffsetEnd=30] [Text=. CharacterOffsetBegin=30 CharacterOffsetEnd=31]

```



# 使用 LingPipe 记号赋予器

LingPipe 支持许多记号赋予器。在这一节中，我们将说明`IndoEuropeanTokenizerFactory`类的用法。在后面的章节中，我们将展示 LingPipe 支持标记化的其他方式。它的`INSTANCE`字段提供了一个印欧语标记器的实例。`tokenizer`方法根据要处理的文本返回一个`Tokenizer`类的实例，如下所示:

```java
char text[] = paragraph.toCharArray(); 
TokenizerFactory tokenizerFactory = 
 IndoEuropeanTokenizerFactory.INSTANCE; 
Tokenizer tokenizer = tokenizerFactory.tokenizer(text, 0, 
 text.length); 
for (String token : tokenizer) { 
    System.out.println(token); 
}
```

输出如下所示:

```java
Let
'
s
pause
,
and
then
reflect
.  
```

这些标记化器支持普通文本的标记化。在下一节中，我们将演示如何训练分词器来处理独特的文本。



# 训练分词器查找部分文本

当我们遇到标准分词器不能很好处理的文本时，训练分词器是很有用的。我们可以创建一个用于执行标记化的标记化器模型，而不是编写一个定制的标记化器。

为了演示如何创建这样的模型，我们将从文件中读取训练数据，然后使用该数据训练模型。数据存储为由空格和`<SPLIT>`字段分隔的一系列单词。这个`<SPLIT>`字段用于提供关于如何识别令牌的进一步信息。它们可以帮助识别数字(如`23.6`)和标点符号(如逗号)之间的分隔符。我们将使用的训练数据存储在`training-data.train`文件中，如下所示:

```java
These fields are used to provide further information about how tokens should be identified<SPLIT>.  
They can help identify breaks between numbers<SPLIT>, such as 23.6<SPLIT>, punctuation characters such as commas<SPLIT>. 
```

我们使用的数据并不代表唯一的文本，但它确实说明了如何注释文本以及用于训练模型的过程。

我们将使用 OpenNLP `TokenizerME`类的重载`train`方法来创建一个模型。最后两个参数需要额外的解释。MaxEnt 用于确定文本元素之间的关系。

我们可以指定模型在包含到模型中之前必须处理的功能的数量。这些特征可以被认为是模型的方面。迭代指的是在确定模型参数时训练过程将迭代的次数。一些`TokenME`类参数如下:

| **参数** | **用途** |
| `String` | 所用语言的代码 |
| `ObjectStream<TokenSample>` | 包含训练数据的`ObjectStream`参数 |
| `boolean` | 如果`true`，则字母数字数据被忽略 |
| `int` | 指定处理特征的次数 |
| `int` | 用于训练
MaxEnt 模型的迭代次数 |

在下面的例子中，我们首先定义一个用于存储新模型的`BufferedOutputStream`对象。本例中使用的几个方法将生成异常，这些异常在`catch`块中处理:

```java
BufferedOutputStream modelOutputStream = null; 
try { 
    ... 
} catch (UnsupportedEncodingException ex) { 
    // Handle the exception 
} catch (IOException ex) { 
    // Handle the exception 
} 
```

使用`PlainTextByLineStream`类创建一个`ObjectStream`类的实例。这使用训练文件和字符编码方案作为其构造函数参数。这用于创建`TokenSample`对象的第二个`ObjectStream`实例。这些对象是包含令牌范围信息的文本:

```java
ObjectStream<String> lineStream = new PlainTextByLineStream( 
    new FileInputStream("training-data.train"), "UTF-8"); 
ObjectStream<TokenSample> sampleStream =  
    new TokenSampleStream(lineStream); 
```

现在可以使用`train`方法了，如下面的代码所示。英语被指定为语言。字母数字信息被忽略。特征值和迭代值分别设置为`5`和`100`:

```java
TokenizerModel model = TokenizerME.train( 
    "en", sampleStream, true, 5, 100);
```

下表详细给出了`train`方法的参数:

| **参数** | **意为** |
| 语言代码 | 指定所用自然语言的字符串 |
| 样品 | 示例文本 |
| 字母数字优化 | 如果`true`，则跳过字母数字 |
| 近路 | 处理特征的次数 |
| 迭代次数 | 为定型模型而执行的迭代次数 |

下面的代码序列将创建一个输出流，然后将模型写出到`mymodel.bin`文件。然后模型就可以使用了:

```java
BufferedOutputStream modelOutputStream = new 
 BufferedOutputStream( 
    new FileOutputStream(new File("mymodel.bin"))); 
model.serialize(modelOutputStream); 
```

这里将不讨论输出的细节。然而，它实际上记录了训练过程。序列的输出如下所示，但是最后一部分被缩短了，为了节省空间，大部分迭代步骤都被删除了:

```java
    Indexing events using cutoff of 5

    Dropped event F:[p=2, s=3.6,, p1=2, p1_num, p2=bok, p1f1=23, f1=3, f1_num, f2=., f2_eos, f12=3.]
    Dropped event F:[p=23, s=.6,, p1=3, p1_num, p2=2, p2_num, p21=23, p1f1=3., f1=., f1_eos, f2=6, f2_num, f12=.6]
    Dropped event F:[p=23., s=6,, p1=., p1_eos, p2=3, p2_num, p21=3., p1f1=.6, f1=6, f1_num, f2=,, f12=6,]
      Computing event counts...  done. 27 events
      Indexing...  done.
    Sorting and merging events... done. Reduced 23 events to 4.
    Done indexing.
    Incorporating indexed data for training...  
    done.
      Number of Event Tokens: 4
          Number of Outcomes: 2
        Number of Predicates: 4
    ...done.
    Computing model parameters ...
    Performing 100 iterations.
      1:  ...loglikelihood=-15.942385152878742  0.8695652173913043
      2:  ...loglikelihood=-9.223608340603953  0.8695652173913043
      3:  ...loglikelihood=-8.222154969329086  0.8695652173913043
      4:  ...loglikelihood=-7.885816898591612  0.8695652173913043
      5:  ...loglikelihood=-7.674336804488621  0.8695652173913043
      6:  ...loglikelihood=-7.494512270303332  0.8695652173913043
    Dropped event T:[p=23.6, s=,, p1=6, p1_num, p2=., p2_eos, p21=.6, p1f1=6,, f1=,, f2=bok]
      7:  ...loglikelihood=-7.327098298508153  0.8695652173913043
      8:  ...loglikelihood=-7.1676028756216965  0.8695652173913043
      9:  ...loglikelihood=-7.014728408489079  0.8695652173913043
    ...
    100:  ...loglikelihood=-2.3177060257465376  1.0

```

我们可以使用该模型，如下面的序列所示。这与我们在*使用 TokenizerME 类*一节中使用的技术相同。唯一的区别是这里使用的模型:

```java
try { 
    paragraph = "A demonstration of how to train a 
 tokenizer."; 
    InputStream modelIn = new FileInputStream(new File( 
        ".", "mymodel.bin")); 
    TokenizerModel model = new TokenizerModel(modelIn); 
    Tokenizer tokenizer = new TokenizerME(model); 
    String tokens[] = tokenizer.tokenize(paragraph); 
    for (String token : tokens) { 
        System.out.println(token); 
} catch (IOException ex) { 
    ex.printStackTrace(); 
} 
```

输出如下所示:

```java
A
demonstration
of
how
to
train
a
tokenizer
.
```



# 比较标记化器

下表显示了 NLP API 标记化器的简要比较。生成的令牌列在令牌化器的名称下。它们都基于同一个文本:“让我们暂停，然后反思。”请记住，输出是基于类的简单使用。可能存在示例中未包含的选项，这些选项会影响令牌的生成方式。目的是简单地展示基于示例代码和数据的预期输出类型:

| `SimpleTokenizer` | `WhitespaceTokenizer` | `TokenizerME` | `PTBTokenizer` | `DocumentPreprocessor` | `IndoEuropeanTokenizerFactory` |
| 让 | 让我们 | 让 | 让 | 让 | 让 |
| ' | 暂停， | s | s | s | ' |
| s | 和 | 中止 | 中止 | 中止 | s |
| 中止 | 然后 | , | , | , | 中止 |
| , | 反思。 | 和 | 和 | 和 | , |
| 和 |  | 然后 | 然后 | 然后 | 和 |
| 然后 |  | 显示 | 显示 | 显示 | 然后 |
| 显示 |  | 。 | 。 | 。 | 显示 |
| 。 |  |  |  |  | 。 |



# 理解标准化

规范化是将单词列表转换为更统一的序列的过程。这有助于为以后的处理准备文本。通过将单词转换成标准格式，其他操作就能够处理数据，而不必处理可能会影响该过程的问题。例如，将所有单词转换为小写将简化搜索过程。

规范化过程可以改进文本匹配。例如，术语*调制解调器路由器*有几种表达方式，如调制解调器和路由器、调制解调器&路由器、调制解调器/路由器和调制解调器-路由器。通过将这些词规范化为通用形式，可以更容易地向购物者提供正确的信息。

请理解，规范化过程也可能会影响 NLP 任务。当大小写很重要时，转换成小写字母会降低搜索的可靠性。

规范化操作可以包括以下内容:

*   将字符改为小写
*   扩展缩写
*   删除停用词
*   词干化和词汇化

我们将在这里研究这些技术，除了扩展缩写。这种技术类似于用于删除停用词的技术，只是缩写被替换为它们的扩展版本。



# 转换成小写

将文本转换为小写是一个简单的过程，可以改善搜索结果。我们既可以使用 Java 方法，比如`String`类的`toLowerCase`方法，也可以使用一些 NLP APIs 中的功能，比如 LingPipe 的`LowerCaseTokenizerFactory`类。这里演示了`toLowerCase`方法:

```java
String text = "A Sample string with acronyms, IBM, and UPPER " 
   + "and lowercase letters."; 
String result = text.toLowerCase(); 
System.out.println(result); 
```

输出如下所示:

```java
    a sample string with acronyms, ibm, and upper and lowercase letters.

```

LingPipe 的`LowerCaseTokenizerFactory`方法在*使用管道*规范化一节中进行了说明。



# 删除停用词

有几种方法可以删除停用词。一个简单的方法是创建一个类来保存和删除停用词。此外，几个 NLP APIs 提供了对停用词移除的支持。我们将创建一个名为`StopWords`的简单类来演示第一种方法。然后我们将使用 LingPipe 的`EnglishStopTokenizerFactory`类来演示第二种方法。



# 创建停用字词类

移除停用字词的过程包括检查令牌流，将它们与停用字词列表进行比较，然后从流中移除停用字词。为了说明这种方法，我们将创建一个支持基本操作的简单类，如下表所示:

| **构造器/方法** | **用途** |
| 默认构造函数 | 使用一组默认的停用词 |
| 单参数构造函数 | 使用存储在文件中的停用词 |
| `addStopWord` | 向内部列表添加新的停用字词 |
| `removeStopWords` | 接受一个单词数组，并返回一个删除了停用词的新数组 |

创建一个名为`StopWords`的类，声明两个实例变量，如下面的代码块所示。`defaultStopWords`变量是一个保存默认停用词表的数组。`HashSet`变量的`stopWords`列表用于保存用于处理的停用词:

```java
public class StopWords { 

    private String[] defaultStopWords = {"i", "a", "about", "an", 
       "are", "as", "at", "be", "by", "com", "for", "from", "how", 
       "in", "is", "it", "of", "on", "or", "that", "the", "this", 
       "to", "was", "what", "when", where", "who", "will", "with"}; 

    private static HashSet stopWords  = new HashSet(); 
    ... 
} 
```

接下来是该类的两个构造函数，它们填充了`HashSet`:

```java
public StopWords() { 
    stopWords.addAll(Arrays.asList(defaultStopWords)); 
} 

public StopWords(String fileName) { 
    try { 
        BufferedReader bufferedreader =  
                new BufferedReader(new FileReader(fileName)); 
        while (bufferedreader.ready()) { 
            stopWords.add(bufferedreader.readLine()); 
        } 
    } catch (IOException ex) { 
        ex.printStackTrace(); 
    } 
}
```

`addStopWord`便利方法允许添加额外的单词:

```java
public void addStopWord(String word) { 
    stopWords.add(word); 
}
```

`removeStopWords`方法用于删除停用词。它创建`ArrayList`来保存传递给该方法的原始单词。`for`循环用于从列表中移除停用词。`contains`方法将确定提交的单词是否是停用词，如果是，则删除它。`ArrayList`被转换成一个字符串数组，然后返回。这显示如下:

```java
public String[] removeStopWords(String[] words) { 
    ArrayList<String> tokens =  
        new ArrayList<String>(Arrays.asList(words)); 
    for (int i = 0; i < tokens.size(); i++) { 
        if (stopWords.contains(tokens.get(i))) { 
            tokens.remove(i); 
        } 
    } 
    return (String[]) tokens.toArray(
         new String[tokens.size()]); 
} 
```

下面的序列说明了如何使用停用词。首先，我们使用默认构造函数声明了一个`StopWords`类的实例。声明了 OpenNLP `SimpleTokenizer`类并定义了示例文本，如下所示:

```java
StopWords stopWords = new StopWords(); 
SimpleTokenizer simpleTokenizer = SimpleTokenizer.INSTANCE; 
paragraph = "A simple approach is to create a class " 
    + "to hold and remove stopwords."; 
```

示例文本被标记化，然后传递给`removeStopWords`方法。
然后显示新列表:

```java
String tokens[] = simpleTokenizer.tokenize(paragraph); 
String list[] = stopWords.removeStopWords(tokens); 
for (String word : list) { 
    System.out.println(word); 
}
```

执行时，我们得到以下输出。`A`未被移除，因为它是大写的，并且该类不执行大小写转换:

```java
A
simple
approach
create
class
hold
remove
stopwords
.  
```



# 使用 LingPipe 删除停用词

LingPipe 拥有`EnglishStopTokenizerFactory`类，我们将使用它来识别和删除停用词。这个列表中的单词可以在[`alias-I . com/ling pipe/docs/API/com/aliasi/token izer/englishstoptokenizerfactory . html`](http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/EnglishStopTokenizerFactory.html)找到。它们包括诸如 a、was、but、he 和 for 之类的词。

`factory`类的构造函数需要一个`TokenizerFactory`实例作为它的参数。我们将使用工厂的`tokenizer`方法来处理单词列表并删除停用词。我们首先声明要标记化的字符串:

```java
String paragraph = "A simple approach is to create a class "  
    + "to hold and remove stopwords."; 
```

接下来，我们基于`IndoEuropeanTokenizerFactory`类创建一个`TokenizerFactory`的实例。然后，我们使用该工厂作为参数来创建我们的`EnglishStopTokenizerFactory`实例:

```java
TokenizerFactory factory = 
 IndoEuropeanTokenizerFactory.INSTANCE; 
factory = new EnglishStopTokenizerFactory(factory); 
```

使用 LingPipe `Tokenizer`类和工厂的`tokenizer`方法，处理在`paragraph`变量中声明的文本。`tokenizer`方法使用了一个`char`数组，一个起始索引，其长度为:

```java
Tokenizer tokenizer = factory.tokenizer(paragraph.toCharArray(), 
   0, paragraph.length());
```

下面的 for-each 语句将迭代修改后的列表:

```java
for (String token : tokenizer) { 
    System.out.println(token); 
} 
```

输出如下所示:

```java
A
simple
approach
create
class
hold
remove
stopwords
.  
```

注意，尽管字母`A`是一个停用词，但它并没有从列表中删除。这是因为停用词表使用小写的 *a* ，而不是大写的 *A* 。结果，它漏掉了这个词。我们将在*使用管道*规格化部分纠正这个问题。



# 使用词干

寻找单词的词干需要去掉任何前缀或后缀，剩下的就是词干。识别词干对于寻找相似单词很重要的任务很有用。例如，搜索可能会寻找出现的单词，如 *book* 。有很多单词包含这个单词，包括 books、booked、bookings 和 bookmark。识别词干，然后在文档中查找它们的出现是很有用的。在许多情况下，这可以提高搜索的质量。

词干分析器可能产生不是真实单词的词干。例如，它可以决定 bounty、bounty 和 bountiful 都有相同的词干， *bounti* 。这对于搜索仍然很有用。

与词干相似的是词汇化。这是寻找它的引理的过程，它的形式就像在字典中找到的一样。这对于一些搜索也很有用。词干提取通常被视为一种更原始的技术，试图找到一个单词的*词根*涉及到切掉一个标记的开头和/或结尾部分。
词汇化可以被认为是一种更复杂的方法，它致力于寻找一个单词的词法或词汇意义。例如，单词*have*的词干为 *hav* ，而其词条为 *have* 。此外，单词*是*和*是*有不同的词干，但相同的引理，*是*。
词汇化通常比词干化使用更多的计算资源。它们都有自己的位置，它们的效用部分取决于需要解决的问题。



# 使用波特斯特梅尔

波特斯特梅尔是英语中常用的词干分析器。它的主页可以在 http://tartarus.org/martin/PorterStemmer/[找到。它用五个步骤来做一个单词。这些步骤是:](http://tartarus.org/martin/PorterStemmer/)

1.  改变复数，简单现在，过去和过去分词，将 y 转换为 I，例如 agreed 将被改为 agree，sleep 将被改为 sleepi
2.  将双后缀改为单后缀，例如专门化将改为专门化
3.  按照*步骤 2* 中的方法，将“特殊”更改为“特殊”
4.  通过将 speci 改为 speci 来更改剩余的单个后缀
5.  它删除 e 或删除末尾的双字母，例如属性将被更改为 attrib 或将被更改为 wil

虽然 Apache OpenNLP 1.5.3 不包含`PorterStemmer`类，但其源代码可以从[`SVN . Apache . org/repos/ASF/open NLP/trunk/open NLP-tools/src/main/Java/open NLP/tools/stemmer/porter stemmer . Java`](https://svn.apache.org/repos/asf/opennlp/trunk/opennlp-tools/src/main/java/opennlp/tools/stemmer/PorterStemmer.java)下载。然后可以将它添加到您的项目中。

在下面的例子中，我们演示了针对单词数组的`PorterStemmer`类。输入可能很容易来自其他文本源。创建了一个`PorterStemmer`类的实例，然后将它的`stem`方法应用于数组中的每个单词:

```java
String words[] = {"bank", "banking", "banks", "banker", "banked", 
     "bankart"}; 
PorterStemmer ps = new PorterStemmer(); 
for(String word : words) { 
    String stem = ps.stem(word); 
    System.out.println("Word: " + word + "  Stem: " + stem); 
} 
```

执行时，您将获得以下输出:

```java
Word: bank  Stem: bank
Word: banking  Stem: bank
Word: banks  Stem: bank
Word: banker  Stem: banker
Word: banked  Stem: bank
Word: bankart  Stem: bankart  
```

最后一个词与单词*损害*结合使用，如 *Bankart 损害*。这是肩膀的伤，和前面的话没有太大关系。它确实表明在寻找词干时只使用普通词缀。

其他可能有用的`PorterStemmer`类方法可以在下表中找到:

| **方法** | **意为** |
| `add` | 这将在当前词干的末尾添加一个`char` |
| `stem` | 如果出现不同的词干，不带参数的方法将返回`true` |
| `reset` | 重置词干分析器，以便使用不同的单词 |



# 用 LingPipe 堵塞

`PorterStemmerTokenizerFactory`类用于使用 LingPipe 查找词干。在这个例子中，我们将使用与使用波特斯特梅尔一节中的*相同的单词数组。`IndoEuropeanTokenizerFactory`类用于执行初始标记化，随后使用波特斯特梅尔。这些类别的定义如下:*

```java
TokenizerFactory tokenizerFactory = 
 IndoEuropeanTokenizerFactory.INSTANCE; 
TokenizerFactory porterFactory =  
    new PorterStemmerTokenizerFactory(tokenizerFactory); 
```

接下来声明一个保存词干的数组。我们重用了上一节中声明的`words`数组。每个单词都是单独处理的。单词被标记化，其词干存储在`stems`中，如下面的代码块所示。然后显示单词及其词干:

```java
String[] stems = new String[words.length]; 
for (int i = 0; i < words.length; i++) { 
    Tokenization tokenizer = new Tokenization(words[i],porterFactory); 
    stems = tokenizer.tokens(); 
    System.out.print("Word: " + words[i]); 
    for (String stem : stems) { 
        System.out.println("  Stem: " + stem); 
    } 
} 
```

执行时，我们得到以下输出:

```java
Word: bank  Stem: bank
Word: banking  Stem: bank
Word: banks  Stem: bank
Word: banker  Stem: banker
Word: banked  Stem: bank
Word: bankart  Stem: bankart  
```

我们已经使用 OpenNLP 和 LingPipe 示例演示了波特斯特梅尔。值得注意的是，还有其他类型的词干分析器可用，包括 Ngrams 和各种混合概率/算法方法。



# 使用词汇化

许多 NLP APIs 都支持词汇化。在这一节中，我们将说明如何使用`StanfordCoreNLP`和`OpenNLPLemmatizer`类来执行词汇化。词汇化过程决定了一个词的词汇。一个引理可以被认为是一个单词的字典形式。例如，*的引理是*是*是*。



# 使用 StanfordLemmatizer 类

我们将使用带有管道的`StanfordCoreNLP`类来演示术语化。我们首先用四个标注器设置管道，包括`lemma`，如下所示:

```java
StanfordCoreNLP pipeline; 
Properties props = new Properties(); 
props.put("annotators", "tokenize, ssplit, pos, lemma"); 
pipeline = new StanfordCoreNLP(props);
```

这些注释器是必需的，解释如下:

| **注释者** | **要执行的操作** |
| --- | --- |
| `tokenize` | 标记化 |
| `ssplit` | 分句 |
| `pos` | 词性标注 |
| `lemma` | 词汇化 |
| `ner` | NER |
| `parse` | 句法分析 |
| `dcoref` | 共指消解 |

一个`paragraph`变量与`Annotation`构造函数一起使用，然后执行`annotate`方法，如下所示:

```java
String paragraph = "Similar to stemming is Lemmatization. "  
    +"This is the process of finding its lemma, its form " +  
    +"as found in a dictionary."; 
Annotation document = new Annotation(paragraph); 
pipeline.annotate(document); 
```

我们现在需要迭代语句和语句的标记。`Annotation`和`CoreMap` class' `get`方法将返回指定类型的值。如果没有指定类型的值，它将返回`null`。我们将使用这些类来获得一个引理列表。

首先，返回一个句子列表，然后处理每个句子的每个单词以找到词条。这里声明了`sentences`和`lemmas`的列表:

```java
List<CoreMap> sentences = 
     document.get(SentencesAnnotation.class); 
List<String> lemmas = new LinkedList<>(); 
```

两个 for-each 语句迭代语句来填充`lemmas`列表。
完成后，将显示列表:

```java
for (CoreMap sentence : sentences) { 
    for (CoreLabelword : sentence.get(TokensAnnotation.class)) { 
        lemmas.add(word.get(LemmaAnnotation.class)); 
    } 
} 

System.out.print("[");
```

```java
for (String element : lemmas) { 
    System.out.print(element + " "); 
} 
System.out.println("]"); 
```

该序列的输出如下:

```java
    [similar to stem be lemmatization . this be the process of find its lemma , its form as find in a dictionary . ]
```

将它与原始测试进行比较，我们可以看到它做得非常好:

```java
    Similar to stemming is Lemmatization. This is the process of finding its lemma, its form as found in a dictionary. 
```



# 在 OpenNLP 中使用词汇化

OpenNLP 还支持使用`JWNLDictionary`类的词汇化。此类的构造函数使用一个字符串，该字符串包含用于标识根的字典文件的路径。我们将使用普林斯顿大学([wordnet.princeton.edu](https://wordnet.princeton.edu/))开发的 WordNet 字典。实际的字典是存储在目录中的一系列文件。这些文件包含单词列表和它们的*词根*。对于本节中使用的示例，我们将使用在[`code.google.com/p/xssm/downloads/detail?找到的词典 name = similarityutils . zip&can = 2&q =`](https://code.google.com/p/xssm/downloads/detail?name=SimilarityUtils.zip&can=2&q=)。

向`JWNLDictionary` class' `getLemmas`方法传递我们想要处理的单词和第二个参数，该参数指定单词的位置。如果我们想要准确的结果，那么词性匹配实际的单词类型是很重要的。

在下面的代码序列中，我们使用以`\dict\`结尾的路径创建了一个`JWNLDictionary`类的实例。这是字典的位置。我们还定义了样本文本。构造函数可以抛出`IOException`和`JWNLException`，我们在`try...catch`块序列中处理它们:

```java
try { 
    dictionary = new JWNLDictionary("...\dict\"); 
    paragraph = "Eat, drink, and be merry, for life is but a dream"; 
    ... 
} catch (IOException | JWNLException ex) 
    // 
}
```

在文本初始化之后，添加以下语句。首先，我们使用`WhitespaceTokenizer`类对字符串进行标记，正如在*使用 WhitespaceTokenizer 类*一节中所解释的。然后，将每个令牌传递给`getLemmas`方法，用一个空字符串作为 POS 类型。然后显示原始令牌及其`lemmas`:

```java
String tokens[] = 
     WhitespaceTokenizer.INSTANCE.tokenize(paragraph); 
for (String token : tokens) { 
    String[] lemmas = dictionary.getLemmas(token, ""); 
    for (String lemma : lemmas) { 
        System.out.println("Token: " + token + "  Lemma: " 
             + lemma); 
    } 
} 
```

输出如下所示:

```java
Token: Eat,  Lemma: at
Token: drink,  Lemma: drink
Token: be  Lemma: be
Token: life  Lemma: life
Token: is  Lemma: is
Token: is  Lemma: i
Token: a  Lemma: a
Token: dream  Lemma: dream  
```

除了返回两个引理的`is`标记之外，引理化过程工作得很好。第二个无效。这说明了为令牌使用正确 POS 的重要性。我们可以使用一个或多个 POS 标签作为`getLemmas`方法的参数。然而，这回避了一个问题:我们如何确定正确的位置？这个话题在第五章、*检测词性*中详细讨论。

下表列出了 POS 标签的简短列表。本榜单改编自[`www . ling . upenn . edu/courses/Fall _ 2003/ling 001/Penn _ tree bank _ pos . html`](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)。宾夕法尼亚大学树库标签集的完整列表可以在 http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html[找到:](http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html)

| **标签** | **描述** |
| --- | --- |
| 姐姐(网络用语)ˌ法官ˌ裁判员(judges) | 形容词 |
| 神经网络 | 名词，单数，还是复数 |
| NNS | Noun, plural |
| NNP | 专有名词，单数 |
| NNPS | 专有名词，复数 |
| 刷卡机 | 所有格结尾 |
| 富含血小板血浆 | 人称代词 |
| 铷 | 副词 |
| 菲律宾共和国 | 颗粒 |
| 动词 | 动词，基本形式 |
| VBD | 动词，过去式 |
| VBG | 动词、动名词或现在分词 |



# 使用管道进行规范化

在这一节中，我们将使用管道结合许多规范化技术。为了演示这个过程，我们将扩展在*中使用 LingPipe* 一节中使用的例子来删除停用词。我们将添加两个额外的工厂来规范化文本:`LowerCaseTokenizerFactory`和`PorterStemmerTokenizerFactory`。

在`EnglishStopTokenizerFactory`创建之前添加`LowerCaseTokenizerFactory`工厂，在`EnglishStopTokenizerFactory`创建之后添加`PorterStemmerTokenizerFactory`，如下图:

```java
paragraph = "A simple approach is to create a class " 
     + "to hold and remove stopwords."; 
TokenizerFactory factory = 
     IndoEuropeanTokenizerFactory.INSTANCE; 
factory = new LowerCaseTokenizerFactory(factory); 
factory = new EnglishStopTokenizerFactory(factory); 
factory = new PorterStemmerTokenizerFactory(factory); 
Tokenizer tokenizer = 
     factory.tokenizer(paragraph.toCharArray(), 0, 
     paragraph.length()); 
for (String token : tokenizer) { 
    System.out.println(token); 
} 
```

输出如下所示:

```java
simpl
approach
creat
class
```

```java
hold
remov
stopword
.  
```

我们剩下的是去掉了停用词的小写单词的词干。



# 摘要

在这一章中，我们举例说明了标记文本和对文本执行规范化的各种方法。我们从基于核心 Java 类的简单标记化技术开始，比如`String`类的`split`方法和`StringTokenizer`类。当我们决定放弃使用 NLP API 类时，这些方法会很有用。

我们演示了如何使用 OpenNLP、Stanford 和 LingPipe APIs 执行标记化。我们发现，在如何执行标记化以及可以在这些 API 中应用的选项方面存在差异。提供了它们产出的简要比较。

讨论了规范化，这可能涉及到将字符转换为小写、扩展缩写、删除停用词、词干和词汇化。我们展示了如何使用核心 Java 类和 NLP APIs 来应用这些技术。

在下一章第三章、*寻找句子、*中，我们将研究使用各种 NLP APIs 确定句子结尾所涉及的问题。