

# 一、自然语言处理简介

**自然语言处理** ( **NLP** )是一个广泛的话题，专注于使用计算机来分析自然语言。它涉及诸如语音处理、关系提取、文档分类和文本汇总等领域。然而，这些类型的分析是基于一套基本的技术，如标记化、句子检测、分类和提取关系。这些基本技术是本书的重点。我们将从 NLP 的详细讨论开始，研究它为什么重要，并确定应用领域。

有许多工具支持 NLP 任务。我们将重点关注 Java 语言以及各种 Java **应用程序员接口**(**API**)如何支持 NLP。在这一章中，我们将简要介绍主要的 API，包括 Apache 的 OpenNLP、Stanford NLP 库、LingPipe 和 GATE。

接下来是对本书中描述的基本 NLP 技术的讨论。这些技术的本质和使用通过一个 NLP APIs 来展示和说明。这些技术中的许多将使用模型。模型类似于一组用于执行任务(如标记文本)的规则。它们通常由从文件实例化的类来表示。我们将用一个关于如何准备数据来支持 NLP 任务的简短讨论来结束这一章。

NLP 不容易。虽然有些问题可以相对容易地解决，但还有许多其他问题需要使用复杂的技术。我们将努力为 NLP 处理提供一个基础，这样你将能够更好地理解哪种技术适用于给定的问题。

NLP 是一个庞大而复杂的领域。在本书中，我们只能解决其中的一小部分。我们将关注可以使用 Java 实现的核心 NLP 任务。在本书中，我们将使用 Java SE SDK 和其他库(如 OpenNLP 和 Stanford NLP)演示许多 NLP 技术。要使用这些库，需要将特定的 API JAR 文件与使用它们的项目相关联。关于这些库的讨论可以在 NLP 工具的部分的*调查中找到，并且包含到这些库的下载链接。本书中的示例是使用 NetBeans 8.0.2 开发的。这些项目要求将 API JAR 文件添加到“项目属性”对话框的“库”类别。*

在本章中，我们将了解以下主题:

*   什么是 NLP？
*   为什么要用 NLP？
*   为什么 NLP 这么难？
*   自然语言处理工具综述
*   面向 Java 的深度学习
*   文本处理任务概述
*   了解 NLP 模型
*   准备数据



# 什么是 NLP？

NLP 的正式定义经常包括大意如下的措辞:它是使用计算机科学、**人工智能** ( **AI** )和形式语言学概念来分析自然语言的研究领域。一个不太正式的定义表明，它是一组用于从自然语言源(如网页和文本文档)中获取有意义和有用信息的工具。

有意义和有用意味着它有一些商业价值，尽管它经常用于学术问题。这一点从它对搜索引擎的支持中可以很容易地看出来。使用 NLP 技术处理用户查询，以便生成用户可以使用的结果页面。现代搜索引擎在这方面非常成功。NLP 技术也在自动帮助系统和支持复杂查询系统中得到应用，如 IBM 的 Watson 项目。

当我们使用一种语言时，经常会遇到语法和语义这两个术语。语言的句法指的是控制有效句子结构的规则。例如，英语中常见的句子结构以主语开头，后跟动词，然后是宾语，如“蒂姆击球了”我们不习惯不寻常的句子顺序，如“击球添”虽然英语的句法规则不像计算机语言那样严格，但我们仍然希望句子遵循基本的句法规则。

句子的语义就是它的意思。作为说英语的人，我们理解“蒂姆击球”这句话的意思然而，英语和其他自然语言有时会有歧义，一个句子的意思可能只能根据它的上下文来确定。正如我们将看到的，各种机器学习技术可以用来尝试推导文本的含义。

随着讨论的深入，我们将介绍许多语言学术语，这些术语将帮助我们更好地理解自然语言，并为我们提供一个通用词汇来解释各种 NLP 技术。我们将看到如何将文本分割成单个元素，以及如何对这些元素进行分类。

一般来说，这些方法用于增强应用程序，从而使它们对用户更有价值。NLP 的使用范围可以从相对简单的使用到那些推动今天可能的事情。在本书中，我们将展示一些例子来说明简单的方法，这些方法可能是解决某些问题所需要的全部，这些方法可以使用更高级的库和类来解决复杂的需求。



# 为什么要用 NLP？

NLP 在各种各样的学科中被用来解决许多不同类型的问题。文本分析是在文本上执行的，范围从用户输入的用于因特网查询的几个单词到需要摘要的多个文档。近年来，我们看到非结构化数据的数量和可用性大幅增长。这采取了博客、推特和各种其他社交媒体的形式。NLP 是分析这类信息的理想工具。

机器学习和文本分析经常被用来增强应用程序的效用。应用领域的简要列表如下:

*   **搜索**:识别文本的特定元素。它可以简单到在文档中查找某个名称的出现，或者可能涉及使用同义词和替换拼写/拼写错误来查找与原始搜索字符串接近的条目。
*   机器翻译:这通常包括将一种自然语言翻译成另一种语言。
*   **总结**:段落、文章、文档或文档集合可能需要进行总结。NLP 已经成功地用于这个目的。
*   命名实体识别 ( **NER** ):这包括从文本中提取地名、人名和事物名。通常，这与其他 NLP 任务结合使用，如处理查询。
*   **信息分组**:这是一个重要的活动，它获取文本数据并创建一组反映文档内容的类别。您可能遇到过许多网站，这些网站根据您的需求组织数据，并在网站的左侧列出了类别。
*   词性标注 ( **词性**):在这个任务中，文本被分成不同的语法元素，比如名词和动词。这对进一步分析文本很有用。
*   **情感分析**:人们对电影、书籍和其他产品的感觉和态度可以用这种技术来确定。这有助于提供关于产品感觉如何的自动反馈。
*   **回答问题**:当 IBM 的沃森成功赢得一场危险竞赛时，这种类型的处理得到了说明。然而，它的用途并不仅限于赢得比赛，还被用于许多其他领域，包括医学。
*   语音识别:人类的语音很难分析。这个领域的许多进展都是 NLP 努力的结果。
*   自然语言生成 ( **NLG** ):这是从一个数据或知识源，比如数据库，生成文本的过程。它可以自动报告信息，如天气报告，或总结医疗报告。

NLP 任务经常使用不同的机器学习技术。一种常见的方法是从训练模型执行任务开始，验证模型是否正确，然后将模型应用于问题。我们将在*了解 NLP 模型*部分进一步研究这一过程。



# 为什么 NLP 这么难？

NLP 不容易。有几个因素使这个过程变得困难。例如，有数百种自然语言，每种语言都有不同的语法规则。当单词的意思依赖于上下文时，它们可能是模糊的。在这里，我们将检查几个更重要的问题领域。

在性格层面，有几个因素需要考虑。例如，需要考虑用于文档的编码方案。可以使用 ASCII、UTF-8、UTF-16 或 Latin-1 等方案对文本进行编码。可能需要考虑其他因素，如文本是否应区分大小写。标点符号和数字可能需要特殊处理。我们有时需要考虑使用表情符号(字符组合和特殊字符图像)、超链接、重复标点(...或-)、文件扩展名以及嵌入句点的用户名。正如我们将在*准备数据*一节中讨论的，其中许多都是通过预处理文本来处理的。

当我们**对**文本进行标记时，通常意味着我们将文本分解成一系列单词。这些文字被称为**令牌**。这个过程被称为**标记化**。当一种语言使用空白字符来描述单词时，这个过程并不太困难。对于像汉语这样的语言来说，这可能相当困难，因为它使用独特的单词符号。

单词和词素可能需要被分配一个**词性** ( **词性**)标签，以识别它是什么类型的单位。一个**语素**是有意义的文本的最小划分。前缀和后缀是语素的例子。通常，当我们处理单词时，我们需要考虑同义词、缩写词、首字母缩略词和拼写。

词干是可能需要应用的另一项任务。**词干化**就是找到一个单词的词干的过程。例如，像*走*、*走*或*走*这样的单词有词干*走*。搜索引擎经常使用词干来帮助查询。

与词干紧密相关的是**词条化**的过程。这个过程决定了一个词的基本形式，称为它的**引理**。比如*操作*这个词，它的词干是 *oper* 但它的词条是*oper*。引理化是一个比词干化更精细的过程，它使用词汇和形态学技术来找到一个引理。在某些情况下，这可以导致更精确的分析。

单词被组合成短语和句子。句子检测可能会有问题，并且不像查找句子末尾的句号那么简单。句点出现在许多地方，包括缩写如 Ms .，以及数字如 12.834。

我们经常需要了解句子中哪些词是名词，哪些是动词。我们经常关心词与词之间的关系。例如，**共指解析**确定一个或多个句子中某些词之间的关系。考虑下面的句子:

“这座城市很大，但很美。它充满了整个山谷。”

单词 *it* 是 city 的同指。当一个单词有多个意思时，我们可能需要执行**词义消歧** ( **WSD** )来确定想要的意思。这有时很难做到。例如，“约翰回家了。”家是指房子、城市还是其他单位？它的意思有时可以从使用它的上下文中推断出来。例如，“约翰回家了。它位于一条死胡同的尽头。”

尽管有这些困难，NLP 在大多数情况下都能够很好地完成这些任务，并为许多问题域提供附加值。例如，可以对客户推文进行情感分析，从而为不满意的客户提供可能的免费产品。可以很容易地对医学文档进行总结，以突出相关主题并提高生产率。

**总结**是产生不同单位的简短描述的过程。这些单元可以包括多个句子、段落、一个文档或多个文档。目的可能是识别那些传达单元意思的句子，确定理解单元的先决条件，或者在这些单元中寻找项目。通常，文本的上下文对完成这项任务很重要。



# 自然语言处理工具综述

有许多工具支持 NLP。Java SE SDK 提供了其中的一些功能，但是除了最简单的问题之外，这些功能的实用性有限。其他库，如 Apache 的 OpenNLP 和 LingPipe，为 NLP 问题提供了广泛而复杂的支持。

低级 Java 支持包括字符串库，比如`String`、`StringBuilder`和`StringBuffer`。这些类拥有执行搜索、匹配和文本替换的方法。**正则表达式**使用特殊编码来匹配子字符串。Java 提供了一套丰富的使用正则表达式的技术。

如前所述，记号赋予器用于将文本分割成单独的元素。Java 通过以下方式为标记器提供支持:

*   `String`类的`split`方法
*   `StreamTokenizer`类
*   `StringTokenizer`类

也存在许多用于 Java 的 NLP 库/API。下表列出了部分基于 Java 的 NLP APIs。其中大部分都是开源的。此外，还有许多商业 API 可供使用。我们将关注开源 API:

| **API** | **网址** |
| Apertium | [`www.apertium.org/`](http://www.apertium.org/) |
| 文本工程的通用体系结构 | [`gate.ac.uk/`](http://gate.ac.uk/) |
| 基于学习的 Java | [`github.com/CogComp/lbjava`](https://github.com/CogComp/lbjava) |
| 灵管 | [`alias-i.com/lingpipe/`](http://alias-i.com/lingpipe/) |
| 木槌 | [`mallet.cs.umass.edu/`](http://mallet.cs.umass.edu/) |
| 蒙特林瓜 | [`web.media.mit.edu/~hugo/montylingua/`](http://web.media.mit.edu/~hugo/montylingua/) |
| Apache OpenNLP | [`opennlp.apache.org/`](http://opennlp.apache.org/) |
| UIMA | [`uima.apache.org/`](http://uima.apache.org/) |
| 斯坦福解析器 | [`nlp.stanford.edu/software`](http://nlp.stanford.edu/software) |
| Apache Lucene 核心 | [`lucene.apache.org/core/`](https://lucene.apache.org/core/) |
| 雪球 | [`snowballstem.org/`](http://snowballstem.org/) |

这些 NLP 任务中的许多被组合起来形成一个**管道**。流水线由各种 NLP 任务组成，这些任务被集成到一系列步骤中以实现处理目标。支持管道的框架的例子有**文本工程通用架构** ( **门**)和阿帕奇 UIMA。

在下一节中，我们将更深入地讨论几个 NLP APIs。本文将简要概述它们的功能，并列出每个 API 的有用链接。



# Apache OpenNLP

Apache OpenNLP 项目是一个基于机器学习的工具包，用于处理自然语言文本；它解决了常见的 NLP 任务，并将贯穿本书。它由几个组件组成，这些组件执行特定的任务，允许对模型进行训练，并支持对模型进行测试。OpenNLP 使用的一般方法是从文件中实例化一个支持任务的模型，然后针对该模型执行方法来执行任务。

例如，在下面的序列中，我们将标记一个简单的字符串。为了正确执行该代码，它必须处理`FileNotFoundException`和`IOException`异常。我们使用 try-with-resource 块通过`en-token.bin`文件打开一个`FileInputStream`实例。该文件包含一个使用英文文本训练的模型:

```java
try (InputStream is = new FileInputStream( 
        new File(getModelDir(), "en-token.bin"))){ 
    // Insert code to tokenize the text 
} catch (FileNotFoundException ex) { 
    ... 
} catch (IOException ex) { 
    ... 
} 
```

然后使用这个文件在`try`块中创建一个`TokenizerModel`类的实例。接下来，我们创建一个`Tokenizer`类的实例，如下所示:

```java
TokenizerModel model = new TokenizerModel(is); 
Tokenizer tokenizer = new TokenizerME(model); 
```

然后应用`tokenize`方法，它的参数是要标记的文本。该方法返回一组`String`对象:

```java
String tokens[] = tokenizer.tokenize("He lives at 1511 W." 
  + "Randolph."); 
```

for-each 语句显示标记，如下所示。左括号和右括号用于清楚地识别标记:

```java
for (String a : tokens) { 
  System.out.print("[" + a + "] "); 
} 
System.out.println(); 
```

当我们执行这个命令时，我们将得到以下输出:

```java
[He] [lives] [at] [1511] [W.] [Randolph] [.]  
```

在这种情况下，标记器识别出`W.`是一个缩写，最后一个句点是一个单独的标记，用来区分句子的结尾。

我们将在本书的许多例子中使用 OpenNLP API。OpenNLP 链接列于下表 *:*

| **OpenNLP** | **网站** |
| 主页 |  [【https://opennlp.apache.org/】](https://opennlp.apache.org/)  |
| 证明文件 |  [](https://opennlp.apache.org/docs/)  |
| Javadoc |  [【http://nlp.stanford.edu/nlp/javadoc/javanlp/index.html】](http://nlp.stanford.edu/nlp/javadoc/javanlp/index.html)  |
| [计] 下载 |  [【https://opennlp.apache.org/cgi-bin/download.cgi】](https://opennlp.apache.org/cgi-bin/download.cgi)  |
| 维基网 |  [`CWI ki . Apache . org/confluence/display/open NLP/Index % 3b 408 c 73729 acccdd 071d 9 EC 354 fc 54`](https://cwiki.apache.org/confluence/display/OPENNLP/Index%3bjsessionid=32B408C73729ACCCDD071D9EC354FC54)  |



# 斯坦福 NLP

斯坦福大学自然语言处理小组进行自然语言处理研究，并为自然语言处理任务提供工具。斯坦福 CoreNLP 就是这些工具集之一。此外，还有其他工具集，如斯坦福解析器、斯坦福 POS 标记器和斯坦福分类器。斯坦福工具支持英语和汉语以及基本的 NLP 任务，包括标记化和名称实体识别。

这些工具是在完整的 GPL 下发布的，但是它不允许在商业应用程序中使用它们，尽管商业许可证是可用的。该 API 组织良好，支持核心的 NLP 功能。

斯坦福小组支持几种标记化方法。我们将使用`PTBTokenizer`类来说明这个 NLP 库的使用。这里演示的构造函数使用一个`Reader`对象、一个`LexedTokenFactory<T>`参数和一个字符串来指定要使用几个选项中的哪一个。

`LexedTokenFactory`是由`CoreLabelTokenFactory`和`WordTokenFactory`类实现的接口。前一个类支持保留标记的开始和结束字符位置，而后一个类只是将标记作为不带任何位置信息的字符串返回。默认情况下使用`WordTokenFactory`类。

在下面的例子中使用了`CoreLabelTokenFactory`类。使用字符串创建一个`StringReader`。最后一个参数用于选项参数，在本例中是`null`。`Iterator`接口由`PTBTokenizer`类实现，允许我们使用`hasNext`和`next`方法来显示令牌:

```java
PTBTokenizer ptb = new PTBTokenizer( 
new StringReader("He lives at 1511 W. Randolph."), 
new CoreLabelTokenFactory(), null); 
while (ptb.hasNext()) { 
  System.out.println(ptb.next()); 
} 
```

输出如下所示:

```java
He
lives
at
1511
W.
Randolph
.  
```

我们将在本书中广泛使用斯坦福大学的 NLP 图书馆。下表列出了斯坦福大学的链接。每个发行版中都有文档和下载链接:

| **斯坦福 NLP** | **网站** |
| 主页 |  [【http://nlp.stanford.edu/index.shtml】](http://nlp.stanford.edu/index.shtml)  |
| -好的 |  [【http://nlp.stanford.edu/software/corenlp.shtml#Download】](http://nlp.stanford.edu/software/corenlp.shtml#Download)  |
| 句法分析程序 |  [【http://nlp.stanford.edu/software/lex-parser.shtml】](http://nlp.stanford.edu/software/lex-parser.shtml)  |
| POS 标签 |  [【http://nlp.stanford.edu/software/tagger.shtml】](http://nlp.stanford.edu/software/tagger.shtml)  |
| Java-NLP-用户邮件列表 |  [【https://mailman.stanford.edu/mailman/listinfo/java-nlp-user】](https://mailman.stanford.edu/mailman/listinfo/java-nlp-user)  |



# 灵管

LingPipe 由一组执行常见 NLP 任务的工具组成。它支持模型训练和测试。该工具有免版税版本和授权版本。免费版本的生产使用是有限的。

为了演示 LingPipe 的用法，我们将使用`Tokenizer`类来说明如何使用它来标记文本。首先声明两个列表，一个保存标记，另一个保存空白:

```java
List<String> tokenList = new ArrayList<>(); 
List<String> whiteList = new ArrayList<>(); 
```

You can download the example code files for all Packt books you have purchased from your account at [`www.packtpub.com`](http://www.packtpub.com). If you purchased this book elsewhere, you can visit [`www.packtpub.com/support`](http://www.packtpub.com/support) and register to have the files emailed directly to you.

接下来，声明一个字符串来保存要标记的文本:

```java
String text = "A sample sentence processed \nby \tthe " + 
    "LingPipe tokenizer."; 
```

现在，创建一个`Tokenizer`类的实例。如下面的代码块所示，一个静态的`tokenizer`方法被用来创建一个基于`Indo-European factory`类的`Tokenizer`类的实例:

```java
Tokenizer tokenizer = IndoEuropeanTokenizerFactory.INSTANCE. 
tokenizer(text.toCharArray(), 0, text.length()); 
```

然后使用该类的`tokenize`方法来填充这两个列表:

```java
tokenizer.tokenize(tokenList, whiteList); 
```

使用 for-each 语句显示标记:

```java
for(String element : tokenList) { 
  System.out.print(element + " "); 
} 
System.out.println(); 
```

此示例的输出如下所示:

```java
A sample sentence processed by the LingPipe tokenizer
```

下表列出了 LingPipe 链接:

| **灵管** | **网站** |
| 主页 |  [【http://alias-i.com/lingpipe/index.html】](http://alias-i.com/lingpipe/index.html)  |
| 教程 |  [【http://alias-i.com/lingpipe/demos/tutorial/read-me.html】](http://alias-i.com/lingpipe/demos/tutorial/read-me.html)  |
| JavaDocs |  [【http://alias-i.com/lingpipe/docs/api/index.html】](http://alias-i.com/lingpipe/docs/api/index.html)  |
| [计] 下载 |  [【http://alias-i.com/lingpipe/web/install.html】](http://alias-i.com/lingpipe/web/install.html)  |
| 核心 |  [【http://alias-i.com/lingpipe/web/download.html】](http://alias-i.com/lingpipe/web/download.html)  |
| 模型 |  [【http://alias-i.com/lingpipe/web/models.html】](http://alias-i.com/lingpipe/web/models.html)  |



# 大门

GATE 是一套用 Java 编写的工具，由英国谢菲尔德大学开发。它支持许多 NLP 任务和语言。它也可以用作 NLP 处理的管道。它支持一个 API 和 GATE Developer，GATE Developer 是一个显示文本和注释的文档查看器。这对于使用突出显示的注释检查文档非常有用。盖茨米伊美，一个索引和搜索各种来源产生的文本的工具，也是可用的。使用 GATE 完成许多 NLP 任务需要一些代码。GATE Embedded 用于将 GATE 功能直接嵌入到代码中。下表列出了有用的门链接:

| **Gate** | **网站** |
| 主页 |  [【https://gate.ac.uk/】](https://gate.ac.uk/)  |
| 证明文件 |  [【https://gate.ac.uk/documentation.html】](https://gate.ac.uk/documentation.html)  |
| JavaDocs |  [【http://jenkins.gate.ac.uk/job/GATE-Nightly/javadoc/】](http://jenkins.gate.ac.uk/job/GATE-Nightly/javadoc/)  |
| [计] 下载 |  [【https://gate.ac.uk/download/】](https://gate.ac.uk/download/)  |
| 维基网 |  [【http://gatewiki.sf.net/】](http://gatewiki.sf.net/)  |

TwitIE 是一个开源的信息提取管道。它包含以下内容:

*   社交媒体数据-语言识别
*   Twitter 标记器，用于处理表情符号、用户名、URL 等等
*   POS 标签
*   文本规范化

它是 GATE Twitter 插件的一部分。下表列出了所需的链接:

| **定理** | **网站** |
| 主页 | [`gate.ac.uk/wiki/twitie.html`](https://gate.ac.uk/wiki/twitie.html) |
| 证明文件 | [`gate . AC . uk/sale/ranlp 2013/tw itie/tw itie-ranlp 2013 . pdf？m=1`](https://gate.ac.uk/sale/ranlp2013/twitie/twitie-ranlp2013.pdf?m=1) |



# UIMA

结构化信息标准促进组织是一个专注于面向信息的商业技术的联盟。它开发了**非结构化信息管理架构** ( **UIMA** )标准作为 NLP 管道的框架。它得到了阿帕奇 UIMA 公司的支持。

尽管它支持管道创建，但它也描述了一系列用于文本分析的设计模式、数据表示和用户角色。下表列出了 UIMA 链接:

| **阿帕奇 UIMA** | **网站** |
| 主页 |  [【https://uima.apache.org/】](https://uima.apache.org/)  |
| 证明文件 |  [【https://uima.apache.org/documentation.html】](https://uima.apache.org/documentation.html)  |
| JavaDocs |  [【https://uima.apache.org/d/uimaj-2.6.0/apidocs/index.html】](https://uima.apache.org/d/uimaj-2.6.0/apidocs/index.html)  |
| [计] 下载 |  [【https://uima.apache.org/downloads.cgi】](https://uima.apache.org/downloads.cgi)  |
| 维基网 |  [【https://cwiki.apache.org/confluence/display/UIMA/Index】](https://cwiki.apache.org/confluence/display/UIMA/Index)  |



# Apache Lucene 核心

Apache Lucene Core 是用 Java 编写的全功能文本搜索引擎的开源库。它使用标记化将文本分成小块来索引元素。它还为分析目的提供了标记化前和标记化后的选项。它支持词干化、过滤、文本规范化和标记化后的同义词扩展。使用时，它创建一个目录和索引文件，并可用于搜索内容。它不能作为 NLP 工具包，但它提供了处理文本和高级字符串操作的强大工具。它提供了一个免费的搜索引擎。下表列出了 Apache Lucene 的重要链接:

| **阿帕奇 Lucene** | **网站** |
| 主页 | [`lucene.apache.org/`](http://lucene.apache.org/) |
| 证明文件 | [`lucene.apache.org/core/documentation.html`](http://lucene.apache.org/core/documentation.html) |
| JavaDocs | [`lucene.apache.org/core/7_3_0/core/index.html`](http://lucene.apache.org/core/7_3_0/core/index.html) |
| [计] 下载 | [`Lucene . Apache . org/core/mirrors-core-latest-redir . html？`](http://lucene.apache.org/core/mirrors-core-latest-redir.html?) |



# 面向 Java 的深度学习

深度学习是机器学习的一部分，是人工智能的一个子集。深度学习的灵感来自人类大脑在其生物形式中的功能。它使用神经元等术语来创建神经网络，这可以是监督或无监督学习的一部分。深度学习概念广泛应用于计算机视觉、语音识别、NLP、社交网络分析和过滤、欺诈检测、预测等领域。深度学习在 2010 年的图像处理领域证明了自己，当时它在一次图像网络竞赛中胜过了所有其他人，现在它已经开始在 NLP 中显示出有希望的结果。深度学习表现非常好的一些领域包括**命名实体识别** ( **NER** )、情感分析、词性标注、机器翻译、文本分类、字幕生成和问题回答。

这段精彩阅读可以在戈德堡在 https://arxiv.org/abs/1510.00726 的作品中找到。有各种工具和库可用于深度学习。下面是一个库列表，可以帮助您入门:

*   **deep learning 4j**(【https://deeplearning4j.org/】??):这是一个面向 JVM 的开源、分布式深度学习库。
*   Weka([`www.cs.waikato.ac.nz/ml/weka/index.html`](https://www.cs.waikato.ac.nz/ml/weka/index.html)):它是 Java 中的一款数据挖掘软件，拥有一系列支持预处理、预测、回归、聚类、关联规则和可视化的机器学习算法。
*   **海量在线分析**(**MOA**)(【https://moa.cms.waikato.ac.nz/】??):用于实时流。支持机器学习和数据挖掘。
*   由索引结构支持的 KDD 应用程序开发环境(??)(**埃尔基**)([`elki-project.github.io/`](https://elki-project.github.io/)):这是一个数据挖掘软件，专注于研究算法，强调聚类分析和离群点检测中的非监督方法。
*   **Neuroph**(【http://neuroph.sourceforge.net/index.html】??):它是一个轻量级的 Java 神经网络框架，用于开发 Apache Licensee 2.0 许可的神经网络架构。它还支持用于创建和训练数据集的 GUI 工具。
*   aero solve(【http://airbnb.io/aerosolve/】??):这是一个为人类设计的机器学习包，就像在网上看到的那样。它由 Airbnb 开发，更倾向于机器学习。

你可以在 GitHub([`github.com/search?l=Java&amp；q =深度+学习&amp；储存库& amp。utf8=%E2%9C%93`](https://github.com/search?l=Java&q=deep+learning&type=Repositories&utf8=%E2%9C%93) )用于深度学习和 Java。



# 文本处理任务概述

尽管有许多 NLP 任务可以执行，我们将只关注这些任务的一个子集。此处对这些任务进行了简要概述，这也反映在以下章节中:

*   第二章，*寻找正文部分*
*   第三章，*找句子*
*   第四章、*寻找人和事*
*   第五章、*检测词性*
*   第八章、*对文本和文件进行分类*
*   第十章，*使用解析器提取关系*
*   第十一章、*组合方式*

这些任务中有许多是与其他任务一起使用来实现目标的。我们将在阅读本书的过程中看到这一点。例如，在许多其他任务中，标记化经常被用作初始步骤。这是一个基本的步骤。



# 查找部分文本

文本可以分解成许多不同类型的元素，如单词、句子和段落。有几种方法对这些元素进行分类。当我们在本书中提到部分文本时，我们指的是单词，有时称为标记。**形态学**是研究词的结构。在我们对自然语言处理的探索中，我们将使用一些形态学术语。但是，对单词进行分类的方法有很多种，包括以下几种:

*   **简单词**:这些是一个词的意思的共同内涵，包括这句话里的 17 个词。
*   **语素**:这是一个词有意义的最小单位。例如，在有界一词中，*有界*被认为是一个语素。语素还包括后缀、 *ed* 等部分。
*   **前缀/后缀**:在词根之前或之后。例如，在单词 graduate 中， *ation* 是基于单词 *graduate 的后缀。*
*   **同义词**:这是一个和另一个单词意思相同的单词。像 small 和 tiny 这样的词可以被认为是同义词。解决这个问题需要词义消歧。
*   缩写:这些缩写词缩短了一个单词的用法。我们不用史密斯先生，而是用史密斯先生。
*   首字母缩写词:它们被广泛应用于许多领域，包括计算机科学。他们用字母组合来表示短语，如 FORTRAN 的公式翻译。它们可以是递归的，比如 GNU。当然，我们将继续使用的是 NLP。
*   缩写:我们会发现这些对于常用的单词组合很有用，比如这个句子的第一个单词。
*   **数字**:通常只用数字的专用词。然而，更复杂的版本可以包含一个句点和一个特殊字符，以反映科学记数法或特定基数的数字。

识别这些部分对其他 NLP 任务很有用。例如，要确定一个句子的边界，就必须将它拆分开来，并确定哪些元素终止了一个句子。

将文本分开的过程称为标记化。结果是令牌流。决定元素拆分位置的文本元素称为**分隔符**。对于大多数英文文本，空格被用作分隔符。这种类型的分隔符通常包括空格、制表符和换行符。

标记化可以简单也可以复杂。这里，我们将使用`String` class' `split`方法演示一个简单的标记化。首先，声明一个字符串来保存要标记的文本:

```java
String text = "Mr. Smith went to 123 Washington avenue."; 
```

`split`方法使用一个正则表达式参数来指定如何拆分文本。在下面的代码序列中，它的参数是`\\s+`字符串。这指定将使用一个或多个空格作为分隔符:

```java
String tokens[] = text.split("\\s+"); 
```

for-each 语句用于显示结果标记:

```java
for(String token : tokens) { 
  System.out.println(token); 
} 
```

执行时，输出将如下所示:

```java
Mr.
Smith
went
to
123
Washington
avenue.  
```

在第二章、*寻找部分文本*中，我们将深入探讨标记化过程。



# 寻找句子

我们倾向于认为识别句子的过程很简单。在英语中，我们寻找终止字符，如句号、问号或感叹号。然而，正如我们将在第三章、*寻找句子*中看到的，这并不总是那么简单。使寻找句末变得更加困难的因素包括在诸如*史密斯博士*或 *204 SW 这样的短语中使用嵌入式句号。公园街*。

这个过程也叫**句界消歧**(**)。这在英语中是一个比汉语或日语更严重的问题，因为汉语或日语有明确的句子分隔符。**

 **识别句子是有用的，原因有很多。一些自然语言处理任务，比如词性标注和实体提取，是针对单个句子的。问答应用程序也需要识别单个句子。为了使这些过程正确工作，必须正确确定句子边界。

下面的例子演示了如何使用 Stanford `DocumentPreprocessor`类找到句子。这个类将生成一个基于简单文本或 XML 文档的句子列表。该类实现了`Iterable`接口，允许它在 for-each 语句中轻松使用。

首先声明一个包含以下句子的字符串:

```java
String paragraph = "The first sentence. The second sentence."; 
```

基于字符串创建一个`StringReader`对象。这个类支持简单的`read`类型方法，并被用作`DocumentPreprocessor`构造函数的参数:

```java
Reader reader = new StringReader(paragraph); 
DocumentPreprocessor documentPreprocessor =  
new DocumentPreprocessor(reader); 
```

`DocumentPreprocessor`对象现在将保存段落中的句子。在下面的语句中，创建了一个字符串列表，用于保存找到的句子:

```java
List<String> sentenceList = new LinkedList<String>(); 
```

然后，`documentPreprocessor`对象的每个元素被处理，并由一系列的`HasWord`对象组成，如下面的代码块所示。`HasWord`元素是代表一个单词的对象。`StringBuilder`的一个实例被用来构造句子，其中`hasWordList`元素的每个元素都被添加到列表中。句子完成后，将被添加到`sentenceList`列表中:

```java
for (List<HasWord> element : documentPreprocessor) { 
  StringBuilder sentence = new StringBuilder(); 
  List<HasWord> hasWordList = element; 
  for (HasWord token : hasWordList) { 
      sentence.append(token).append(" "); 
  } 
  sentenceList.add(sentence.toString()); 
} 
```

然后使用 for-each 语句来显示句子:

```java
for (String sentence : sentenceList) { 
  System.out.println(sentence); 
} 
```

输出如下所示:

```java
The first sentence . 
The second sentence .   
```

SBD 过程在第三章、*的*中有详细介绍。



# 特征工程

特征工程在开发自然语言处理应用程序中起着重要的作用；这对于机器学习非常重要，尤其是在基于预测的模型中。它是使用领域知识将原始数据转换为特征的过程，以便机器学习算法能够工作。特征为我们提供了原始数据的更集中的视图。一旦识别出特征，就进行特征选择以降低数据的维度。当处理原始数据时，检测到模式或特征，但是这可能不足以增强训练数据集。工程特征通过提供有助于区分数据模式的相关信息来增强训练。在原始数据集或提取的要素中，新要素可能不会被捕获或不明显。因此，特征工程是一门艺术，需要领域专业知识。它仍然是人类的手艺，是机器还不擅长的东西。

第六章、*用特征表示文本，*将展示如何将文本文档表示为对文本文档不起作用的传统特征。



# 寻找人和事物

搜索引擎很好地满足了大多数用户的需求。人们经常使用搜索引擎来查找商业地址或电影放映时间。文字处理器可以执行简单的搜索来定位文本中的特定单词或短语。然而，当我们需要考虑其他因素时，这项任务会变得更加复杂，例如是否应该使用同义词，或者我们是否有兴趣找到与某个主题密切相关的东西。

例如，假设我们访问一个网站，因为我们有兴趣购买一台新的笔记本电脑。毕竟，谁不需要新的笔记本电脑呢？当您访问该网站时，搜索引擎将用于查找具备您正在寻找的功能的笔记本电脑。这种搜索通常是根据以前对供应商信息的分析进行的。这种分析通常需要对文本进行处理，以便获得最终可以呈现给客户的有用信息。

呈现可以是小平面的形式。这些通常显示在网页的左侧。例如，笔记本电脑的方面可能包括超极本、Chromebook 或硬盘大小等类别。下面的截图说明了这一点，它是亚马逊网页的一部分:

![](img/00005.jpeg)

有些搜索可能非常简单。例如，`String`类和相关类都有方法，比如`indexOf`和`lastIndexOf`方法，可以找到`String`类的出现。在下面的简单例子中，目标字符串出现的索引由`indexOf`方法返回:

```java
String text = "Mr. Smith went to 123 Washington avenue."; 
String target = "Washington"; 
int index = text.indexOf(target); 
System.out.println(index); 
```

此序列的输出如下所示:

```java
22
```

这种方法只对最简单的问题有用。

当搜索文本时，一种常见的技术是使用一种称为倒排索引的数据结构。这个过程包括对文本进行标记，并识别文本中感兴趣的术语及其位置。然后，术语及其位置存储在倒排索引中。当对术语进行搜索时，在倒排索引中查找术语，并检索位置信息。这比每次需要时在文档中搜索术语要快。这种数据结构经常用于数据库、信息检索系统和搜索引擎。

更复杂的搜索可能涉及对诸如“波士顿有哪些好餐馆？”为了回答这个查询，我们可能需要执行实体识别/解析来识别查询中的重要术语，执行语义分析来确定查询的含义，进行搜索，然后对候选响应进行排序。

为了说明查找姓名的过程，我们结合使用了一个标记器和 OpenNLP `TokenNameFinderModel`类来查找文本中的姓名。由于这个技术可能会抛出`IOException`，我们将使用一个`try...catch`块来处理它。声明这个块和包含句子的字符串数组，如下所示:

```java
try { 
    String[] sentences = { 
         "Tim was a good neighbor. Perhaps not as good a Bob " +  
        "Haywood, but still pretty good. Of course Mr. Adam " +  
        "took the cake!"}; 
    // Insert code to find the names here 
  } catch (IOException ex) { 
    ex.printStackTrace(); 
}
```

在对句子进行处理之前，我们需要对文本进行分词。使用`Tokenizer`类设置记号赋予器，如下所示:

```java
Tokenizer tokenizer = SimpleTokenizer.INSTANCE; 
```

我们将需要使用一个模型来检测句子。这是为了避免对可能跨越句子边界的术语进行分组。我们将基于在`en-ner-person.bin`文件中找到的模型使用`TokenNameFinderModel`类。从这个文件中创建一个`TokenNameFinderModel`的实例，如下所示:

```java
TokenNameFinderModel model = new TokenNameFinderModel( 
new File("C:\\OpenNLP Models", "en-ner-person.bin")); 
```

`NameFinderME`类将执行查找名称的实际任务。这个类的一个实例是使用`TokenNameFinderModel`实例创建的，如下所示:

```java
NameFinderME finder = new NameFinderME(model); 
```

使用 for-each 语句处理每个句子，如下面的代码序列所示。`tokenize`方法将把句子分割成记号，而`find`方法返回一组`Span`对象。这些对象存储由`find`方法标识的名称的起始和结束索引:

```java
for (String sentence : sentences) { 
    String[] tokens = tokenizer.tokenize(sentence); 
    Span[] nameSpans = finder.find(tokens); 
    System.out.println(Arrays.toString( 
    Span.spansToStrings(nameSpans, tokens))); 
} 
```

执行时，它将生成以下输出:

```java
[Tim, Bob Haywood, Adam]  
```

第四章、*寻找人和物*的主要焦点是名字识别。



# 检测词类

另一种对文本各部分进行分类的方法是在句子层面。一个句子可以根据类别(如名词、动词、副词和介词)分解成单个单词或单词组合。我们大多数人在学校里学会了如何做这件事。我们还学习了不要用介词结束一个句子，这与我们在本段第二句中所做的相反。

检测词性在其他任务中很有用，例如提取关系和确定文本的含义。确定这些关系被称为解析。POS 处理对于提高发送到管道其他元素的数据质量非常有用。

POS 流程的内部可能很复杂。幸运的是，大多数复杂性对我们来说是隐藏的，封装在类和方法中。我们将使用几个 OpenNLP 类来说明这个过程。我们需要一个模型来检测 POS。将使用在`en-pos-maxent.bin`文件中找到的模型来使用和实例化`POSModel`类，如下所示:

```java
POSModel model = new POSModelLoader().load( 
    new File("../OpenNLP Models/" "en-pos-maxent.bin")); 
```

`POSTaggerME`类用于执行实际的标记。基于以前的模型创建该类的实例，如下所示:

```java
POSTaggerME tagger = new POSTaggerME(model); 
```

接下来，声明包含要处理的文本的字符串:

```java
String sentence = "POS processing is useful for enhancing the "  
   + "quality of data sent to other elements of a pipeline."; 
```

这里，我们将使用`WhitespaceTokenizer`来标记文本:

```java
String tokens[] = WhitespaceTokenizer.INSTANCE.tokenize(sentence); 
```

然后使用`tag`方法来查找那些将结果
存储在字符串数组中的词性:

```java
String[] tags = tagger.tag(tokens); 
```

然后将显示令牌及其相应的标签:

```java
for(int i=0; i<tokens.length; i++) { 
    System.out.print(tokens[i] + "[" + tags[i] + "] "); 
} 
```

执行时，将产生以下输出:

```java
    POS[NNP] processing[NN] is[VBZ] useful[JJ] for[IN] enhancing[VBG] the[DT] quality[NN] of[IN] data[NNS] sent[VBN] to[TO] other[JJ] elements[NNS] of[IN] a[DT] pipeline.[NN]  
```

每个标记后面都有一个缩写，包含在括号内，表示它的位置。例如，NNP 表示它是专有名词。这些缩写将在第五章、*检测词类*中涉及，专门深入探讨这个话题。



# 文本和文档分类

分类涉及给文本或文档中的信息分配标签。当过程发生时，这些标签可能是已知的，也可能是未知的。当标签已知时，该过程称为**分类**。当标签未知时，该过程被称为**聚类**。

在自然语言处理中感兴趣的还有分类的过程。这是将一些文本元素分配到几个可能的组之一的过程。例如，军用飞机可以分为战斗机、轰炸机、侦察机、运输机或救援飞机。

分类器可以按照它们产生的输出类型来组织。这可以是二进制的，产生是/否输出。这种类型通常用于支持垃圾邮件过滤器。其他类型将导致多个可能的类别。

与许多其他 NLP 任务相比，分类更像是一个过程。它包括我们将在*了解 NLP 模型*部分讨论的步骤。由于这个过程的长度，我们将不在这里举例说明。在第八章、*对文本和文档进行分类*中，我们将研究分类过程并提供一个详细的例子。



# 提取关系

关系抽取识别文本中存在的关系。例如，用“生活的意义和目的显而易见”这个句子，我们知道这个句子的主题是“生活的意义和目的”它与暗示“显而易见”的最后一个短语有关。

人类可以很好地确定事物之间的关系，至少在高层次上。确定深层关系可能更加困难。使用计算机提取关系也很有挑战性。然而，计算机可以处理大型数据集，以找到对人类来说不明显或在合理的时间内无法完成的关系。

许多关系都是可能的。这些包括一些关系，比如某物的位置，两个人之间的关系，系统的组成部分，以及谁是负责人。关系提取对许多任务都很有用，包括建立知识库、进行趋势分析、收集情报和进行产品搜索。寻找关系有时被称为**文本分析**。

我们可以使用几种技术来执行关系提取。这些在第十章、*使用解析器提取关系*中有更详细的介绍。这里，我们将使用斯坦福 NLP `StanfordCoreNLP`类说明一种识别句子中关系的技术。这个类支持一个管道，在这个管道中标注器被指定并应用于文本。**注释器**可以被认为是要执行的操作。当创建类的实例时，使用在`java.util`包中找到的`Properties`对象添加注释器。

首先，创建一个`Properties`类的实例。然后，按如下方式分配注释者:

```java
Properties properties = new Properties();         
properties.put("annotators", "tokenize, ssplit, parse"); 
```

我们使用了三个注释器，它们指定了要执行的操作。在这种情况下，这些是解析文本所需的最低要求。第一个是`tokenize`，将对文本进行标记。`ssplit`注释器将标记分割成句子。最后一个注释者`parse`，执行语法分析，对文本进行解析。

接下来，使用属性的引用变量创建一个`StanfordCoreNLP`类的实例:

```java
StanfordCoreNLP pipeline = new StanfordCoreNLP(properties); 
```

然后，创建一个`Annotation`实例，它使用文本作为它的参数:

```java
Annotation annotation = new Annotation( 
    "The meaning and purpose of life is plain to see."); 
```

对`pipeline`对象应用`annotate`方法来处理`annotation`对象。最后，使用`prettyPrint`方法显示处理结果:

```java
pipeline.annotate(annotation); 
pipeline.prettyPrint(annotation, System.out); 
```

这段代码的输出如下所示:

```java
    Sentence #1 (11 tokens):
    The meaning and purpose of life is plain to see.
    [Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT] [Text=meaning CharacterOffsetBegin=4 CharacterOffsetEnd=11 PartOfSpeech=NN] [Text=and CharacterOffsetBegin=12 CharacterOffsetEnd=15 PartOfSpeech=CC] [Text=purpose CharacterOffsetBegin=16 CharacterOffsetEnd=23 PartOfSpeech=NN] [Text=of CharacterOffsetBegin=24 CharacterOffsetEnd=26 PartOfSpeech=IN] [Text=life CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=NN] [Text=is CharacterOffsetBegin=32 CharacterOffsetEnd=34 PartOfSpeech=VBZ] [Text=plain CharacterOffsetBegin=35 CharacterOffsetEnd=40 PartOfSpeech=JJ] [Text=to CharacterOffsetBegin=41 CharacterOffsetEnd=43 PartOfSpeech=TO] [Text=see CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=VB] [Text=. CharacterOffsetBegin=47 CharacterOffsetEnd=48 PartOfSpeech=.] 
    (ROOT
      (S
        (NP
          (NP (DT The) (NN meaning)
            (CC and)
            (NN purpose))
          (PP (IN of)
            (NP (NN life))))
        (VP (VBZ is)
          (ADJP (JJ plain)
            (S
              (VP (TO to)
                (VP (VB see))))))
        (. .)))

    root(ROOT-0, plain-8)
    det(meaning-2, The-1)
    nsubj(plain-8, meaning-2)
    conj_and(meaning-2, purpose-4)
    prep_of(meaning-2, life-6)
    cop(plain-8, is-7)
    aux(see-10, to-9)
    xcomp(plain-8, see-10)

```

输出的第一部分显示文本以及令牌和位置。接下来是树状结构，显示句子的组织。最后一部分显示了语法层面上的元素之间的关系。考虑下面的例子:

```java
prep_of(meaning-2, life-6)  
```

这显示了介词的*是如何被用来将表示*的*和表示*的*联系起来的。这些信息对于许多文本简化任务非常有用。*



# 使用综合方法

如前所述，NLP 问题通常涉及使用一个以上的基本 NLP 任务。这些经常在一个管道中组合以获得期望的结果。在前一节中，我们看到了管道的一个用途，*提取关系*。

大多数 NLP 解决方案将使用管道。我们将在第十一章、*组合管道*中提供几个管道的例子。



# 了解 NLP 模型

不管执行的是 NLP 任务还是使用的 NLP 工具集，它们都有几个共同的步骤。在本节中，我们将介绍这些步骤。当你阅读本书中介绍的章节和技术时，你会看到这些步骤会有细微的变化。现在对它们有一个很好的理解将会减轻学习技术的任务。

基本步骤包括以下内容:

1.  确定任务
2.  选择模型
3.  构建和训练模型
4.  验证模型
5.  使用模型

我们将在下面的小节中讨论这些步骤。



# 确定任务

理解需要解决的问题很重要。基于这种理解，可以设计出由一系列步骤组成的解决方案。这些步骤中的每一步都将使用 NLP 任务。

例如，假设我们想要回答一个查询，比如“谁是巴黎的市长？”我们需要将查询解析到 POS 中，确定问题的性质、问题的限定元素，并最终使用通过其他 NLP 任务创建的知识库来回答问题。

其他问题可能不太复杂。我们可能只需要将文本分解成组件，这样文本就可以与类别相关联。例如，可以分析供应商的产品描述来确定潜在的产品类别。对汽车描述的分析将允许它被分类为轿车、跑车、SUV 或小型车。

一旦你对 NLP 任务有了概念，你就能更好地将它们与你试图解决的问题相匹配。



# 选择模型

我们将要研究的许多任务都是基于模型的。例如，如果我们需要将一个文档拆分成句子，我们需要一个算法来完成这项工作。然而，即使是最好的句子边界检测技术也很难每次都做到正确。这导致了模型的发展，该模型检查文本的元素，然后使用该信息来确定断句发生的位置。

正确的模型可能取决于正在处理的文本的性质。在确定历史文献的句子结尾方面做得很好的模型在应用于医学文本时可能不太好。

已经创建了许多模型，我们可以用它们来完成手头的 NLP 任务。根据需要解决的问题，我们可以做出明智的决策，确定哪种模型是最好的。在某些情况下，我们可能需要训练一个新的模型。这些决策经常涉及准确性和速度之间的权衡。理解问题域和所需的结果质量使我们能够选择合适的模型。



# 构建和训练模型

训练模型是针对一组数据执行算法、制定模型，然后验证模型的过程。我们可能会遇到这样的情况，需要处理的文本与我们以前看到和使用的文本有很大不同。例如，使用新闻文本训练的模型在处理推文时可能效果不佳。这可能意味着现有的模型将无法很好地处理这些新数据。当这种情况出现时，我们将需要训练一个新的模型。

为了训练一个模型，我们通常会使用以我们知道正确答案的方式标记的数据。例如，如果我们处理的是词性标注，那么数据中会标记出词性元素(比如名词和动词)。当模型被训练时，它将使用这些信息来创建模型。这个数据集被称为**语料库**。



# 验证模型

一旦创建了模型，我们需要用一个样本集来验证它。典型的验证方法是使用已知正确响应的样本集。当模型与这些数据一起使用时，我们能够将其结果与已知的好结果进行比较，并评估模型的质量。通常，只有一部分语料库用于训练，而另一部分用于验证。



# 使用模型

使用模型只是将模型应用于手头的问题。细节取决于所使用的模型。这在之前的几个演示中有所说明，比如在*检测词性*部分，我们使用了包含在`en-pos-maxent.bin`文件中的 POS 模型。



# 准备数据

自然语言处理中的一个重要步骤是寻找和准备要处理的数据。这包括用于培训目的的数据和需要处理的数据。有几个因素需要考虑。在这里，我们将关注 Java 为处理字符提供的支持。

我们需要考虑角色是如何表现的。虽然我们将主要处理英语文本，但其他语言也存在独特的问题。不仅一个字符的编码方式不同，阅读文本的顺序也会不同。例如，日语从右向左按列排列文本。

也有许多可能的编码。这些语言包括 ASCII、拉丁语和 Unicode 等等。下表列出了更完整的列表。尤其是 Unicode，它是一种复杂而广泛的编码方案:

| **编码** | **描述** |
| 美国信息交换标准代码 | 使用 128 (0-127)个值的字符编码。 |
| 拉丁语 | 有几个拉丁变体使用 256 个值。它们包括变音符号和其他字符的各种组合。不同版本的拉丁语被用来称呼不同的印欧语言，如土耳其语和世界语。 |
| Big5 | 寻址中文字符集的双字节编码。 |
| 统一码 | Unicode 有三种编码:UTF-8、UTF-16 和 UTF-32。它们分别使用 1、2 和 4 个字节。这种编码能够代表当今存在的所有已知语言，包括较新的语言，如克林贡语和精灵语。 |

Java 能够处理这些编码方案。`javac`可执行文件的`-encoding`命令行选项用于指定要使用的编码方案。在下面的命令行中，指定了`Big5`编码方案:

```java
javac -encoding Big5
```

使用原始的`char`数据类型、`Character`类以及其他几个类和接口来支持字符处理，如下表所示:

| **字符类型** | **描述** |
| `char` | 原始数据类型。 |
| `Character` | `char`的包装类。 |
| `CharBuffer` | 这个类支持一个缓冲区`char`，为获取/放置字符或一系列字符操作提供方法。 |
| `CharSequence` | 由`CharBuffer`、`Segment`、`String`、`StringBuffer`、`StringBuilder`实现的接口。它支持对字符序列的只读访问。 |

Java 还提供了许多支持字符串的类和接口。下表对这些进行了总结。我们将在许多例子中使用这些。`String`、`StringBuffer`和`StringBuilder`类提供了相似的字符串处理能力，但是在它们是否可以被修改以及它们是否是线程安全的方面有所不同。`CharacterIterator`接口和`StringCharacterIterator`类提供了遍历字符序列的技术。

`Segment`类表示一段文本:

| **类/接口** | **描述** |
| `String` | 不可变的字符串。 |
| `StringBuffer` | 表示可修改的字符串。它是线程安全的。 |
| `StringBuilder` | 与`StringBuffer`类兼容，但
不是线程安全的。 |
| `Segment` | 表示字符数组中的一段文本。它提供了对数组中字符数据的快速访问。 |
| `CharacterIterator` | 为文本定义迭代器。它支持文本的双向遍历。 |
| `StringCharacterIterator` | 为`String`实现`CharacterIterator`接口的类。 |

如果我们从文件中读取，我们还需要考虑文件格式。通常，数据是从注释单词的来源获得的。例如，如果我们使用一个网页作为文本的来源，我们会发现它是用 HTML 标签标记的。这些不一定与分析过程相关，可能需要删除。

**多用途互联网邮件扩展** ( **MIME** )类型用于表征文件使用的格式。下表列出了常见的文件类型。要么我们需要明确地删除或改变文件中的标记，要么使用专门的软件来处理它。一些 NLP APIs 提供了处理特殊文件格式的工具:

| **文件格式** | **哑剧类型** | **描述** |
| 文本 | 纯文本/文本 | 简单文本文件 |
| 办公室类型文件 | 应用程序/MS Word 应用/ `vnd.oasis.opendocument.text` | 微软办公开放式办公室 |
| 便携文档格式 | 应用程序/PDF | Adobe 可移植文档格式 |
| 超文本标记语言 | 文本/HTML | 网页 |
| 可扩展标记语言 | 文本/XML | 可扩展标记语言 |
| 数据库ˌ资料库 | 不适用 | 数据可以有多种不同的格式 |

许多 NLP APIs 都假设数据是干净的。当它不是的时候，就需要清理，以免我们得到不可靠和误导的结果。



# 摘要

在本章中，我们介绍了 NLP 及其用途。我们发现它在许多地方被用来解决许多不同类型的问题，从简单的搜索到复杂的分类问题。介绍了 Java 在核心字符串支持和高级 NLP 库方面对 NLP 的支持。使用代码解释和说明了基本的 NLP 任务。还包括了 NLP 和特征工程中深度学习的基础，以显示深度学习如何影响 NLP。我们还研究了训练、验证和使用模型的过程。

在本书中，我们将使用简单和更复杂的方法为使用基本的 NLP 任务打下基础。您可能会发现有些问题只需要简单的方法，在这种情况下，知道如何使用简单的技术可能就足够了。在其他情况下，可能需要更复杂的技术。无论是哪种情况，您都要准备好确定需要哪种工具，并能够为任务选择合适的技术。

在下一章中，第二章，*寻找文本部分，*我们将考察标记化的过程，看看它如何被用来寻找文本部分。**