© Mark Wickham 2018 Mark WickhamPractical Java Machine Learning[https://doi.org/10.1007/978-1-4842-3951-3_4](https://doi.org/10.1007/978-1-4842-3951-3_4)

# 4.算法:机器学习的大脑

Mark Wickham<sup class="calibre28">[1](#Aff2) </sup> (1)Irving, TX, USA   Selecting the best algorithm for your ML problem is extremely important. This chapter will explore algorithms and meet the following objectives:

*   解释创造 ML 算法的科学家使用的术语。

*   向您展示如何通过考虑多种因素来选择最佳算法。

*   总结算法的三种高级风格。

*   提供一个完整的 CML 算法索引列表，这样你就可以很容易地识别出一个特定的算法使用哪种风格的 ML。

*   给出一个决策流程图和一个功能流程图，帮助你为你的问题选择最佳算法。

*   概述七种最重要的最大似然算法。

*   比较 CML 算法的性能，包括 CML 和 DL 算法在 MNIST 数据集上的总结。

*   回顾流行算法的 Java 源代码。

## 4.1 导言

When asked why they fail to deploy ML solutions, developers often express two main reasons:

*   这需要太多的体力劳动。

*   算法和模型创建太复杂。

手动工作通常指的是数据争论。在第 [2](2.html) 章中，我介绍了一些有助于简化这一过程的工具。第 5 章[将介绍作为 Java ML 环境的一部分集成的附加工具。使用 ML，很难避免手工处理数据。我再次引用 Silver 先生的一句有趣的话，即在我们对数据有更多期望之前，先对自己有更多期望。](5.html)

在图 [1-20](1.html#Fig20) 中，你看到算法植根于科学领域。开发人员回避部署 ML 的主要原因之一是算法选择和模型创建过于复杂。幸运的是，通过学习一些基本原理并理解与 ML 算法相关的科学语言，您可以克服算法复杂性问题。

这本书和这一章将主要涵盖 CML 算法。在第 [3](3.html) 章中，您通过 Google Cloud Speech API 访问了 DL 算法，因为云提供商倾向于将他们的 ML APIs 集中在 DL 解决方案上。

### ML-门 3

MLG3 是生成 ML 模型的阶段。这个阶段最重要的行动是选择和验证最大似然算法。本章将帮助你选择最佳算法，并确定它的执行情况。

When you first embark on ML solutions, choosing the best algorithm seems somewhat arbitrary. In Chapter [5](5.html), you will see that it is actually quite easy to randomly select and apply any algorithm to a dataset. This is not a particularly good use of time. In Chapter [3](3.html), you saw disappointing results when the AWS Machine Learning wizard chose the wrong algorithm after misunderstanding the label data type. There is a conventional wisdom for algorithm selection. Answers to the following questions help to determine which algorithm is best suited for your model:

*   你有多少数据？

*   你想预测什么？

*   数据是有标签的还是无标签的？

*   您需要增量培训还是批量培训？

随着经验的积累，您可以快速确定哪种算法最适合您的问题和数据。

## 4.2 算法风格

ML 算法的世界分为两个同等重要和有用的类别。在介绍科学家用来描述每个类别的花哨术语之前，让我们先看看定义每个类别的数据类型。

### 标记数据与未标记数据

你会记得在第 [2](2.html) 章中，我将术语 ***标签*** 定义为你试图预测或预报的东西。在第 [2](2.html) 章中，标记了 PAMAP2_Dataset。第 1 列包含标签值。在收集数据时，佩戴传感器的参与者被要求记录他们的活动。该标签值随后与来自传感器的所有其他数据一起被存储。

ML 中使用的许多数据集由带标签的数据组成。加州大学欧文分校存储库中的大多数数据集都有标签。大部分由 ML 网站**主办的 ML 比赛都使用标签化数据。在现实世界中，情况并非总是如此。**

 **一些组织认为有标签的数据比无标签的数据更有价值。组织有时甚至认为未标记的数据毫无价值。这大概是短视的。您将看到 ML 可以使用有标签和无标签的数据。

Whether or not the data contains labels is the key factor in determining the ML algorithm style. ML algorithms fall into three general ML styles:

*   监督学习

*   无监督学习

*   半监督学习

Figure [4-1](#Fig1) summarizes these three ML algorithm styles.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig1_HTML.jpg](Images/468661_1_En_4_Fig1_HTML.jpg) Figure 4-1

ML 算法样式

我将讨论的所有算法都属于这些类别之一。监督算法使用带有标签的数据。无监督算法使用没有标签的数据。半监督算法使用带标签和不带标签的数据。

## 4.3 监督学习

Supervised learning is the easiest ML learning style to understand. Supervised learning algorithms operate on data with labels. Because each sample includes a label, a function (which we will call a critic) is able to calculate an error value for each sample. Figure [4-2](#Fig2) shows a graphical representation of the supervised learning process.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig2_HTML.jpg](Images/468661_1_En_4_Fig2_HTML.jpg) Figure 4-2

监督学习逻辑

The term “supervision” refers to the critic-error operation. This operation enables the algorithm to compare actual vs. desired and learn from this. There are many algorithms under the supervised learning umbrella. Later in this chapter, you will explore some of the most useful supervised algorithms:

*   支持向量机(SVM)

*   朴素贝叶斯网络

*   随机森林决策树

当您将监督学习与标记数据相结合时，您就能够对样本进行分类。术语“监督学习”和“分类”因此是高度相关的。

监督学习分类通常发生在两个阶段。您将数据分为两部分:训练数据和测试数据。两组数据都包含完全标记的样本。在第一阶段，使用训练数据训练映射函数，直到它满足某种性能水平(实际输出与期望输出)。在第二阶段，您使用测试数据作为映射函数的输入。第二阶段提供了一个很好的方法来衡量模型在处理不可见数据时的表现。

## 4.4 无监督学习

Unsupervised learning algorithms operate on data without labels. The key difference is that unsupervised learning lacks a critic and has no way to measure its performance. Figure [4-3](#Fig3) shows the representation of an unsupervised learning system.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig3_HTML.jpg](Images/468661_1_En_4_Fig3_HTML.jpg) Figure 4-3

无监督学习逻辑

在该系统中，您没有必要的标签来对单个样本进行分类。相反，您接受没有标签的数据作为输入，并执行输出类的任务。无监督学习就是寻找数据的结构，这项任务通常被称为聚类或知识发现。

一个经常被引用来帮助解释集群的常见例子是未知的 Excel 电子表格。考虑一个包含数据列但没有标识数据的标题的电子表格。你能确定什么？您需要分析每一列中包含的数据，并试图发现其含义。您实际上是在试图揭示电子表格中数据的结构。如果一个新的样本到达，您想决定如何将它连接到电子表格中的其他数据。这就是聚类。

聚类算法涉及确定映射函数，该函数根据隐藏在数据中的特征将数据分类。对于无监督学习，因为没有标签，所以你缺乏知识来知道你希望从模型中得到什么。相反，您在数据中寻找关系或相关性。

Unsupervised learning algorithms work their magic by dividing the data into clusters. In Figure [2-10](2.html#Fig10), you saw an interesting dataset that contained erroneous value. If you pass this dataset into a clustering algorithm, you might possibly obtain a result like that shown in Figure [4-4](#Fig4).![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig4_HTML.jpg](Images/468661_1_En_4_Fig4_HTML.jpg) Figure 4-4

数据集的聚类

Visualization is very helpful in understanding clusters. Obviously, Cluster 2 represents the erroneous data in the dataset. Algorithms have many different techniques to determine what constitutes a cluster. Later in this chapter, you will explore the pros and cons of the following clustering algorithms:

*   DBSCAN

*   期望最大化

*   k 均值聚类

在第 [5](5.html) 章，你将用 Java 和 Weka 实现集群。

## 4.5 半监督学习

Semi-supervised learning algorithms operate on mixed data where only some of the data contains labels. This is often the case for real-world data. Semi-supervised learning is becoming increasingly popular for two reasons:

*   数据爆炸的大趋势导致了越来越多的非结构化数据的收集，这些数据没有一致的标签应用。

*   一种可能的选择是标记混合数据集中所有未标记的数据，然后使用监督学习算法。然而，由于标记过程是手动且繁琐的，因此用缺失的标签标记样品的成本太高。

Figure [4-5](#Fig5) shows the process diagram for semi-supervised learning. The input and outputs are the same as with the supervised learning style, with the exception of a switch placed before the critic. The switch allows the critic function to be disabled when a data sample is unlabeled.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig5_HTML.jpg](Images/468661_1_En_4_Fig5_HTML.jpg) Figure 4-5

半监督学习逻辑

与监督和非监督学习风格一样，半监督学习有许多算法。在第 [5](5.html) 章，你将使用一组集体分类算法实现半监督学习。越来越多的学术研究表明，半监督算法可以优于监督算法。然而，使用半监督算法的主要优势仍然是节省时间，因为不需要标记未标记的数据。

## 4.6 另类学习风格

回归、深度学习和强化学习是具有独特算法的学习风格，我不会在本章中介绍，但有一个例外:支持向量机算法，它可以很好地进行监督学习分类。

### 线性回归算法

Regression is useful for predicting outputs that are continuous, rather than outputs confined to a set of labels. Linear regression is the most popular regression algorithm. All of the cloud and desktop ML environments support the linear regression algorithm. It is one of the more simple algorithms. If you recall back to the statistics class you suffered through, linear regression is the process of finding the best fit line through a series of data points. Figure [4-6](#Fig6) shows a linear regression example.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig6_HTML.jpg](Images/468661_1_En_4_Fig6_HTML.jpg) Figure 4-6

线性回归

回归线使与每个数据点相关的误差值最小化。生成的最佳拟合线可以成功地生成或“预测”沿线的任何独立变量值。

回想一下在第 [2](2.html) 章中，您将 AWS ML 回归算法应用于 PAMAP2_Dataset。结果很差，因为该数据集与回归算法不匹配。在这个例子中，AWS ML 向导错误地认为您的目标标签是一个连续值，而不是一个整数标签。稍后，您将看到如何为该数据集选择更好的算法。

在本文的其余部分，您将主要关注用于监督和非监督学习的 CML 算法，但如果您有一个需要连续值预测而不是离散标签的简单问题，请记住线性回归算法。

### 深度学习算法

DL 风格依赖于具有隐藏层的神经网络。在 DL 中有几个流行的算法家族，包括卷积神经网络(CNN)和递归神经网络(RNN)。DL 算法最好的总结资源之一是“深度学习算法最完整指南”，可从[*【https://asimovinstitute.org】*](https://asimovinstitute.org)的**获得。**

Figure [4-7](#Fig7) displays a simplified summary of the cells and hidden layers that comprise some of the most popular neural networks.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig7_HTML.jpg](Images/468661_1_En_4_Fig7_HTML.jpg) Figure 4-7

DL 算法总结

神经网络有许多类型的隐藏层。请参考链接来区分不同的隐藏图层类型，因为很难在灰度图像中可视化图层类型。

图的左下角 [4-7](#Fig7) 是支持向量机 DL 算法。SVM DL 算法是一个监督的 ML 算法，你也可以应用于 CML。你将在本章的后半部分以及第五章[中仔细观察这个算法的性能。](5.html)

### 强化学习

半监督学习有时会与强化学习(RL)风格相混淆。他们不一样。RL 是一种有区别的监督学习。对于 RL，每个输入并不总是产生反馈。半监督学习使用带有混合标签的数据，而 RL 则没有标签。

在 RL 中，监督来自一个奖励信号，它告诉批评家它做得有多好，但没有说正确的行动应该是什么。强化学习处理批评家与其环境(状态)的相互作用。批评家所采取的行动影响着它在未来将观察到的状态的分布。在监督学习中，每个决定都是独立的。在 RL 中，标签与序列相关联，这与监督学习中的单个样本相反。

回想一下第 [1](1.html) 章，扑克问题很难解决，因为扑克是一种不确定或不完整信息的游戏。RL 适用于导航不确定的环境，因此经常用于扑克、象棋、二十一点或围棋等游戏。

前面我提到了 Skymind，它是基于 Java 的 DL 库的创建者。Skymind 在 RL 上也有一些很棒的内容。它将 RL 描述为面向目标的 ML 方法。您可以通过以下链接了解更多关于 RL 的信息:

[*【https://skymind.ai/wiki/deep-reinforcement-learning】*](https://skymind.ai/wiki/deep-reinforcement-learning)

在本章和本书的其余部分，您将把重点限制在有标签或无标签的监督和无监督算法上，因为它们与您对 CML 问题的关注有很好的重叠。

## 4.7 CML 算法概述

With an understanding of the major algorithm styles, supervised, unsupervised, and semi-supervised, and their relation to labeled vs. unlabeled data, it is now time to look at the specific algorithms within these styles. Table [4-1](#Tab1) provides an index of most of the popular supervised CML algorithms. The shaded cells indicate the key algorithms you will explore.Table 4-1

监督最大似然算法

<colgroup class="calibre12"><col class="tcol"> <col class="tcol"></colgroup> 
| 

监督最大似然算法

 | 

家庭/阶级

 |
| --- | --- |
| 平均一元估计量(AODE) | 优于贝叶斯 |
| 方差分析(ANOVA) | 统计的 |
| 人工神经网络 | 神经网络 |
| Apriori 算法 | 关联学习(数据库) |
| 朴素贝叶斯 | 贝叶斯(概率) |
| 贝叶斯统计 | 贝叶斯(概率) |
| 助推 | 集成学习 |
| 条件随机场 | 统计的 |
| C45 | 决策图表 |
| 手推车 | 决策图表 |
| 随机森林 | 决策图表 |
| 雪橇亲卫队 | 决策图表 |
| 冲刺 | 决策图表 |
| Eclat 算法 | 联想学习 |
| 分类器集成 | 集成学习 |
| 信息模糊网络 | 决策树，但带有有向图 |
| 隐马尔可夫模型 | 统计，马尔可夫过程 |
| k-最近邻(KNN) | 基于实例(懒惰学习) |
| 学习自动机 | 加强 |
| 学习矢量量化(LVQ) | 神经网络 |
| 逻辑模型树 | 结合回归和决策树 |
| 最小消息长度 | 基于实例的 |
| 可能近似正确的学习 | 统计的 |
| 二次分类 | 线性分类器 |
| 支持向量机 | 非概率线性分类器 |

Table [4-2](#Tab2) provides an index of most of the popular unsupervised ML algorithms.Table 4-2

无监督 ML 算法

<colgroup class="calibre12"><col class="tcol"> <col class="tcol"></colgroup> 
| 

无监督 ML 算法

 | 

家庭/阶级

 |
| --- | --- |
| 蜘蛛网 | 概念聚类 |
| 概念聚类 | 数据分组 |
| DBSCAN | 基于密度的聚类 |
| 期望最大化 | 迭代法 |
| FP-growth 算法(频繁模式) | 递归树 |
| 模糊聚类 | 类似于 k-means |
| 生成地形图 | 盖然论的 |
| 工具软件 | 基于密度的聚类 |
| 信息瓶颈 | 深度学习 |
| k-均值算法 | 矢量量化，类似于 KNN |
| 局部异常因素 | 异常检测 |
| 光学算法 | 基于密度的聚类 |
| 自组织映射(ANN) | 神经网络 |
| 单连锁聚类 | 分层聚类 |
| 支持向量聚类 | 矢量量化，类似于 SVM |
| 矢量量化 | 矢量量化 |

Lastly, Table [4-3](#Tab3) shows a list of the semi-supervised algorithms.Table 4-3

半监督 CML 算法

<colgroup class="calibre12"><col class="tcol"> <col class="tcol"></colgroup> 
| 

半监督最大似然算法

 | 

家庭/阶级

 |
| --- | --- |
| 合作培训 | 大量未标记的数据 |
| 集体分类 | 新西兰黑秧鸡 |
| 生成模型 | 基于图形的 |
| 基于图形的方法 | 基于图形的 |
| 撒尔沙(状态-行动-奖励-状态-行动) | 加强 |
| 暂时的 | 加强 |

These tables are a nearly complete list of CML algorithms. It is not necessary to understand all of these algorithms to write successful ML applications. If you see a particular algorithm referenced in an ML solution, you can use the tables to identify the class or family of algorithms and the learning style. Wikipedia has decent reference pages for all of the algorithms if you wish to learn more about one of them.

> *CML 算法是一种商品。广泛实现了算法创新和性能提升。然而，CML 算法更喜欢特定类型的问题，所以在选择算法时要考虑算法偏好偏差。*

While knowing the details of all the CML algorithms is not necessary, it is necessary to understand the following:

*   你有什么类型的数据？

*   哪种学习方式最适合你的数据？

*   每个 ML 算法的偏好偏差是多少？

接下来，您将探索选择最佳算法的过程。

## 4.8 选择正确的算法

There is a popular saying among data scientists...

> *算法容易；难的是数据。*

The part about data being hard was a theme of Chapter [2](2.html). At first glance, CML algorithms do not appear to be easy. Unlike DL algorithms, which are still undergoing significant development, CML algorithms are widely deployed and relatively stable. This allows us to define a decision flowchart for choosing the right CML algorithm. Figure [4-8](#Fig8) shows the algorithm decision process.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig8_HTML.jpg](Images/468661_1_En_4_Fig8_HTML.jpg) Figure 4-8

ML 算法决策流程图

该决策图是 Scikit-learn 资源页面上一个更复杂图表的修改版。如果你仔细观察图 [4-8](#Fig8) ，你会注意到简单的决策引导你进入三个主要学习风格的盒子之一:分类、聚类和回归。每个方框都突出了你需要知道的关键算法。

在浏览流程图时，决策节点取决于数据量和数据类型。在某些情况下，你会发现你可以使用不止一种算法。一般的经验法则是从简单的开始，首先运行基本的算法。

即使表 [4-1](#Tab1) ，表 [4-2](#Tab2) ，表 [4-3](#Tab3) 包含了很多算法，你真的只需要考虑图 [4-8](#Fig8) 所示的算法。在这一章的后面，你将会更仔细地研究这些特定的算法。

### 功能算法决策过程

有时候，ML 从业者采取一种更加功能化的方法来选择算法。当云平台希望将用户从与选择算法所需的数据类型决策相关的复杂性中移除时，它们使用这种方法。微软 Azure ML 在使用这种方法帮助用户选择正确的算法方面做得特别好。

The idea is to ask yourself the simple question, “What do I want to find out?” The answer to the question will lead you to the correct learning style and then to specific algorithms. Figure [4-9](#Fig9) show a summary of this approach for each of five distinct answers to the question, including

*   预测值

*   发现结构

*   在几个类别之间预测

*   查找不寻常的事件

*   在两个类别之间预测

Figure [4-9](#Fig9) shows examples and algorithms for each of the five categories. Some users appreciate this approach to algorithm selection, because it is simpler. “Discover structure” is an easier concept to understand than “clustering.”![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig9_HTML.jpg](Images/468661_1_En_4_Fig9_HTML.jpg) Figure 4-9

功能算法决策流程图

以这种方式组织 ML 算法表明，相同的算法可以回答几个不同的问题。区别在于输出类的数量。例如，请注意决策树或随机森林算法的情况。您可以将这些算法作为多类算法应用于几个类别之间的预测，作为 2 类算法应用于两个类别之间的预测，或者作为回归算法应用于值的预测。

图 [4-8](#Fig8) 中的数据驱动决策流程图和图 [4-9](#Fig9) 中的功能方法都将引导您针对您的问题选择相同的正确学习方式和算法。

接下来，您将进一步了解解决 CML 问题所需的关键算法。

## 4.9 七种最有用的 CML 算法

图 [4-8](#Fig8) 和图 [4-9](#Fig9) 中的算法决策图指导您选择最佳的 ML 算法。有了经验，你会发现屈指可数的算法就能解决你的大部分问题。本节将涵盖你工具箱中需要的七种最有用的 CML 算法。

The following seven algorithms are the “go-to” algorithms for CML problems. The list includes four classifiers and three clustering algorithms.

*   朴素贝叶斯(分类)

*   随机森林(分类)

*   k-最近邻(分类)

*   支持向量机(分类器)

*   数据库扫描(集群)

*   期望值最大化(聚类)

*   k 均值(聚类)

当然，当你需要一个模糊的算法时，特殊的情况就会出现，但是 95%的情况下，这七个算法都会产生很好的结果。最棒的是，所有算法都有开源的 Java 产品代码。

### 朴素贝叶斯算法

NB 是一种基于贝叶斯定理的概率建模算法。这本书的目标之一是避免数学方程式。因为它的根源是概率，NB 算法代表了一种保证使用数学方程的情况，但是我们将避免这种诱惑。

Bayes’ theorem simply states the following:

*   事件的概率是基于可能与事件相关的条件的先验知识。贝叶斯定理讨论条件概率。条件概率是假设条件 B 为真，事件 A 发生的可能性。

例如，考虑人的视力及其与人的年龄的关系。根据贝叶斯定理，与在不知道一个人年龄的情况下做出的评估相比，年龄可以帮助更准确地评估一个人戴眼镜的概率。在这个例子中，人的年龄是条件。

名称中“幼稚”部分的原因是算法对属性的独立性做了一个非常“幼稚”的假设。NB 算法假设给定类，所有属性都是条件独立的。即使有这种假设，NB 算法也经常胜过使用更复杂技术的分类器。

Some advantages of NB algorithms include

*   NB 适用于垃圾邮件检测，其中分类返回一个类别，如垃圾邮件或非垃圾邮件。

*   NB 可以接受分类和连续数据类型。

*   NB 可以通过在估计概率时忽略数据集中的缺失值来处理它们。

*   NB 对于有噪声的数据也是有效的，因为噪声通过使用概率来平均。

*   NB 是高度可伸缩的，它特别适合大型数据库。

*   NB 可以适应大多数分类。NB 是文档分类、垃圾邮件过滤和欺诈检测的优秀算法选择。

*   NB 有利于增量更新。

*   NB 提供了有效的内存使用和快速的训练速度。该算法适合并行处理。

Disadvantages of NB include

*   当数据属性具有某种程度的相关性时，NB 算法不能很好地工作。这违背了算法的“幼稚”假设。

您将在第 5 章中使用朴素贝叶斯实现文档分类。

### 随机森林算法

如果我只能为我的 ML 工具箱选择一种算法，我会选择随机森林算法。

要理解 RF，首先要理解决策树。决策树是一种用于分类的监督学习方法。决策树算法使用训练数据集来生成树。决策树可以对测试数据集中的实例进行分类。决策树是一种分而治之的学习方法。

决策树是一种结构，其中“内部节点”代表数据集中的每个属性。树的每个“分支”代表一个测试的结果，树底部的“叶节点”代表所做的分类。

The test can take on a variety of forms, including

*   将属性值与常数进行比较。

*   如果属性是名义属性，那么子属性的数量通常代表匹配的类别。

*   如果属性是数字，子属性可以表示“>”或“

CART(分类和回归树)算法是最基本的决策树算法之一。CART 使用有两个输出的二叉树。C45 是一种改进的算法，可以处理缺失值，并具有修剪功能来帮助解决过度拟合问题。通过决策树，您可以对离散值目标使用分类树，对连续值目标使用回归树。

The RF algorithm is an improvement over the basic decision tree algorithms such as CART and C45\. RF is an ensemble model because it uses multiple decision trees and bases each decision tree on a random subset of attributes (columns) and observations (rows) from the original data. Figure [4-10](#Fig10) shows a graphical representation of how the RF algorithm classifies an instance.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig10_HTML.jpg](Images/468661_1_En_4_Fig10_HTML.jpg) Figure 4-10

随机森林算法

许多树组成随机森林，多数投票决定最终分类。

The RF algorithm has several advantages:

*   RF 易于可视化，因此您可以了解导致分类结果的因素。如果您必须向业务领域专家或用户解释您的算法是如何工作的，这将非常有用。

*   随机森林中的每棵树都在随机特征上生长结构，从而最小化偏差。

*   与朴素贝叶斯算法不同，基于决策树的算法在属性具有某种相关性时工作良好。

*   RF 是最简单、最稳定、最容易理解的算法之一。

*   RF bagging 功能非常有用。它提供了牢固的贴合性，通常不会过度贴合。

*   RF 高度可扩展，并提供合理的性能。

RF has some disadvantages:

*   当决策树很复杂时，由于训练时间很长，可能会很慢。

*   缺失值会给基于决策树的算法带来问题。

*   属性排序很重要，这样那些“信息增益”最大的属性会首先出现。

RF 算法是对朴素贝叶斯算法的很好补充。射频变得流行的一个主要原因是因为它非常容易得到好的结果。你将在本章的后面看到，RF 算法通常优于所有其他 CML 分类器算法。

### k 近邻算法(KNN)

k-最近邻算法是一种产生良好结果的简单算法。KNN 对于分类和回归是有用的。回想一下表 [4-1](#Tab1) ，KNN 算法是一种基于实例的算法，一种学习，也称为懒惰学习。原因是这项工作是在您准备对新实例进行分类时完成的，而不是在处理定型集时完成的。基于实例的学习因此变得“懒惰”。KNN 算法不会对底层数据做出任何假设，也不会从训练数据中构建模型。

因为 KNN 算法依赖于距离计算，所以它可以很好地处理数值属性，但是 KNN 也可以通过转换为数值或二进制值来支持分类属性。

KNN algorithms classify each new instance based on the classification of its nearby neighbor(s). Figure [4-11](#Fig11) shows a graphical representation of a KNN algorithm with K=5.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig11_HTML.jpg](Images/468661_1_En_4_Fig11_HTML.jpg) Figure 4-11

KNN 算法(k=5)

图 [4-11](#Fig11) 显示了三个不同类别的二维数据。该算法从它试图分类的实例向外扫描。该算法维护一个距离数组，该数组包含从未分类实例到每个已分类邻居的距离。因为 K=5，所以只有五个最近的邻居被考虑用于确定类别的多数投票。在本例中，计数为 3 的情况对未知实例进行了分类，如下所示。

The trick with KNN is determining the best value for K. If you pick a K that is too large, perhaps even equal to the total number of observations, then your classification result will simply be the most populous class. If K is too small, the result will simply be the class of the closest neighbor. There are many approaches to choosing K:

*   进行猜测，并通过反复试验来完善。

*   选择与类别数相关的 K 值，例如，类别数+ 1。

*   用另一种算法选 k。

KNN advantages:

*   KNN 对基础数据不做任何假设。

*   KNN 是一个简单的分类器，可以很好地处理基本的识别问题。

*   KNN 很容易想象和理解分类是如何确定的。

*   与朴素贝叶斯不同，KNN 对相关属性没有问题，如果数据集不大，它也能很好地处理有噪声的数据。

KNN disadvantages:

*   选择 K 可能会有问题，您可能需要花时间调整 K 值。

*   由于依赖于基于距离的测量，KNN 受到维数灾难的影响。为了帮助解决这个问题，您可以尝试在建模之前减少尺寸或执行特征选择。

*   KNN 是基于实例的，处理整个数据集进行分类，这是资源密集型的。对于大型数据集，KNN 不是一个很好的算法选择。

*   将分类值转换为数值并不总能产生好的结果。

*   作为一个懒惰的分类器，KNN 不是实时分类的好算法选择。

KNN 是一个简单而有用的分类器。在最初的分类尝试中考虑它，特别是如果上面列出的缺点对您的问题来说不是问题的话。

### 支持向量机算法(SVM)

支持向量机算法在你的七种算法工具箱中占据了一个有用的位置。SVM 在技术上是一个线性分类器，但有一种方法也允许它处理复杂的非线性数据。

对于其输入，SVM 仅对数字要素有效，但该算法的大多数实现都允许您将分类要素转换为数值。SVM 输出是一个类别预测。

The SVM works its magic similar to the linear regression algorithm discussed earlier. Figure [4-12](#Fig12) shows the SVM concept.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig12_HTML.jpg](Images/468661_1_En_4_Fig12_HTML.jpg) Figure 4-12

支持向量机

图 [4-12](#Fig12) 中虚线上的点是支持向量。该算法试图通过最大化支持向量之间的间隔来创建类之间的最优超平面决策边界。

The SVM algorithm has several advantages:

*   在构建模型时，支持向量机需要设置的参数较少。

*   SVM 算法有很好的理论基础。

*   支持向量机在支持的数据类型上非常灵活。

*   与决策树相比，支持向量机需要更少的计算资源来获得精确的模型。

*   支持向量机对噪声数据不敏感。

*   对于二进制两类输出，SVM 是一个很好的算法。

*   通过使用核变换，可以用支持向量机完成非线性分类。

*   支持向量机可以很好地处理大量的特征和较少的训练数据。

Disadvantages of SVM algorithms:

*   支持向量机是黑盒。与决策树不同，很难解释或说明幕后发生了什么。

*   支持向量机会消耗大量内存。它们被认为是 O( *n* <sup class="calibre28">2</sup> )和 O( *n* <sup class="calibre28">3</sup> )，这意味着它们随着实例的数量呈指数级增长，因此会产生可伸缩性问题。

*   支持向量机通常适用于二元分类(两类)，但不适用于多类分类。

你并不总是有如图 [4-12](#Fig12) 所示的线性数据。幸运的是，您可以通过使用核变换(也称为“核技巧”)用 SVM 对非线性数据进行分类使用核技巧，支持向量机可以通过将输入映射到高维特征空间来有效地分类非线性数据。

为了理解二维到三维的转换，考虑下面的例子。一套由便士和一角硬币组成的硬币随机散落在桌子上。它们肯定会以非线性模式着陆，这样就没有线可以把它们分成两个不同的类别。想象一下，如果你把所有的一角硬币举起(变换)到桌子上方几英寸的地方。你可以这样做，因为这是监督学习，你已经标记了数据。经过这种转换后，您现在可以轻松地用一个平面来分隔这些类。这是本质上的核心技巧，也是支持向量机如此流行的原因。

Spoiler alert: In the next section, you will discover that SVMs can perform almost as well as neural networks on the MNIST image classification problem. Because of such excellent performance, SVMs are becoming increasingly popular and many people are starting to ask if they are an alternative to neural networks. SVMs cannot match the performance of deep networks, but they do have some advantages worth mentioning:

*   支持向量机比神经网络更不容易过度拟合。

*   支持向量机比神经网络需要更少的内存来存储预测模型。

*   支持向量机产生一个更可读的结果，因为他们有一个几何解释。

你将在第 5 章中进一步探索 SVM。

### k-均值算法

Clustering is the main task of explorative ML, and when seeking a clustering algorithm , k-means is the usual starting point. It works well for many datasets. Figure [4-13](#Fig13) shows a simplified view of how the algorithm works.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig13_HTML.jpg](Images/468661_1_En_4_Fig13_HTML.jpg) Figure 4-13

k-均值聚类算法

如图所示，k-means 算法是迭代的。该算法试图将 N 个观察值划分成 K 个聚类。您必须从集群的数量开始。k-means 算法的主要缺点是需要预先知道有多少个聚类。在所示的例子中，K=3。然后，该算法随机选择三个初始“平均值”,并通过将每个观察值分配到最近的平均值来创建初始聚类。每个聚类的质心成为新的平均值，并且重复该过程，直到实现收敛。

K-means 自 1967 年以来一直存在，它是用于聚类问题的最简单的无监督算法之一。K-means 表现也比较好。

除了必须事先知道星团的数量，k-means 还面临另一个缺点:它对非球状星团不太适用。k-means 算法倾向于找到具有相同可比形状的聚类。幸运的是，额外的聚类算法可以处理 k-means 的弱点。

### DBSCAN 算法

DBSCAN stands for density-based spatial clustering of applications with noise. The easiest way to understand density-based clustering is to look at Figure [4-14](#Fig14), a reproduction of the original clusters shown from Ester et al. Ester introduced the DBSCAN algorithm in 1996\. The graphic shows three database examples. Each of the examples contains four easily visible clusters. The clusters are non-globular, inconsistent shapes. If you were to run a k-means clustering algorithm on the datasets shown in Figure [4-14](#Fig14), the k-means algorithm would fail miserably. A density-based algorithm like DBSCAN is required to cluster such datasets.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig14_HTML.jpg](Images/468661_1_En_4_Fig14_HTML.jpg) Figure 4-14

DBSCAN 集群示例

显示的三幅图像突出了基于密度的算法的优势。在数据库 1 中，与三个周围聚类相比，较大的中心聚类的相对聚类大小对于 k-means 来说是有问题的。在数据库 2 中，围绕两个较小聚类的 S 形聚类会给 k-means 带来麻烦。在数据库 3 中，散布在整个区域的随机噪声点对于 k-means 算法来说是有问题的。

The DBSCAN algorithm employs an approach not unlike human intuition to identify clusters and noise. To accomplish this, DBSCAN requires two important parameters:

*   MinPts:数据集中的维数。该值必须至少为 3。

*   e:ε是欧氏距离。较小的值更好。如果 e 太小，很大一部分数据就不会聚类。如果 e 太大，集群将会合并。选择一个好的 e 值是 DBSCAN 成功的关键。

DBSCAN is one of the most common clustering algorithms and its advantages include

*   DBSCAN 不需要预先知道簇的数量。

*   DBSCAN 可以找到任何形状的集群。

*   DBSCAN 可以发现异常值。

*   DBSCAN 可以识别噪声。

*   DBSCAN 只需要两个参数。

*   数据集的顺序无关紧要。

The key disadvantages of DBSCAN include

*   DBSCAN 的质量取决于 e 值。对于高维数据，可能很难为 e 找到一个好的值。这就是所谓的“维数灾难”。如果数据和规模不太了解，很难选择 e。

*   DBSCAN 不能很好地聚类密度差异较大的数据集。

请注意，光学算法是 DBSCAN 的分层版本。HDBSCAN 算法是 optics 算法的更快版本。

### 期望最大化算法

当 k-means 不能达到理想的结果时，考虑 EM 算法。EM 通常能为真实世界的数据集提供出色的结果，尤其是当您感兴趣的区域很小时。

EM 是一种迭代算法，在模型依赖于未观察到的潜在变量时效果很好。该算法在两个步骤之间迭代:期望(E)和最大化(M)。在期望步骤(E)中，为可能性的期望创建一个函数。在最大化步骤(M)中，创建参数以最大化 E 步骤中的预期可能性。

EM 算法背后的理论很难理解。使用 EM 聚类，您是在概率上将候选项分配给聚类。EM 算法往往运行相对较慢，因为它需要计算大量的协方差和均值。

你将在下一章探索如何在真实世界的数据上实现 EM 算法。

## 4.10 算法性能

无论是分类还是聚类，算法预测准确性都是衡量所选算法性能的关键。你所要求的准确度是相对于你要解决的问题而言的。如果您正在构建一个 ML 模型来确定打高尔夫球的最佳日期，90%的置信度是可以接受的。如果你试图确定一张皮肤斑点的照片是否是癌性的，或者一块土地是否埋有地雷，90%是不可接受的。

深度学习的兴起在某种程度上是不断增加的对算法准确性的搜索的结果。CML 算法的用户寻求更高程度的准确性，这导致他们使用 DL 算法。

MNIST 数据库是 ML 中最流行的“Hello World”应用程序之一。MNIST 是一个大型手写数字数据库，用于训练图像识别中的神经网络和 CML 算法。因为图像识别非常适合 DL 算法，所以不需要实现 MNIST。然而，由于 MNIST 已经存在了很长时间，你可以深入了解算法的性能。

### MNIST 算法评估

MNIST is the abbreviation for Mixed National Institute of Standards and Technology. The MNIST database consists of 60,000 handwritten digits. Figure [4-15](#Fig15) shows what these images look like.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig15_HTML.jpg](Images/468661_1_En_4_Fig15_HTML.jpg) Figure 4-15

MNIST 示例图像

Characteristics of the MNIST image database:

*   60，000 次训练

*   10，000 个测试实例

*   每个图像尺寸为 28x28 像素

*   所有图像都是灰度的

除了图像文件，MNIST 数据库还包括每个图像的标签。因为 MNIST 数据集包含标签，MNIST 是一个分类 ML 问题。

Figure [4-16](#Fig16) shows a visualization of the MNIST dataset. You can distinguish the 10 unique digits. Digits that have similar appearance appear in close proximity. For example, x and y appear together at the top left. Digits 1 and 2 are also similar and appear in close proximity to each other at the center of the visualization.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig16_HTML.jpg](Images/468661_1_En_4_Fig16_HTML.jpg) Figure 4-16

MNIST 可视化

有大量关于 MNIST 数据库的学术研究。您可以利用结果来帮助您理解算法。因为您已经标记了数据，所以您可以了解监督学习分类器如何相互堆叠。

为了在 MNIST 数据库上评估 ML 模型，有必要用 60，000 个训练实例来训练该模型。评估使用额外的 10，000 个测试实例。因为 MNIST 是如此受欢迎的数据集，许多模型和算法已经解决了 MNIST。您可以使用这项工作的结果来获得一些关于相对算法性能的见解。

Table [4-4](#Tab4) shows a summary of the results for many of the popular classifiers on MNIST. The table summarizes MNIST results from the following references:

*   ***https://en . Wikipedia . org/wiki/mnst _ database***

*   *MNIST 评估绩效总结，数据挖掘*，Witten 等人，第 421 页

*   网站，[](http://www.nist.gov/itl/iad/image-group/emnist-dataset)

***   作者 MNIST 评价** 

**The second column in the table displays the algorithm category as either CML or DL. Most of the recent results with error rates < 1% use DL algorithms.Table 4-4

MNIST 分类算法性能总结

<colgroup class="calibre12"><col class="tcol"> <col class="tcol"> <col class="tcol"> <col class="tcol"></colgroup> 
| 

分类者

 | 

类型

 | 

误差(%)

 | 

参考

 |
| --- | --- | --- | --- |
| 线性分类器(1 层神经网络) | currentmodelogic 电流型逻辑的 | Twelve | LeCun 等人(1998 年) |
| 线性分类器(成对) | currentmodelogic 电流型逻辑的 | Seven point six | 维基百科(一个基于 wiki 技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书 |
| k-最近邻，欧几里德(L2) | currentmodelogic 电流型逻辑的 | Five | LeCun 等人(1998 年) |
| 2 层神经网络，300 个隐藏单元 | currentmodelogic 电流型逻辑的 | Four point seven | LeCun 等人(1998 年) |
| 随机森林 | currentmodelogic 电流型逻辑的 | Two point eight | 韦翰 |
| 高斯支持向量机 | currentmodelogic 电流型逻辑的 | One point four | MNIST 网站 |
| 卷积网，LeNet-5 | 分升 | Zero point nine five | LeCun 等人(1998 年) |
| 虚拟支持向量机 | 分升 | Zero point five six | 德科斯特(2002 年) |
| KNN(可移动边缘预处理) | currentmodelogic 电流型逻辑的 | Zero point five six | 维基百科(一个基于 wiki 技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书 |
| 卷积神经网络 | 分升 | Zero point four | Simard (2003 年) |
| 6 层前馈神经网络 GPU | 分升 | Zero point three five | Ciresan (2010 年) |
| 大型深度卷积神经网络 | 分升 | Zero point three five | Ciresan (2011 年) |
| 35 个卷积神经网络委员会 | 分升 | Zero point two three | Ciresan (2012 年) |

回想一下图 [4-8](#Fig8) ，k 近邻和支持向量机是分类问题的两种推荐算法。KNN 的 MNIST 性能是 95%的分类准确度，而 SVM 达到 98.6%的准确度。这些都是令人印象深刻的结果，尤其是对 SVM 而言。SVM 的结果与几个 DL 算法的结果相当。向下扫描表格的下半部分，可以看到上面的结果是用 DL 算法得到的，> 99.5%的准确率。

MNIST 是一个图像识别问题。虽然给出的准确性结果很有用，但是在得出任何结论之前，您还需要考虑不同的问题类型。

Earlier in the book, I discussed leveraging academic research papers as a means to gain a competitive advantage. One of the common metrics published by researchers is algorithm classification accuracy. Figure [4-17](#Fig17) shows the my summary of CML algorithm performance across a wide variety of ML classification problems published in academic research.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig17_HTML.jpg](Images/468661_1_En_4_Fig17_HTML.jpg) Figure 4-17

CML 分类算法比较

You saw earlier in the chapter that different algorithms are useful for different types of problems. The data in Figure [4-17](#Fig17) represents a very wide snapshot, and it is not very scientific to aggregate the results, but you can use it to make some general insights:

*   由于图表中使用了以零为基的 Y 轴，很明显算法之间的差异并不大。所有这些 CML 算法都实现了 90% - 95%范围内的准确性。

*   随机森林通常做得很好，通常优于其他分类器。这使得随机森林成为大多数 CML 分类的首选算法。

You saw with MNIST results that the SVM algorithm outperformed all of the non-DL algorithms, including random forest. This is likely because decision trees do not work as well for high-dimensional problems like MNIST. Depending on the problem, random forest and SVM are two very important, yet different, algorithms for your toolbox.

> 如果您正在寻求对多类标记数据进行分类，只需选择随机森林，节省您宝贵的时间和精力。十有八九，随机森林将优于其他 CML 分类算法。对于高维数据，如模式识别，选择 SVM 算法作为首选算法。

分类准确性不是唯一重要的衡量标准。接下来，您将看到用于算法度量的其他重要工具。

## 4.11 算法分析

Because of their close affiliation to the field of statistics, ML environments are loaded with statistical analysis features, some of which are useful, and others not so much. Next, you will explore the three algorithm analysis tools every ML practitioner should master:

*   混淆矩阵

*   ROC 曲线

*   k 倍交叉验证

在第 5 章中，你将探索如何使用这些工具来验证一个模型。

### 混淆矩阵

One of the most important outputs of the ML model is the confusion matrix . You saw the following confusion matrix in Chapter [3](3.html) when you ran a Weka classifier in the cloud:001   === Confusion Matrix === 002 003     a  b  c   <-- classified as 004    50  0  0 |  a = Iris-setosa 005     0 49  1 |  b = Iris-versicolor 006     0  2 48 |  c = Iris-virginica The confusion matrix is a two-dimensional plot with a row and column for each class. The example above had three classes. You can generate a confusion matrix for any number of dataset classes. Figure [4-18](#Fig18) shows a generic 2-class confusion matrix.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig18_HTML.jpg](Images/468661_1_En_4_Fig18_HTML.jpg) Figure 4-18

一个 2 类一般混淆矩阵

混淆矩阵中的每个元素都显示了实际类是行，预测类是列的测试实例的数量。好的结果对应于沿着矩阵对角线的大数。在 2 类混淆矩阵中，对角线值代表真阳性和真阴性。

Glancing at the values not on the main diagonal can give you excellent feedback on how the model is performing, or more specifically, when the model is getting “confused”. You can gain the following insights from the 3-class confusion matrix shown earlier:

*   2 例海滨鸢尾(c 型)被误分类为杂色鸢尾(b 型)。

*   1 例杂色鸢尾(b 型)被误诊为海滨鸢尾(c 型)。

*   所有 50 例鸢尾-圣淘沙被正确分类。

> *每次运行分类器时，ML-Gate 2 最佳实践是对照您预先确定的 mo* *del 可接受的值来检查混淆矩阵结果。*

### ROC 曲线

ROC 代表受试者操作者特征。ROC 曲线起源于第二次世界大战，被雷达操作员用于统计建模噪声环境中的假阳性和假阴性检测。由于其历史背景，ROC 曲线具有比大多数其他方法更好的统计背景。ROC 是医学和生物学中的标准度量。

The ROC curve has become very popular in ML to help evaluate the effectiveness of the models we create. ROC curves plot the true positive rate on the Y-axis and the false positive rate along the X-axis. Figure [4-19](#Fig19) shows a typical ROC curve.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig19_HTML.jpg](Images/468661_1_En_4_Fig19_HTML.jpg) Figure 4-19

受试者工作特征曲线

ROC curves have some interesting properties:

*   ROC 曲线的斜率是非递增的。斜率通常会减小。较陡的 ROC 曲线代表较好的分类。一个完美的分类器会产生一条垂直线摇摆曲线。

*   ROC 曲线上的每个点代表假阳性和假阴性之间不同的权衡或成本比率。

*   ROC 曲线切线的斜率定义了成本比率。

*   ROC 面积是 ROC 曲线下的面积。ROC 面积代表所有可能的成本比率的平均绩效。

The ROC area represents the area under the ROC curve. Table [4-5](#Tab5) shows the prediction level that is associated with the ML model given its corresponding ROC area.Table 4-5

ROC 区域预测水平

<colgroup class="calibre12"><col class="tcol"> <col class="tcol"></colgroup> 
| 

ROC 区域

 | 

预测水平

 |
| --- | --- |
| One | 完美的 |
| Zero point nine | 优秀的 |
| Zero point eight | 好的 |
| Zero point seven | 普通的 |
| Zero point six | 穷人 |
| Zero point five | 完全随机 |
| < 0.5 | 病人 |

In the next chapter, you will use the Weka ML environment to graph multiple ROC curves representing multiple algorithms. Such graphs are very useful for comparing algorithms. When graphing multiple ROC curves, you can gain the following additional insights:

*   如果两条 ROC 曲线不相交，一种方法优于另一种方法，您应该选择其相应的算法。

*   如果两条 ROC 曲线相交，一种方法(算法)对某些成本比率更好，另一种方法(算法)对其他成本比率更好。

正如您将看到的，ML 环境使多 ROC 曲线可视化变得容易。

### k 倍交叉验证

在现实世界中，我们似乎永远没有足够的数据。可用于训练和测试我们的模型的数据量通常是有限的。你看到 MNIST 的标准训练方法是 60，000 个已定义的实例用于训练，另外 10，000 个实例用于测试。这种方法被称为 ***维持方法*** ，因为您出于测试目的保留了部分数据。拿出三分之一的数据进行测试是很常见的。因为数据集很大，所以维持方法很适合 MNIST。对于较小的数据集，这可能会有问题。你如何知道数据的哪一部分是可靠的？

选择一组特定的数据进行测试会导致偏差。例如，如果您运气不好，某个特定的类在训练数据中完全缺失，当它出现在测试数据中时，您怎么能期望分类器预测这样的值呢？针对由选择用于维持的特定样本引起的偏差，解决方案是重复该过程若干(K)次，其中 K 可以是任何数字，例如 3、5 或 10。如果你希望用三分之二的数据进行训练，用三分之一的数据进行测试，那么 K 等于三。如果你希望十分之九的时间训练，十分之一的时间测试，那么 K 等于十。

This approach is called k-fold cross-validation. Figure [4-20](#Fig20) shows the example of a 5-fold cross-validation process.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig20_HTML.jpg](Images/468661_1_En_4_Fig20_HTML.jpg) Figure 4-20

五重交叉验证法

When you perform an n-fold cross-validation, you are actually performing multiple (K) passes of the training and validation cycle. The resulting accuracies of each pass are averaged to obtain the mean accuracy for the model.

> K 倍交叉验证是一个简单但强大的概念。预测学习技术的错误率的标准方法是使用 10 重交叉验证，其中数据集被随机分成 10 个部分。10 重交叉验证应该是每个 ML-Gate 2 模型评估的一部分。

大量测试表明，K=10 给出了最佳结果，与数据集大小无关。

## 4.12 Java 源代码

用于创建 ML 模型的所有流行 CML 算法的 Java 源代码都是现成的。怀卡托大学为 Java ML 算法的发展做出了巨大的贡献，最新版本的算法可以在 Weka 3.8 中获得。源代码也可以在 Subversion 存储库中找到:

***【https://SVN . CMS . waikato . AC . NZ/SVN/WEKA/branches/stability-3-8/WEKA/src/main/Java/WEKA/***

Apache Commons Math 还包括几个集群 ML 算法的 Java 代码:

***https://commons . Apache . org/proper/commons-math/user guide/ml . html***

本节给出了算法的概述，如果您想详细研究它们，您将知道在 Weka Subversion 存储库中何处可以找到它们。如果您对某个特定的算法感兴趣，查看它的 Java 源代码会非常有帮助。这些算法是在无数项目中使用的稳定的产品代码。您可以从他们的实现中学到很多东西，包括他们对 Java 集合、线程和继承的使用。

The base directory of the Subversion repository includes several folders:

*   ***核心*** 文件夹包含 Weka 代码。

*   **文件夹包含分类算法。**

***   ***聚类器*** 文件夹包含聚类算法。** 

 **Java 文件结构是分层的。这些算法继承自其他底层算法。例如，如果您希望实现随机林算法，则在构建随机林时，将包括以下每个文件:

***RandomForest.java*** will set the base classifier as ***RandomTree***:001   /** 002    * Constructor that sets base classifier to RandomTree 003    */ 004   public RandomForest() { 005       RandomTree rTree = new RandomTree(); 006       rTree.setDoNotCheckCapabilities(true); 007       super.setClassifier(rTree); 008       super.setRepresentCopiesUsingWeights(true); 009       setNumIterations(defaultNumberOfIterations()); 010   } The ***RandomTree.java*** class will extend the ***AbstractClassifier*** class:001   /** 002    * Constructor for Random Tree that extends AbstractClassifier 003    */ 004   public class RandomTree extends AbstractClassifier implements OptionHandler, 005      WeightedInstancesHandler, Randomizable, Drawable, PartitionGenerator { 006   } The ***AbstractClassifier.java*** class will implement the ***Classfier*** class:001   /** 002    * Abstract classifier. All schemes for numeric or nominal prediction in Weka 003    * extend this class. Note that a classifier MUST either implement 004    * distributionForInstance() or classifyInstance(). 005    */ 006   public abstract class AbstractClassifier implements Classifier, BatchPredictor, 007       Cloneable, Serializable, OptionHandler, CapabilitiesHandler, RevisionHandler, 008       CapabilitiesIgnorer, CommandlineRunnable { 009   } ***Classifier.java*** 001   /** 002    * Classifier interface. All schemes for numeric or nominal prediction in 003    * Weka implement this interface. Note that a classifier MUST either implement 004    * distributionForInstance() or classifyInstance(). 005    */ 006   public interface Classifier { 007   }

要查看任何 CML 算法的 Java 代码，只需浏览存储库，进入 ***分类器*** 或 ***聚类器*** 文件夹。

### 分类算法

Within the ***classifiers*** folder, the algorithms fall into three categories:

*   ***贝叶斯*** :包含朴素贝叶斯算法的几种变体。

*   ***pmml*** :预测模型标记语言是一种基于 XML 的交换格式。此文件夹包含基于 PMML 的模型，如 SVM 和回归算法。

*   ***树*** :包含所有基于决策树的算法，如随机森林。

Figure [4-21](#Fig21) shows an expanded view of the key classifier algorithms.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig21_HTML.jpg](Images/468661_1_En_4_Fig21_HTML.jpg) Figure 4-21

Java 分类器算法(Weka Subversion 存储库)

在图 [4-8](#Fig8) 中列出的最有用的分类器包含在存储库中。图 [4-21](#Fig21) 显示朴素贝叶斯、随机森林、SVM 源代码都包含在 ***分类器*** 目录中。

### 聚类算法

Figure [4-22](#Fig22) shows the Java clustering algorithms available in Weka.![../images/468661_1_En_4_Chapter/468661_1_En_4_Fig22_HTML.jpg](Images/468661_1_En_4_Fig22_HTML.jpg) Figure 4-22

Java 集群器(Weka Subversion 仓库)

在图 [4-8](#Fig8) 中，您可以看到 k-means 和 DBSCAN 是用于无标签数据聚类问题的常用算法。这些算法的源代码在 SimpleKMeans.java 的**和 DensityBasedCluster.java 的**内。****

 ****正如您看到的分类算法一样，聚类算法也建立在目录中其他更高级别的聚类类之上，例如***【Cluster.java】***和***heiarchicalclusterer******。java*** 。

### Java 算法修改

我之前说过 CML 算法是商品。Weka Subversion 存储库中 Java 算法的源代码显示算法代码已经稳定了好几年。Weka 库中的所有 Java 代码都是在 GNU 通用公共许可证下授权的。代码是免费的，您可以根据许可条款分发或修改它。

[*【www.gnu.org/licenses/】*](http://www.gnu.org/licenses/)

Due to advances in the Java platform, there are several areas where these algorithms could be improved:

*   **Lambda 表达式**:添加了 ***lambda 表达式*** 正在开始重塑 Java。Lambda 表达式允许 Java API 库中的新功能。例如，lambda 表达式简化了对每个 样式操作的 ***处理，允许我们更好地利用多核环境的并行处理能力。通过引入 lambda 表达式，有机会提高大多数 Java 算法的性能。***

*   Stream API:Java 8 中引入的新的 Stream API 允许我们以更强大的方式操作数据。新的流 API 与 Java 集合类和 lambda 表达式协同工作。新的流 API 可以处理高级数据查询，并提供更高的效率，尤其是对于大型数据集。

*   **并发**:如果你查看 Java 代码，你会发现很多算法都使用 Java***ThreadPoolExecutor***来处理它们的多线程操作。***ThreadPoolExecutor***是组成 Java***ExecutorService***的服务之一。JDK 7 看到了一个新服务的引入， ***ForkJoinPool*** 。 ***ForkJoinPool*** 类支持并行编程的实现，使得线程可以在多个 CPU/GPU 上运行。一个 ***ForkJoin*** 任务背后的主要思想是分治策略，非常适合决策树家族的算法，包括随机森林。更新 Java 算法以利用 ***ForkJoin*** 可以显著提高性能。

这些潜在的增强是与性能相关的改进。正如在第 [2](2.html) 章中所讨论的，基于云的提供商使得按需扩展 CPU 资源变得容易。因此，在云中构建模型时，添加 CPU 资源通常比优化算法以利用最新的 JAVA API 特性更容易。这些增强在模型构建时 CPU 约束有限的环境中非常有用，比如在移动或嵌入式设备上构建模型。**********