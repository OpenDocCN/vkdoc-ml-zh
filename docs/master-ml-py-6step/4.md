# 4.步骤 4:模型诊断和调整

在这一章中，我们将了解在构建机器学习(ML)系统时应该意识到和会遇到的不同陷阱。我们还将学习行业标准的高效设计实践来解决这个问题。

在本章中，我们将主要使用来自 UCI 知识库的数据集“Pima Indian diabetes”，它有 768 个记录、8 个属性、2 个类、268 个(34.9%)糖尿病测试的阳性结果和 500 个(65.1%)阴性结果。所有患者均为至少 21 岁的皮马印第安裔女性。

数据集的属性:

1.  怀孕次数

2.  口服葡萄糖耐量试验中 2 小时的血浆葡萄糖浓度

3.  舒张压(毫米汞柱)

4.  三头肌皮褶厚度(毫米)

5.  2 小时血清胰岛素(微单位/毫升)

6.  身体质量指数(体重，单位为千克/(身高，m)^2)

7.  糖尿病谱系功能

8.  年龄(岁)

## 最佳概率截止点

预测概率是一个介于 0 和 1 之间的数字。传统上，> . 5 是用于将预测概率转换为 1(正)的分界点，否则为 0(负)。当你的训练数据集有一个正例和反例相等的例子时，这个逻辑工作得很好；然而，在真实的场景中却不是这样。

解决方法是找到最优的截断点(即真阳性率高，假阳性率低的点)。任何高于这个阈值的都可以标记为 1，否则为 0。清单 [4-1](#PC1) 应该说明了这一点，所以让我们加载数据并检查类分布。

```py
import pandas as pd
import pylab as plt
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

# target variable % distribution
print (df['class'].value_counts(normalize=True))
#----output----
0    0.651042
1    0.348958

Listing 4-1Load Data and Check the Class Distribution

```

让我们建立一个快速逻辑回归模型，并检查其准确性(清单 [4-2](#PC2) )。

```py
X = df.ix[:,:8]     # independent variables
y = df['class']     # dependent variables

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

# instantiate a logistic regression model, and fit
model = LogisticRegression()
model = model.fit(X_train, y_train)

# predict class labels for the train set. The predict fuction converts probability values > .5 to 1 else 0
y_pred = model.predict(X_train)

# generate class probabilities
# Notice that 2 elements will be returned in probs array,
# 1st element is probability for negative class,
# 2nd element gives probability for positive class
probs = model.predict_proba(X_train)
y_pred_prob = probs[:, 1]

# generate evaluation metrics
print ("Accuracy: ", metrics.accuracy_score(y_train, y_pred))
#----output----
Accuracy:  0.767225325885

Listing 4-2Build a Logistic Regression Model and Evaluate the Performance

```

最佳截止点是真阳性率(tpr)高而假阳性率(fpr)低，并且 tpr - (1-fpr)为零或接近零。清单 [4-3](#PC3) 是绘制 tprvs 的接收器工作特性(ROC)图的示例代码。1-fpr。

```py
# extract false positive, true positive rate
fpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred_prob)
roc_auc = metrics.auc(fpr, tpr)
print("Area under the ROC curve : %f" % roc_auc)

i = np.arange(len(tpr)) # index for df
roc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i),'1-fpr' : pd.Series(1-fpr, index = i), 'tf' : pd.Series(tpr - (1-fpr), index = i),'thresholds' : pd.Series(thresholds, index = i)})
roc.ix[(roc.tf-0).abs().argsort()[:1]]

# Plot tpr vs 1-fpr
fig, ax = plt.subplots()
plt.plot(roc['tpr'], label="tpr")
plt.plot(roc['1-fpr'], color = 'red', label='1-fpr')
plt.legend(loc='best')
plt.xlabel('1-False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.show()
#----output----

Listing 4-3Find Optimal Cutoff Point

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figa_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figa_HTML.jpg)

从图表中可以看出，tpr 与 1-fpr 的交叉点是最佳分界点。为了简化寻找最佳概率阈值和实现可重用性，我创建了一个函数来寻找最佳概率截止点(清单 [4-4](#PC4) )。

```py
def Find_Optimal_Cutoff(target, predicted):
    """ Find the optimal probability cutoff point for a classification model related to the event rate
    Parameters
    ----------
    target: Matrix with dependent or target data, where rows are observations

    predicted: Matrix with predicted data, where rows are observations

    Returns
    -------
    list type, with optimal cutoff value

    """
    fpr, tpr, threshold = metrics.roc_curve(target, predicted)
    i = np.arange(len(tpr))
    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})
    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]

    return list(roc_t['threshold']) 

# Find optimal probability threshold
# Note: probs[:, 1] will have the probability of being a positive label
threshold = Find_Optimal_Cutoff(y_train, probs[:, 1])
print ("Optimal Probability Threshold: ", threshold)

# Applying the threshold to the prediction probability
y_pred_optimal = np.where(y_pred_prob >= threshold, 1, 0)

# Let's compare the accuracy of traditional/normal approach vs optimal cutoff
print ("\nNormal - Accuracy: ", metrics.accuracy_score(y_train, y_pred))
print ("Optimal Cutoff - Accuracy: ", metrics.accuracy_score(y_train, y_pred_optimal))
print ("\nNormal - Confusion Matrix: \n", metrics.confusion_matrix(y_train, y_pred))
print ("Optimal - Cutoff Confusion Matrix: \n", metrics.confusion_matrix(y_train, y_pred_optimal))
#----output----
Optimal Probability Threshold:  [0.36133240553264734]

Normal - Accuracy:  0.767225325885
Optimal Cutoff - Accuracy:  0.761638733706

Normal - Confusion Matrix:
[[303  40]
 [ 85 109]]
Optimal - Cutoff Confusion Matrix:
[[260  83]
 [ 47 147]]

Listing 4-4A Function for Finding Optimal Probability Cutoff

```

请注意，正常截止方法与最佳截止方法之间的总体准确性没有显著差异；都是 76%。但是，在最佳临界值方法中，真实阳性率增加了 36%(即，您现在能够将 36%以上的阳性病例捕获为阳性)。此外，假阳性(I 型错误)增加了一倍(即，预测个体没有糖尿病为阳性的概率增加了)。

### 哪个错误代价大？

嗯，这个问题没有一个答案！这取决于领域、您试图解决的问题以及业务需求(图 [4-1](#Fig1) )。在我们的皮马糖尿病案例中，相比之下，第二类错误可能比第一类错误更具破坏性，但这是有争议的。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig1_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig1_HTML.jpg)

图 4-1

I 型与 II 型误差

## 罕见事件或不平衡数据集

向分类算法提供正和负实例的相等样本将产生最佳结果。事实证明，高度偏向一个或多个类的数据集是一个挑战。

重采样是解决不平衡数据集问题的常见做法。虽然重采样中有许多技术，但在这里我们将学习三种最流行的技术(图 [4-2](#Fig2) ):

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig2_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig2_HTML.jpg)

图 4-2

不平衡数据集处理技术

*   *随机欠采样*:减少多数类以匹配少数类计数

*   *随机过采样*:通过在少数类中随机选取样本来增加少数类，直到两个类的计数匹配

*   *合成少数过采样技术(SMOTE)* :通过使用特征空间相似性(欧几里德距离)连接所有 k 个(缺省值= 5)少数类最近邻居，通过引入合成例子来增加少数类

让我们使用 sklearn 的 make_classification 函数创建一个样本不平衡数据集(清单 [4-5](#PC5) )。

```py
# Load libraries
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE

# Generate the dataset with 2 features to keep it simple
X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,
                           n_redundant=0, weights=[0.9, 0.1], random_state=2017)

print ("Positive class: ", y.tolist().count(1))
print ("Negative class: ", y.tolist().count(0))
#----output----
Positive class:  514
Negative class:  4486

Listing 4-5Rare Event or Imbalanced Data Handling

```

让我们将前面描述的三种采样技术应用于数据集，以平衡数据集并进行可视化，以便更好地理解。

```py
# Apply the random under-sampling
rus = RandomUnderSampler()
X_RUS, y_RUS = rus.fit_sample(X, y)

# Apply the random over-sampling
ros = RandomOverSampler()
X_ROS, y_ROS = ros.fit_sample(X, y)

# Apply regular SMOTE
sm = SMOTE(kind='regular')
X_SMOTE, y_SMOTE = sm.fit_sample(X, y)

# Original vs resampled subplots
plt.figure(figsize=(10, 6))
plt.subplot(2,2,1)
plt.scatter(X[y==0,0], X[y==0,1], marker="o", color="blue")
plt.scatter(X[y==1,0], X[y==1,1], marker='+', color="red")
plt.xlabel('x1')
plt.ylabel('x2')
plt.title('Original: 1=%s and 0=%s' %(y.tolist().count(1), y.tolist().count(0)))

plt.subplot(2,2,2)
plt.scatter(X_RUS[y_RUS==0,0], X_RUS[y_RUS==0,1], marker="o", color="blue")
plt.scatter(X_RUS[y_RUS==1,0], X_RUS[y_RUS==1,1], marker='+', color="red")
plt.xlabel('x1')
plt.ylabel('y2')
plt.title('Random Under-sampling: 1=%s and 0=%s' %(y_RUS.tolist().count(1), y_RUS.tolist().count(0)))

plt.subplot(2,2,3)
plt.scatter(X_ROS[y_ROS==0,0], X_ROS[y_ROS==0,1], marker="o", color="blue")
plt.scatter(X_ROS[y_ROS==1,0], X_ROS[y_ROS==1,1], marker='+', color="red")
plt.xlabel('x1')
plt.ylabel('x2') 

plt.title('Random over-sampling: 1=%s and 0=%s' %(y_ROS.tolist().count(1), y_ROS.tolist().count(0)))

plt.subplot(2,2,4)
plt.scatter(X_SMOTE[y_SMOTE==0,0], X_SMOTE[y_SMOTE==0,1], marker="o", color="blue")
plt.scatter(X_SMOTE[y_SMOTE==1,0], X_SMOTE[y_SMOTE==1,1], marker='+', color="red")
plt.xlabel('x1')
plt.ylabel('y2')
plt.title('SMOTE: 1=%s and 0=%s' %(y_SMOTE.tolist().count(1), y_SMOTE.tolist().count(0)))

plt.tight_layout()

plt.show()
#----output----

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figb_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figb_HTML.jpg)

### 警告

请记住，随机欠采样增加了信息或概念丢失的机会，因为我们正在减少多数类，并且随机过采样& SMOTE 会由于多个相关实例而导致过拟合问题。

### 哪种重采样技术是最好的？

嗯，这个问题还是没有答案！让我们对前面三个重采样数据尝试一个快速分类模型，并比较其准确性。我们将使用 AUC 指标，因为这是模型性能的最佳表现之一(清单 [4-6](#PC7) )。

```py
from sklearn import tree
from sklearn import metrics
from sklearn.cross_ model_selection import train_test_split

X_RUS_train, X_RUS_test, y_RUS_train, y_RUS_test = train_test_split(X_RUS, y_RUS, test_size=0.3, random_state=2017)
X_ROS_train, X_ROS_test, y_ROS_train, y_ROS_test = train_test_split(X_ROS, y_ROS, test_size=0.3, random_state=2017)
X_SMOTE_train, X_SMOTE_test, y_SMOTE_train, y_SMOTE_test = train_test_split(X_SMOTE, y_SMOTE, test_size=0.3, random_state=2017)

# build a decision tree classifier
clf = tree.DecisionTreeClassifier(random_state=2017)
clf_rus = clf.fit(X_RUS_train, y_RUS_train)
clf_ros = clf.fit(X_ROS_train, y_ROS_train)
clf_smote = clf.fit(X_SMOTE_train, y_SMOTE_train)

# evaluate model performance
print ("\nRUS - Train AUC : ",metrics.roc_auc_score(y_RUS_train, clf.predict(X_RUS_train)))
print ("RUS - Test AUC : ",metrics.roc_auc_score(y_RUS_test, clf.predict(X_RUS_test)))
print ("ROS - Train AUC : ",metrics.roc_auc_score(y_ROS_train, clf.predict(X_ROS_train)))
print ("ROS - Test AUC : ",metrics.roc_auc_score(y_ROS_test, clf.predict(X_ROS_test)))
print ("\nSMOTE - Train AUC : ",metrics.roc_auc_score(y_SMOTE_train, clf.predict(X_SMOTE_train)))
print ("SMOTE - Test AUC : ",metrics.roc_auc_score(y_SMOTE_test, clf.predict(X_SMOTE_test)))
#----output----

RUS - Train AUC :  0.988945248974
RUS - Test AUC :  0.983964646465
ROS - Train AUC :  0.985666951094
ROS - Test AUC :  0.986630288452

SMOTE - Train AUC :  1.0
SMOTE - Test AUC :  0.956132695918

Listing 4-6Build Models on Various Resampling Methods and Evaluate Performance

```

这里，随机过采样在训练集和测试集上都表现得更好。作为一种最佳实践，在现实世界的用例中，建议查看其他指标(如精确度、召回率、混淆矩阵)并应用业务上下文或领域知识来评估模型的真实性能。

## 偏差和方差

监督学习的一个基本问题是偏差`–`方差权衡。理想情况下，模型应该具有两个关键特征:

1.  它应该足够敏感，以准确地捕获训练数据集中的关键模式。

2.  它应该足够一般化，以便在任何看不见的数据集上都能很好地工作。

不幸的是，在试图实现上述第一点时，很有可能过度拟合有噪声或不具有代表性的训练数据点，从而导致模型泛化失败。另一方面，试图概括一个模型可能会导致无法捕捉重要的规律性(图 [4-3](#Fig3) )。

### 偏见

如果训练数据集和测试数据集上的模型精度较低，则该模型被称为拟合不足或具有较高的偏差。这意味着模型在回归中没有很好地拟合训练数据集点，或者决策边界在分类中没有很好地分离类。偏差的两个主要原因是 1)没有包括正确的特征，以及 2)没有为模型拟合选择正确的多项式次数。

要解决拟合不足的问题或减少偏差，请尝试包含更有意义的特征，并通过尝试更高阶的多项式拟合来增加模型的复杂性。

### 变化

如果模型在训练数据集上给出高精度，但是在测试数据集上精度急剧下降，则该模型被称为过度拟合或具有高方差。过度拟合的主要原因是使用更高阶的多项式次数(可能不是必需的)，这将使决策边界工具很好地拟合所有数据点，包括训练数据集的噪声，而不是基础关系。这将导致训练数据集中的高准确度(实际与预测),并且当应用于测试数据集时，预测误差将会很高。

要解决过度拟合问题:

*   尝试减少要素的数量，即仅保留有意义的要素，或者尝试保留所有要素但减少要素参数大小的正则化方法。

*   降维可以消除噪声特征，从而降低模型方差。

*   引入更多的数据点来增大训练数据集也将减少方差。

*   例如，选择正确的模型参数有助于减少偏差和方差。
    *   使用正确的正则化参数可以减少基于回归的模型中的方差。

    *   For a decision tree, reducing the depth of the decision tree will reduce the variance.

        ![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig3_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig3_HTML.jpg)

        图 4-3

        偏差-方差权衡

## k 倍交叉验证

K-fold 交叉验证将训练数据集分成 k 个折叠，而不进行替换-任何给定的数据点都将只是其中一个子集的一部分，其中 k-1 个折叠用于模型训练，一个折叠用于测试。该过程重复 k 次，以便我们获得 k 个模型和性能估计值(图 [4-4](#Fig4) )。

然后，我们基于单个折叠计算模型的平均性能，以获得与维持或单个折叠方法相比对训练数据的子划分不太敏感的性能估计。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig4_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig4_HTML.jpg)

图 4-4

k 倍交叉验证

清单 [4-7](#PC8) 显示了使用 sklearn 的 k-fold 交叉验证来构建分类模型的示例代码。

```py
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

X = df.ix[:,:8].values     # independent variables
y = df['class'].values     # dependent variables

# Normalize Data
sc = StandardScaler()
sc.fit(X)
X = sc.transform(X)

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)

# build a decision tree classifier
clf = tree.DecisionTreeClassifier(random_state=2017)

# evaluate the model using 10-fold cross-validation
train_scores = cross_val_score(clf, X_train, y_train, scoring="accuracy", cv=5)
test_scores = cross_val_score(clf, X_test, y_test, scoring="accuracy", cv=5)
print ("Train Fold AUC Scores: ", train_scores)
print ("Train CV AUC Score: ", train_scores.mean())

print ("\nTest Fold AUC Scores: ", test_scores)
print ("Test CV AUC Score: ", test_scores.mean())
#---output----
Train Fold AUC Scores:  [0.80555556 0.73148148 0.81308411 0.76635514 0.71028037]
Train CV AUC Score:  0.7653513326410523

Test Fold AUC Scores:  [0.80851064 0.78723404 0.78723404 0.77777778 0.8   ]
Test CV AUC Score:  0.7921513002364066

Listing 4-7
K-fold Cross-Validation

```

### 分层 K 倍交叉验证

扩展交叉验证是分层的 k-fold 交叉验证，其中类别比例在每个 fold 中保持不变，从而导致更好的偏差和方差估计(列表 [4-8](#PC9) 和 [4-9](#PC10) )。

```py
from sklearn.metrics import roc_curve, auc
from itertools import cycle
from scipy import interp

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)

mean_tpr = 0.0
mean_fpr = np.linspace(0, 1, 100)

colors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])
lw = 2

i = 0
for (train, test), color in zip(kfold.split(X, y), colors):
    probas_ = clf.fit(X[train], y[train]).predict_proba(X[test])
    # Compute ROC curve and area the curve
    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])
    mean_tpr += interp(mean_fpr, fpr, tpr)
    mean_tpr[0] = 0.0
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=lw, color=color,
             label='ROC fold %d (area = %0.2f)' % (i, roc_auc))

    i += 1
plt.plot([0, 1], [0, 1], linestyle="--", lw=lw, color="k",
         label='Luck')

mean_tpr /= kfold.get_n_splits(X, y)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
plt.plot(mean_fpr, mean_tpr, color="g", linestyle="--",
         label='Mean ROC (area = %0.2f)' % mean_auc, lw=lw)

plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate') 

plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()
#----Output----

Listing 4-9Plotting the ROC Curve for Stratified K-fold Cross-Validation

```

```py
from sklearn import model_selection

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)

train_scores = []
test_scores = []
k = 0
for (train, test) in kfold.split(X_train, y_train):
    clf.fit(X_train[train], y_train[train])
    train_score = clf.score(X_train[train], y_train[train])
    train_scores.append(train_score)
    # score for test set
    test_score = clf.score(X_train[test], y_train[test])
    test_scores.append(test_score)

    k += 1
    print('Fold: %s, Class dist.: %s, Train Acc: %.3f, Test Acc: %.3f'
          % (k, np.bincount(y_train[train]), train_score, test_score))

print('\nTrain CV accuracy: %.3f' % (np.mean(train_scores)))
print('Test CV accuracy: %.3f' % (np.mean(test_scores)))
#----output----
Fold: 1, Class dist.: [277 152], Train Acc: 0.758, Test Acc: 0.806
Fold: 2, Class dist.: [277 152], Train Acc: 0.779, Test Acc: 0.731
Fold: 3, Class dist.: [278 152], Train Acc: 0.767, Test Acc: 0.813
Fold: 4, Class dist.: [278 152], Train Acc: 0.781, Test Acc: 0.766
Fold: 5, Class dist.: [278 152], Train Acc: 0.781, Test Acc: 0.710

Train CV accuracy: 0.773
Test CV accuracy: 0.765

Listing 4-8
Stratified K-fold Cross-Validation

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figc_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figc_HTML.jpg)

## 集成方法

集成方法能够将多个模型分数组合成单个分数，以创建稳健的通用模型。

在高层次上，有两种类型的集成方法:

1.  组合相似类型的多个模型。
    *   引导聚集

    *   助推

2.  组合各种类型的多个模型。
    *   投票分类

    *   混合或堆叠

### 制袋材料

Bootstrap aggregation(也称为 bagging)由 [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman) 于 1994 年提出；这是一种减少模型方差的模型聚合技术。训练数据被分成多个样本，替换为引导样本。引导样本大小将与原始样本大小相同，原始值的 3/4 和替换导致值的重复(图 [4-5](#Fig5) )。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig5_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig5_HTML.jpg)

图 4-5

拔靴带

构建每个引导样本的独立模型，并使用回归预测的平均值或分类的多数投票来创建最终模型。

图 [4-6](#Fig6) 显示了装袋工艺流程。如果 N 是从原始训练集中创建的引导样本的数量，对于 i = 1 到 N，训练一个基本 ML 模型 C <sub>i</sub> 。

C <sub>最终</sub>= y 的累计最大值![$$ \sum \limits_{\mathrm{i}}\mathrm{I}\left({\mathrm{C}}_{\mathrm{i}}=\mathrm{y}\right) $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq1.png)

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig6_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig6_HTML.jpg)

图 4-6

制袋材料

我们来比较一下独立决策树模型和 100 棵树的 bagging 决策树模型(清单 [4-10](#PC11) )的性能。

```py
# Bagged Decision Trees for Classification
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

X = df.ix[:,:8].values     # independent variables
y = df['class'].values     # dependent variables

#Normalize
X = StandardScaler().fit_transform(X)

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)
num_trees = 100

# Decision Tree with 5 fold cross validation
clf_DT = DecisionTreeClassifier(random_state=2019).fit(X_train,y_train)
results = model_selection.cross_val_score(clf_DT, X_train,y_train, cv=kfold)
print ("Decision Tree (stand alone) - Train : ", results.mean())
print ("Decision Tree (stand alone) - Test : ", metrics.accuracy_score(clf_DT.predict(X_test), y_test))

# Using Bagging Lets build 100 decision tree models and average/majority vote prediction
clf_DT_Bag = BaggingClassifier(base_estimator=clf_DT, n_estimators=num_trees, random_state=2019).fit(X_train,y_train)
results = model_selection.cross_val_score(clf_DT_Bag, X_train, y_train, cv=kfold)
print ("\nDecision Tree (Bagging) - Train : ", results.mean())
print ("Decision Tree (Bagging) - Test : ", metrics.accuracy_score(clf_DT_Bag.predict(X_test), y_test))
#----output----
Decision Tree (stand alone) - Train :  0.6742199894235854
Decision Tree (stand alone) - Test :  0.6428571428571429

Decision Tree (Bagging) - Train :  0.7460074034902167
Decision Tree (Bagging) - Test :  0.8051948051948052

Listing 4-10Stand-Alone Decision Tree vs. Bagging

```

### 特征重要性

决策树模型具有显示重要特征的属性，这些特征基于基尼或熵信息增益(列表 [4-11](#PC12) )。

```py
feature_importance = clf_DT.feature_importances_
# make importances relative to max importance
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)
pos = np.arange(sorted_idx.shape[0]) + .5
plt.subplot(1, 2, 2)
plt.barh(pos, feature_importance[sorted_idx], align="center")
plt.yticks(pos, df.columns[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.show()

#----output----

Listing 4-11Decision Tree Feature Importance Function

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figd_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figd_HTML.jpg)

### 随机森林

随机选取一个观察子集和一个变量子集来构建多个独立的基于树的模型。这些树更不相关，因为在树的分割过程中只使用了变量的子集，而不是在树的构造中贪婪地选择最佳分割点(清单 [4-12](#PC13) )。

```py
from sklearn.ensemble import RandomForestClassifier
num_trees = 100

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)

clf_RF = RandomForestClassifier(n_estimators=num_trees).fit(X_train, y_train)
results = model_selection.cross_val_score(clf_RF, X_train, y_train, cv=kfold)

print ("\nRandom Forest (Bagging) - Train : ", results.mean())
print ("Random Forest (Bagging) - Test : ", metrics.accuracy_score(clf_RF.predict(X_test), y_test))
#----output----
Random Forest - Train :  0.7379693283976732
Random Forest - Test :  0.8051948051948052

Listing 4-12
RandomForest Classifier

```

### 极度随机化的树(ExtraTree)

这种算法是为了给装袋过程引入更多的随机性。树分裂是从每个分裂的样本值范围中完全随机选择的，这允许进一步减少模型的方差，但代价是偏差略有增加(清单 [4-13](#PC14) )。

```py
from sklearn.ensemble import ExtraTreesClassifier
num_trees = 100

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)

clf_ET = ExtraTreesClassifier(n_estimators=num_trees).fit(X_train, y_train)
results = cross_validation.cross_val_score(clf_ET, X_train, y_train, cv=kfold)

print ("\nExtraTree - Train : ", results.mean())
print ("ExtraTree - Test : ", metrics.accuracy_score(clf_ET.predict(X_test), y_test))
#----output----
ExtraTree - Train :  0.7410893707033315
ExtraTree - Test :  0.7987012987012987

Listing 4-13Extremely Randomized Trees (ExtraTree)

```

### 决策边界看起来如何？

让我们执行主成分分析，为了便于绘图，只考虑前两个主要成分。模型构建代码将保持不变，除了在规范化之后和分割数据以进行训练和测试之前，我们需要添加以下代码。

一旦我们成功地运行了模型，我们就可以使用下面的代码来绘制独立模型和不同 bagging 模型的决策边界。

```py
from sklearn.decomposition import PCA
from matplotlib.colors import ListedColormap
# PCA
X = PCA(n_components=2).fit_transform(X)

def plot_decision_regions(X, y, classifier):

    h = .02  # step size in the mesh
    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h),
                           np.arange(x2_min, x2_max, h))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.8, c=colors[idx],
                    marker=markers[idx], label=cl)

# Plot the decision boundary
plt.figure(figsize=(10,6))
plt.subplot(221)
plot_decision_regions(X, y, clf_DT)
plt.title('Decision Tree (Stand alone)')
plt.xlabel('PCA1')
plt.ylabel('PCA2')

plt.subplot(222)
plot_decision_regions(X, y, clf_DT_Bag)
plt.title('Decision Tree (Bagging - 100 trees)')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend(loc='best')

plt.subplot(223)
plot_decision_regions(X, y, clf_RF)
plt.title('RandomForest Tree (100 trees)')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend(loc='best')

plt.subplot(224)
plot_decision_regions(X, y, clf_ET)
plt.title('Extream Random Tree (100 trees)')
plt.xlabel('PCA1')
plt.ylabel('PCA2')
plt.legend(loc='best')
plt.tight_layout()

#----output----

Decision Tree (stand alone) - Train :  0.5781332628239026
Decision Tree (stand alone) - Test :  0.6688311688311688

Decision Tree (Bagging) - Train :  0.6319936541512428
Decision Tree (Bagging) - Test :  0.7467532467532467

Random Forest - Train :  0.6418297197250132
Random Forest  - Test :  0.7662337662337663

ExtraTree - Train :  0.6205446853516658
ExtraTree - Test :  0.7402597402597403

Listing 4-14Plot the Decision Boudaries

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fige_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fige_HTML.jpg)

### bagging—基本调谐参数

让我们看看获得更好模型结果的关键调整参数。

*   *n_estimators:* 这是树的数量——越大越好。注意，超过某一点，结果不会有明显改善。

*   *max_features:* 这是用于分割节点的随机特征子集，越低越有利于减少方差(但会增加偏差)。理想情况下，对于回归问题，它应该等于 n_features(要素总数),对于分类，它应该等于 n_features 的平方根。

*   *n_jobs:* 用于平行建造树的核心数量。如果设置为-1，则使用系统中所有可用的核心，或者您可以指定数量。

## 助推

Freud 和 Schapire 在 1995 年用著名的 AdaBoost 算法引入了 boosting 的概念(自适应 boosting)。boosting 的核心概念是，与其说是一个独立的个体假设，不如说是将假设按顺序组合起来提高了准确性。本质上，助推算法将弱学习者转化为强学习者。升压算法经过精心设计，可以解决偏差问题(图 [4-7](#Fig7) )。

概括来说，AdaBoosting 过程可以分为三个步骤:

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig7_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig7_HTML.jpg)

图 4-7

AdaBoosting

*   为所有数据点分配统一的权重 W <sub>0</sub> (x) = 1 / N，其中 N 为训练数据点的总数。

*   在每次迭代中，将分类器 y <sub>m</sub> (x <sub>n</sub> 拟合到训练数据，并更新权重以最小化加权误差函数。

    重量计算为![$$ {\mathrm{W}}_{\mathrm{n}}^{\left(\mathrm{m}+1\right)}={\mathrm{W}}_{\mathrm{n}}^{\left(\mathrm{m}\right)}\exp \left\{\ {\propto}_{\mathrm{m}}{\mathrm{y}}_{\mathrm{m}}\ \left({\mathrm{x}}_{\mathrm{n}}\right)\ne {\mathrm{t}}_{\mathrm{n}}\right\} $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq2.png)。

    假设权重或损失函数由![$$ {\propto}_{\mathrm{m}}=\frac{1}{2}\log\ \left\{\ \frac{1-{\in}_{\mathrm{m}}}{\in_{\mathrm{m}}}\ \right\} $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq3.png)给出，期限利率由![$$ {\in}_{\mathrm{m}}=\kern0.5em \frac{\sum_{\mathrm{n}=1}^{\mathrm{N}}{\mathrm{W}}_{\mathrm{n}}^{\left(\mathrm{m}\right)}\ \mathrm{I}\ \left({\mathrm{y}}_{\mathrm{m}}\left({\mathrm{x}}_{\mathrm{n}}\right)\ne {\mathrm{t}}_{\mathrm{n}}\right)}{\sum_{\mathrm{n}=1}^{\mathrm{N}}{\mathrm{W}}_{\mathrm{n}}^{\left(\mathrm{m}\right)}} $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq4.png)给出，其中![$$ \left({\mathrm{y}}_{\mathrm{m}}\left({\mathrm{x}}_{\mathrm{n}}\right)\ne {\mathrm{t}}_{\mathrm{n}}\right)\ \mathrm{has}\ \mathrm{values}\ \frac{0}{1}\ \mathrm{i}.\mathrm{e}.,\kern0.375em 0\ \mathrm{i}\mathrm{f}\ \left({\mathrm{x}}_{\mathrm{n}}\right)\ \mathrm{correctly}\ \mathrm{classified}\ \mathrm{else}\ 1 $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq5.png)

*   最终模型由![$$ {\mathrm{Y}}_{\mathrm{m}}=\mathit{\operatorname{sign}}\left(\sum \limits_{\mathrm{m}=1}^{\mathrm{M}}{\propto}_{\mathrm{m}}{\mathrm{y}}_{\mathrm{m}}\left(\mathrm{x}\right)\right) $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq6.png)给出

### AdaBoost 的示例图

让我们考虑具有十个数据点的两个类别标签的训练数据。假设，最初，所有数据点将具有由 1/10 给出的相等权重，如图 [4-8](#Fig8) 所示。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig8_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig8_HTML.jpg)

图 4-8

有十个数据点的样本数据集

#### 提升迭代 1

注意在图 [4-9](#Fig9) 中，正类的三个点被第一个简单分类模型错误分类，因此它们将被赋予更高的权重。误差项和损失函数(学习率)分别计算为 0.30 和 0.42。由于分类错误，数据点 P3、P4 和 P5 将获得更高的权重(0.15)，而其他数据点将保留原始权重(0.1)。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig9_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig9_HTML.jpg)

图 4-9

Y <sub>m1</sub> 是第一个分类或假设

#### 推进迭代 2

让我们拟合另一个分类模型，如图 [4-10](#Fig10) 所示，并注意到负类的三个数据点(P6、P7 和 P8)被错误分类。因此，根据计算，这些点将被赋予更高的权重 0.17，而其余数据点的权重将保持不变，因为它们被正确分类。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig10_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig10_HTML.jpg)

图 4-10

*Y*<sub>T3】m2</sub>T6】是第二个分类或假设

#### 推进迭代 3

第三分类模型错误分类了总共三个数据点:两个阳性类，P1 和 P2；和一个消极阶层，P9。因此，根据计算，这些错误分类的数据点将被分配一个新的更高的权重 0.19，而其余的数据点将保留其先前的权重(图 [4-11](#Fig11) )。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig11_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig11_HTML.jpg)

图 4-11

*Y*<sub>T3】m3</sub>*是第三种*分类或假设

#### 最终模型

现在，根据 AdaBoost 算法，让我们结合如图 [4-12](#Fig12) 所示的弱分类模型。注意，最终的组合模型将具有最小的误差项和最大的学习率，从而导致更高的精确度。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig12_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig12_HTML.jpg)

图 4-12

结合弱分类器的 AdaBoost 算法

让我们从 Pima 糖尿病数据集中挑选弱预测器，并比较独立决策树模型与 AdaBoost 在决策树模型上进行 100 轮提升的性能(清单 [4-15](#PC16) )。

```py
# Bagged Decision Trees for Classification
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

# Let's use some week features to build the tree
X = df[['age','serum_insulin']]     # independent variables
y = df['class'].values              # dependent variables

#Normalize
X = StandardScaler().fit_transform(X)

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)
num_trees = 100

# Dection Tree with 5 fold cross validation
# lets restrict max_depth to 1 to have more impure leaves
clf_DT = DecisionTreeClassifier(max_depth=1, random_state=2019).fit(X_train,y_train)
results = model_selection.cross_val_score(clf_DT, X_train,y_train, cv=kfold.split(X_train, y_train))
print("Decision Tree (stand alone) - CV Train : %.2f" % results.mean())
print("Decision Tree (stand alone) - Test : %.2f" % metrics.accuracy_score(clf_DT.predict(X_train), y_train))
print("Decision Tree (stand alone) - Test : %.2f" % metrics.accuracy_score(clf_DT.predict(X_test), y_test))

# Using Adaptive Boosting of 100 iteration
clf_DT_Boost = AdaBoostClassifier(base_estimator=clf_DT, n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train,y_train)
results = model_selection.cross_val_score(clf_DT_Boost, X_train, y_train, cv=kfold.split(X_train, y_train))
print("\nDecision Tree (AdaBoosting) - CV Train : %.2f" % results.mean())
print("Decision Tree (AdaBoosting) - Train : %.2f" % metrics.accuracy_score(clf_DT_Boost.predict(X_train), y_train))
print("Decision Tree (AdaBoosting) - Test : %.2f" % metrics.accuracy_score(clf_DT_Boost.predict(X_test), y_test))
#----output----
Decision Tree (stand alone) - CV Train : 0.64
Decision Tree (stand alone) - Test : 0.64
Decision Tree (stand alone) - Test : 0.70

Decision Tree (AdaBoosting) - CV Train : 0.68
Decision Tree (AdaBoosting) - Train : 0.71
Decision Tree (AdaBoosting) - Test : 0.79

Listing 4-15Stand-Alone Decision Tree vs. AdaBoost

```

请注意，在这种情况下，与独立的决策树模型相比，AdaBoost 算法在训练/测试数据集之间平均增加了 9%的错误分数。

### 梯度推进

由于分阶段的可加性，损失函数可以用适合于优化的形式来表示。这就产生了一类被称为广义提升机器(GBM)的广义提升算法。梯度推进是 GBM 的一个示例实现，它可以处理不同的损失函数，如回归、分类、风险建模等。顾名思义，这是一种通过梯度识别弱学习者缺点的 boosting 算法(AdaBoost 使用高权重数据点)，因此得名梯度 boosting。

*   迭代地将分类器 y <sub>m</sub> (x <sub>n</sub> 拟合到训练数据。初始模型将具有恒定值![$$ {\mathrm{y}}_0\left(\mathrm{x}\right)=\arg \kern0.375em \min \delta \sum \limits_{\mathrm{i}=1}^{\mathrm{n}}\mathrm{L}\left({\mathrm{y}}_{\mathrm{m}},\delta \right) $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq7.png)。

*   计算每个模型拟合迭代 g <sub>m</sub> (x)的损失(即预测值对实际值),或者计算负梯度并使用它来拟合新的基本学习函数 h <sub>m</sub> (x ),并找到最佳梯度下降步长![$$ {\delta}_{\mathrm{m}}=\arg \kern0.375em \min \delta \sum \limits_{\mathrm{i}=1}^{\mathrm{n}}\mathrm{L}\left({\mathrm{y}}_{\mathrm{m}},{\mathrm{y}}_{\mathrm{m}-1}\left(\mathrm{x}\right)+\delta\ {\mathrm{h}}_{\mathrm{m}}\left(\mathrm{x}\right)\ \right) $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq8.png)。

*   更新函数估计 y<sub>m</sub>(x)= y<sub>m1</sub>(x)+δh<sub>m</sub>(x)并输出 y <sub>m</sub> (x)。

清单 [4-16](#PC17) 显示了一个梯度增强分类器的示例代码实现。

```py
from sklearn.ensemble import GradientBoostingClassifier

# Using Gradient Boosting of 100 iterations
clf_GBT = GradientBoostingClassifier(n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train, y_train)
results = model_selection.cross_val_score(clf_GBT, X_train, y_train, cv=kfold)

print ("\nGradient Boosting - CV Train : %.2f" % results.mean())
print ("Gradient Boosting - Train : %.2f" % metrics.accuracy_score(clf_GBT.predict(X_train), y_train))
print ("Gradient Boosting - Test : %.2f" % metrics.accuracy_score(clf_GBT.predict(X_test), y_test))
#----output----
Gradient Boosting - CV Train : 0.66
Gradient Boosting - Train : 0.79
Gradient Boosting - Test : 0.75

Listing 4-16Gradient Boosting Classifier

```

让我们看看数字分类，以说明模型性能如何随着每次迭代而提高。

```py
from sklearn.ensemble import GradientBoostingClassifier

df= pd.read_csv('Data/digit.csv')

X = df.iloc[:,1:17].values
y = df['lettr'].values

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2019)

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2019)
num_trees = 10

clf_GBT = GradientBoostingClassifier(n_estimators=num_trees, learning_rate=0.1, random_state=2019).fit(X_train, y_train)
results = model_selection.cross_val_score(clf_GBT, X_train, y_train, cv=kfold)

print ("\nGradient Boosting - Train : %.2f" % metrics.accuracy_score(clf_GBT.predict(X_train), y_train))
print ("Gradient Boosting - Test : %.2f" % metrics.accuracy_score(clf_GBT.predict(X_test), y_test))

# Let's predict for the letter 'T' and understand how the prediction accuracy changes in each boosting iteration
X_valid= (2,8,3,5,1,8,13,0,6,6,10,8,0,8,0,8)
print ("Predicted letter: ", clf_GBT.predict([X_valid]))

# Staged prediction will give the predicted probability for each boosting iteration
stage_preds = list(clf_GBT.staged_predict_proba([X_valid]))
final_preds = clf_GBT.predict_proba([X_valid])

# Plot
x = range(1,27)
label = np.unique(df['lettr'])

plt.figure(figsize=(10,3))
plt.subplot(131)
plt.bar(x, stage_preds[0][0], align="center")
plt.xticks(x, label)
plt.xlabel('Label')
plt.ylabel('Prediction Probability')
plt.title('Round One')
plt.autoscale()

plt.subplot(132)
plt.bar(x, stage_preds[5][0],align='center')
plt.xticks(x, label)
plt.xlabel('Label')
plt.ylabel('Prediction Probability')
plt.title('Round Five')
plt.autoscale()

plt.subplot(133)
plt.bar(x, stage_preds[9][0],align='center')
plt.xticks(x, label)
plt.autoscale()
plt.xlabel('Label')
plt.ylabel('Prediction Probability')
plt.title('Round Ten')

plt.tight_layout()
plt.show()
#----output----
Gradient Boosting - Train :  0.75
Gradient Boosting - Test :  0.72
Predicted letter: 'T'

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figf_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figf_HTML.jpg)

梯度增强在后续迭代中纠正错误的增强迭代的负面影响。请注意，在第一次迭代中，字母“T”的预测概率为 0.25，到第十次迭代时逐渐增加到 0.76，而其他字母的概率百分比在每一轮中都有所下降。

### 升压—基本调谐参数

模型复杂性和过拟合可以通过使用两类参数的正确值来控制:

1.  树形结构

    *n_estimators* :这是要建立的弱学习器的数量。

    *max_depth* :这是单个估算器的最大深度。最佳值取决于输入变量的相互作用。

    *min_samples_leaf* :这将有助于确保 leaf 中有足够数量的样本结果。

    *子样本*:这是用于拟合单个模型的样本分数(默认值=1)。通常，0.8(80%)用于引入样本的随机选择，这反过来增加了对过拟合的稳健性。

2.  正则化参数

    *learning_rate* :控制估值器的变化幅度。学习率越低越好，这就需要更高的 n 估计量(这就是权衡)。

### Xgboost(极限梯度提升)

2014 年 3 月，Tianqui Chen 用 C++构建了 xgboost，作为分布式(深度)ML 社区的一部分，它有一个 Python 的接口。它是梯度推进算法的一个扩展的、更加规则的版本。这是表现最出色、大规模、可扩展的 ML 算法之一，在 Kaggle(预测建模和分析竞赛论坛)数据科学竞赛中赢得解决方案方面一直发挥着主要作用。

XGBoost 目标函数 obj(θ)=![$$ \sum \limits_{\mathrm{i}}^{\mathrm{n}}\mathrm{l}\left({\mathrm{y}}_{\mathrm{i}}-{\hat{\mathrm{y}}}_{\mathrm{i}}\right)+\sum \limits_{\mathrm{k}=1}^{\mathrm{K}}\varOmega \left({\mathrm{f}}_{\mathrm{k}}\right) $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq9.png)

正则项由下式给出

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figg_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figg_HTML.jpg)

梯度下降技术用于优化目标函数，关于算法的更多数学知识可在 [`http://xgboost.readthedocs.io/en/latest/`](http://xgboost.readthedocs.io/en/latest/) 网站找到。

xgboost 算法的一些主要优点是

*   它实现了并行处理。

*   它有一个处理缺失值的内置标准，这意味着用户可以指定一个不同于其他观察值的特定值(如-1 或-999)，并将其作为参数传递。

*   它会将树分割到最大深度，这与梯度增强不同，梯度增强在分割中遇到负损失时会停止分割节点。

XGboost 有一组参数，在较高层次上，我们可以将它们分为三类。让我们看看这些类别中最重要的。

1.  一般参数
    1.  *nthread* :并行线程数；如果没有给定值，将使用所有内核。

    2.  *Booster* :这是要运行的模型类型，默认为 gbtree(基于树的模型)。“gblinear”用于线性模型

2.  增压参数
    1.  *eta* :这是防止过拟合的学习率或步长收缩；默认值为 0.3，范围在 0 和 1 之间。

    2.  *max_depth* :树的最大深度，默认为 6

    3.  *min_child_weight* :一个孩子需要的所有观察的最小权重之和。从事件率的 1/平方根开始。

    4.  *colsample_bytree* :对每棵树随机抽样的列的分数，默认值为 1。

    5.  *子样本*:每棵树随机抽样的一部分观察值，默认值为 1。降低该值会使算法变得保守，以避免过度拟合。

    6.  *lambda* :关于权重的 L2 正则化项，默认值为 1

    7.  *alpha* :权重上的 L1 正则项

3.  任务参数
    1.  *目标*:定义要最小化的损失函数，默认值为“reg: linear”对于二进制分类，它应该是“二进制:逻辑”,对于多类，它应该是“多:softprob”以获得概率值，而“多:softmax”以获得预测类。对于多类，将指定 num_class(唯一类的数量)。

    2.  *eval_metric* :用于验证模型性能的指标

sklearn 有一个 xgboost (XGBClassifier)的包装器。让我们继续使用糖尿病数据集，并使用弱学习者建立一个模型(清单 [4-17](#PC19) )。

```py
import xgboost as xgb
from xgboost.sklearn import XGBClassifier

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

predictors = ['age','serum_insulin']
target = 'class'

# Most common preprocessing step include label encoding and missing value treatment
from sklearn import preprocessing
for f in df.columns:
    if df[f].dtype=='object':
        lbl = preprocessing.LabelEncoder()
        lbl.fit(list(df[f].values))
        df[f] = lbl.transform(list(df[f].values))

df.fillna((-999), inplace=True)

# Let's use some week features to build the tree
X = df[['age','serum_insulin']] # independent variables
y = df['class'].values          # dependent variables

#Normalize
X = StandardScaler().fit_transform(X)

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2017)
num_rounds = 100

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=2017)

clf_XGB = XGBClassifier(n_estimators = num_rounds,
                        objective= 'binary:logistic',
                        seed=2017)

# use early_stopping_rounds to stop the cv when there is no score imporovement
clf_XGB.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test, y_test)], verbose=False)

results = model_selection.cross_val_score(clf_XGB, X_train,y_train, cv=kfold)
print ("\nxgBoost - CV Train : %.2f" % results.mean())
print ("xgBoost - Train : %.2f" % metrics.accuracy_score(clf_XGB.predict(X_train), y_train))
print ("xgBoost - Test : %.2f" % metrics.accuracy_score(clf_XGB.predict(X_test), y_test))
#----output----
xgBoost - CV Train : 0.69
xgBoost - Train : 0.73
xgBoost - Test : 0.74

Listing 4-17xgboost Classifier Using sklearn Wrapper

```

现在让我们看看如何使用 xgboost 原生接口构建一个模型。用于输入数据的 xgboostfor 的内部数据结构。将大型数据集转换为 DMatrix 对象以节省预处理时间是一个很好的做法(清单 [4-18](#PC20) )。

```py
xgtrain = xgb.DMatrix(X_train, label=y_train, missing=-999)
xgtest = xgb.DMatrix(X_test, label=y_test, missing=-999)

# set xgboost params
param = {'max_depth': 3,  # the maximum depth of each tree
         'objective': 'binary:logistic'}

clf_xgb_cv = xgb.cv(param, xgtrain, num_rounds,
                    stratified=True,
                    nfold=5,
                    early_stopping_rounds=20,
                    seed=2017)

print ("Optimal number of trees/estimators is %i" % clf_xgb_cv.shape[0])

watchlist  = [(xgtest,'test'), (xgtrain,'train')]
clf_xgb = xgb.train(param, xgtrain,clf_xgb_cv.shape[0], watchlist)

# predict function will produce the probability

# so we'll use 0.5 cutoff to convert probability to class label
y_train_pred = (clf_xgb.predict(xgtrain, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)
y_test_pred = (clf_xgb.predict(xgtest, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)

print ("XGB - Train : %.2f" % metrics.accuracy_score(y_train_pred, y_train))
print ("XGB - Test : %.2f" % metrics.accuracy_score(y_test_pred, y_test))

Listing 4-18xgboost Using It’s Native Python Package Code

```

# -输出-

```py
Optimal number of trees (estimators) is 6
[0]    test-error:0.344156    train-error:0.299674
[1]    test-error:0.324675    train-error:0.273616
[2]    test-error:0.272727    train-error:0.281759
[3]    test-error:0.266234    train-error:0.278502
[4]    test-error:0.266234    train-error:0.273616
[5]    test-error:0.311688    train-error:0.254072
XGB - Train : 0.73
XGB - Test : 0.73

```

### 集合投票——机器学习最大的英雄联盟

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig13_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig13_HTML.jpg)

图 4-13

合奏:ML 最大的英雄联盟

投票分类器使我们能够通过来自不同类型的多个 ML 算法的多数投票来组合预测，不像 Bagging/Boosting，其中相似类型的多个分类器用于多数投票。

首先，您可以从训练数据集创建多个独立模型。然后，当要求对新数据进行预测时，可以使用投票分类器来包装您的模型，并对子模型的预测进行平均。子模型的预测可以被加权，但是手动地或者甚至启发式地指定分类器的权重是困难的。更高级的方法可以学习如何对子模型的预测进行最佳加权，但这被称为堆叠(stacked aggregation ),目前 Scikit-learn 中没有提供。

让我们在 Pima 糖尿病数据集上构建单独的模型，并尝试投票分类器，以结合模型结果来比较准确性的变化(清单 [4-19](#PC22) )。

```py
import pandas as pd
import numpy as np

# set seed for reproducability
np.random.seed(2017)

import statsmodels.api as sm
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import GradientBoostingClassifier

# currently its available as part of mlxtend and not sklearn
from mlxtend.classifier import EnsembleVoteClassifier
from sklearn import model_selection
from sklearn import metrics
from sklearn.model_selection import train_test_split

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

X = df.iloc[:,:8]     # independent variables
y = df['class']       # dependent variables

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2017)

LR = LogisticRegression(random_state=2017)
RF = RandomForestClassifier(n_estimators = 100, random_state=2017)
SVM = SVC(random_state=0, probability=True)
KNC = KNeighborsClassifier()
DTC = DecisionTreeClassifier()
ABC = AdaBoostClassifier(n_estimators = 100)
BC = BaggingClassifier(n_estimators = 100)
GBC = GradientBoostingClassifier(n_estimators = 100)

clfs = []
print('5-fold cross validation:\n')
for clf, label in zip([LR, RF, SVM, KNC, DTC, ABC, BC, GBC],
                      ['Logistic Regression',
                       'Random Forest',
                       'Support Vector Machine',
                       'KNeighbors',
                       'Decision Tree',
                       'Ada Boost',
                       'Bagging',
                       'Gradient Boosting']):
    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring="accuracy") 

    print("Train CV Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    md = clf.fit(X, y)
    clfs.append(md)
    print("Test Accuracy: %0.2f " % (metrics.accuracy_score(clf.predict(X_test), y_test)))
#----output----
5-fold cross validation:

Train CV Accuracy: 0.76 (+/- 0.03) [Logistic Regression]
Test Accuracy: 0.79
Train CV Accuracy: 0.74 (+/- 0.03) [Random Forest]
Test Accuracy: 1.00
Train CV Accuracy: 0.65 (+/- 0.00) [Support Vector Machine]
Test Accuracy: 1.00
Train CV Accuracy: 0.70 (+/- 0.05) [KNeighbors]
Test Accuracy: 0.84
Train CV Accuracy: 0.69 (+/- 0.02) [Decision Tree]
Test Accuracy: 1.00
Train CV Accuracy: 0.73 (+/- 0.04) [Ada Boost]
Test Accuracy: 0.83
Train CV Accuracy: 0.75 (+/- 0.04) [Bagging]
Test Accuracy: 1.00
Train CV Accuracy: 0.75 (+/- 0.03) [Gradient Boosting]
Test Accuracy: 0.92

Listing 4-19
Ensemble Model

```

从之前的基准测试中我们可以看出，与其他模型相比，“逻辑回归”、“随机森林”、“Bagging”和 Ada/梯度提升算法具有更好的准确性。让我们结合非相似模型，如逻辑回归(基础模型)、随机森林(bagging 模型)和梯度推进(boosting 模型)来创建一个健壮的通用模型。

#### 硬投票与软投票

多数投票也被称为硬投票。预测概率之和的 argmax 被称为软投票。参数“权重”可用于为分类器分配特定权重。每个分类器的预测分类概率乘以分类器权重并进行平均。然后，从最高平均概率类别标签中导出最终类别标签。

假设我们给所有的分类器分配一个相等的权重 1(表 [4-1](#Tab1) )。基于软投票，预测的类别标签是 1，因为它具有最高的平均概率。集合投票模型的示例代码实现参见清单 [4-20](#PC23) 。

表 4-1

软投票

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figh_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figh_HTML.jpg)

### 注意

Scikit-learn 的一些分类器不支持 predict_proba 方法。

```py
# ### Ensemble Voting
clfs = []
print('5-fold cross validation:\n')

ECH = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting="hard")
ECS = EnsembleVoteClassifier(clfs=[LR, RF, GBC], voting="soft", weights=[1,1,1])

for clf, label in zip([ECH, ECS],
                      ['Ensemble Hard Voting',
                       'Ensemble Soft Voting']):
    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=5, scoring="accuracy")
    print("Train CV Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
    md = clf.fit(X, y)
    clfs.append(md)
    print("Test Accuracy: %0.2f " % (metrics.accuracy_score(clf.predict(X_test), y_test)))
#----output----
5-fold cross validation:

Train CV Accuracy: 0.75 (+/- 0.02) [Ensemble Hard Voting]
Test Accuracy: 0.93
Train CV Accuracy: 0.76 (+/- 0.02) [Ensemble Soft Voting] 

Test Accuracy: 0.95

Listing 4-20Ensemble Voting Model

```

### 堆垛

David h . WOL pert(1992 年)在他与 *Neural Networks* journal 发表的文章中提出了堆叠泛化的概念，通常被称为“堆叠”。在堆叠中，最初在训练/测试数据集上训练不同类型的多个基础模型。理想的情况是混合使用不同的模型(kNN、bagging、boosting 等)。)这样他们就可以了解问题的某一部分。在第 1 级，使用基本模型的预测值作为特征，并训练一个模型，该模型称为元模型。因此，组合单个模型的学习将导致提高的准确性。这是一个简单的一级堆叠，同样，您可以堆叠不同类型的模型的多个级别(图 [4-14](#Fig14) )。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig14_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig14_HTML.jpg)

图 4-14

简单的二级堆叠模型

让我们应用之前在糖尿病数据集上讨论的堆叠概念，并比较基本模型与元模型的准确性(清单 [4-21](#PC24) )。

```py
# Classifiers
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

seed = 2019

np.random.seed(seed)  # seed to shuffle the train set

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

X = df.iloc[:,0:8] # independent variables
y = df['class'].values     # dependent variables

#Normalize
X = StandardScaler().fit_transform(X)

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=seed)
num_trees = 10
verbose = True # to print the progress

clfs = [KNeighborsClassifier(),
        RandomForestClassifier(n_estimators=num_trees, random_state=seed),
        GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)]

# Creating train and test sets for blending
dataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))
dataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))

print('5-fold cross validation:\n')
for i, clf in enumerate(clfs):
    scores = model_selection.cross_val_score(clf, X_train, y_train, cv=kfold, scoring="accuracy")
    print("##### Base Model %0.0f #####" % i)
    print("Train CV Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std()))
    clf.fit(X_train, y_train)
    print("Train Accuracy: %0.2f " % (metrics.accuracy_score(clf.predict(X_train), y_train)))
    dataset_blend_train[:,i] = clf.predict_proba(X_train)[:, 1]
    dataset_blend_test[:,i] = clf.predict_proba(X_test)[:, 1]
    print("Test Accuracy: %0.2f " % (metrics.accuracy_score(clf.predict(X_test), y_test)))

print ("##### Meta Model #####")

clf = LogisticRegression()
scores = model_selection.cross_val_score(clf, dataset_blend_train, y_train, cv=kfold, scoring="accuracy")
clf.fit(dataset_blend_train, y_train)
print("Train CV Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std()))
print("Train Accuracy: %0.2f " % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_train)))
print("Test Accuracy: %0.2f " % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_test)))
#----output----
5-fold cross validation:

##### Base Model 0 #####
Train CV Accuracy: 0.71 (+/- 0.03)
Train Accuracy: 0.83
Test Accuracy: 0.75
##### Base Model 1 #####
Train CV Accuracy: 0.73 (+/- 0.02)
Train Accuracy: 0.98
Test Accuracy: 0.79
##### Base Model 2 #####
Train CV Accuracy: 0.74 (+/- 0.01)
Train Accuracy: 0.80
Test Accuracy: 0.80
##### Meta Model #####
Train CV Accuracy: 0.99 (+/- 0.02)
Train Accuracy: 0.99
Test Accuracy: 0.77 

Listing 4-21Model Stacking

```

## 超参数调谐

ML 过程中的主要目标和挑战之一是基于数据模式和观察到的证据来提高性能分数。为了实现这一目标，几乎所有的 ML 算法都有一组特定的参数，需要从数据集进行估计，这将使性能得分最大化。假设这些参数是您需要调整到不同值的旋钮，以找到使您获得最佳模型精度的最佳参数组合(图 [4-15](#Fig15) )。选择一个好的超参数的最好方法是通过试错所有可能的参数值组合。Scikit-learn 提供 GridSearchCV 和 RandomSearchCV 函数，以促进超参数调整的自动化和可重复方法。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig15_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig15_HTML.jpg)

图 4-15

超参数调谐

### 网格搜索

对于给定的模型，您可以定义一组想要尝试的参数值。然后，使用 Scikit-learn 的 GridSearchCV 功能，为您提供的超参数值预设列表的所有可能组合构建模型，并根据交叉验证分数选择最佳组合。GridSearchCV 有两个缺点:

1.  *计算量大*:很明显，参数值越多，网格搜索的计算量就越大。考虑一个例子，其中有五个参数，假设您想为每个参数尝试五个值，这将导致 5∫5 = 3，125 个组合。进一步乘以所使用的交叉验证折叠数(例如，如果 k-fold 为 5，则 3125÷5 = 15，625 个模型拟合)。

2.  *不完全最优但接近最优的参数* : GridSearch 将查看您为数字参数提供的固定点，因此很有可能会错过位于固定点之间的最优点。例如，假设您想要尝试决策树模型的‘n _ estimators’:[100，250，500，750，1000]的固定点，并且最优点可能位于两个固定点之间。然而，GridSearch 并不是为了在固定点之间进行搜索而设计的。

让我们在 Pima 糖尿病数据集上为 RandomForest 分类器尝试 GridSearchCV，以找到最佳参数值(清单 [4-22](#PC25) )。

```py
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
seed = 2017

# read the data in
df = pd.read_csv("Data/Diabetes.csv")

X = df.iloc[:,:8].values     # independent variables
y = df['class'].values       # dependent variables

#Normalize
X = StandardScaler().fit_transform(X)

# evaluate the model by splitting into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)

kfold = model_selection.StratifiedKFold(n_splits=5, random_state=seed)
num_trees = 100

clf_rf = RandomForestClassifier(random_state=seed).fit(X_train, y_train)

rf_params = {
    'n_estimators': [100, 250, 500, 750, 1000],
    'criterion':  ['gini', 'entropy'],
    'max_features': [None, 'auto', 'sqrt', 'log2'],
    'max_depth': [1, 3, 5, 7, 9]
}

# setting verbose = 10 will print

the progress for every 10 task completion
grid = GridSearchCV(clf_rf, rf_params, scoring="roc_auc", cv=kfold, verbose=10, n_jobs=-1)
grid.fit(X_train, y_train)

print ('Best Parameters: ', grid.best_params_)

results = model_selection.cross_val_score(grid.best_estimator_, X_train,y_train, cv=kfold)
print ("Accuracy - Train CV: ", results.mean())
print ("Accuracy - Train : ", metrics.accuracy_score(grid.best_estimator_.predict(X_train), y_train))
print ("Accuracy - Test : ", metrics.accuracy_score(grid.best_estimator_.predict(X_test), y_test))
#----output----
Fitting 5 folds for each of 200 candidates, totalling 1000 fits
Best Parameters:  {'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 500}
Accuracy - Train CV:  0.7447905849775008
Accuracy - Train :  0.8621973929236499
Accuracy - Test :  0.7965367965367965

Listing 4-22Grid Search

for Hyperparameter Tuning

```

### 随机搜索

顾名思义，RandomSearch 算法尝试给定参数的一系列值的随机组合。数字参数可以指定为一个范围(与 GridSearch 中的固定值不同)。您可以控制想要执行的随机搜索的迭代次数。众所周知，与 GridSearch 相比，可以在更短的时间内找到非常好的组合；但是，您必须仔细选择参数的范围和随机搜索迭代的次数，因为它可能会错过迭代次数较少或范围较小的最佳参数组合。

让我们使用与 GridSearch 相同的组合来尝试 RandomSearchCV，并比较时间/准确性(清单 [4-23](#PC26) )。

```py
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint as sp_randint

# specify parameters and distributions to sample from
param_dist = {'n_estimators':sp_randint(100,1000),
              'criterion': ['gini', 'entropy'],
              'max_features': [None, 'auto', 'sqrt', 'log2'],
              'max_depth': [None, 1, 3, 5, 7, 9]
             }

# run randomized search
n_iter_search = 20
random_search = RandomizedSearchCV(clf_rf, param_distributions=param_dist, cv=kfold, n_iter=n_iter_search, verbose=10, n_jobs=-1, random_state=seed)

random_search.fit(X_train, y_train)
# report(random_search.cv_results_)

print ('Best Parameters: ', random_search.best_params_)

results = model_selection.cross_val_score(random_search.best_estimator_, X_train,y_train, cv=kfold)
print ("Accuracy - Train CV: ", results.mean())
print ("Accuracy - Train : ", metrics.accuracy_score(random_search.best_estimator_.predict(X_train), y_train))
print ("Accuracy - Test : ", metrics.accuracy_score(random_search.best_estimator_.predict(X_test), y_test))
#----output----
Fitting 5 folds for each of 20 candidates, totalling 100 fits

Best Parameters:  {'criterion': 'entropy', 'max_depth': 3, 'max_features': None, 'n_estimators': 694}
Accuracy - Train CV:  0.7542402215299411

Accuracy - Train :  0.7802607076350093
Accuracy - Test :  0.8051948051948052

Listing 4-23Random Search for Hyperparameter Tuning

```

请注意，在这种情况下，使用 RandomSearchCV，我们能够以 100 次拟合获得与 GridSearchCV 的 1000 次拟合相当的精度结果。

图 [4-16](#Fig16) 是两个参数之间网格搜索与随机搜索结果差异的示例说明(不是实际表示)。假设 max_depth 的最佳区域位于 3 和 5 之间(蓝色阴影)，n_estimators 的最佳区域位于 500 和 700 之间(琥珀色阴影)。组合参数的理想最佳值将位于各个区域的相交处。这两种方法都能够找到接近最优的参数，而不一定是完美的最佳点。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig16_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig16_HTML.jpg)

图 4-16

网格搜索与随机搜索

### 贝叶斯优化

一个关键的新兴超参数调整技术是贝叶斯优化，使用参数及其相关目标值的观测融合的高斯过程回归。贝叶斯优化的目标是在尽可能少的迭代中找到未知函数的最大值。与网格和随机搜索相比，关键区别在于空间对于每个超参数具有概率分布，而不是离散值。这种技术特别适合于高成本函数的优化，在这种情况下，勘探和开发之间的平衡非常重要。虽然这种技术对连续变量很有效，但是没有直观的方法来处理离散参数。参考清单 [4-24](#PC27) 中随机搜索超参数调整的简单代码实现示例。

您可以在 [`https://github.com/fmfn/BayesianOptimization`](https://github.com/fmfn/BayesianOptimization) 了解更多关于包装和示例的信息

```py
# pip install bayesian-optimization
from bayes_opt import BayesianOptimization
from sklearn.model_selection import cross_val_score
from bayes_opt.util import Colours
from sklearn.ensemble import RandomForestClassifier as RFC

def rfc_cv(n_estimators, min_samples_split, max_features, data, targets):
    """Random Forest cross validation.
    This function will instantiate a random forest classifier with parameters
    n_estimators, min_samples_split, and max_features. Combined with data and
    targets this will in turn be used to perform cross-validation. The result of cross validation is returned. Our goal is to find combinations of n_estimators, min_samples_split, and
    max_features that minimzes the log loss.
    """
    estimator = RFC(
        n_estimators=n_estimators,
        min_samples_split=min_samples_split,
        max_features=max_features,
        random_state=2
    )
    cval = cross_val_score(estimator, data, targets,
                           scoring='neg_log_loss', cv=4)
    return cval.mean()

def optimize_rfc(data, targets):
    """Apply Bayesian Optimization to Random Forest parameters."""
    def rfc_crossval(n_estimators, min_samples_split, max_features):
        """Wrapper of RandomForest cross validation.
        Notice how we ensure n_estimators and min_samples_split are casted

        to integer before we pass them along. Moreover, to avoid max_features
        taking values outside the (0, 1) range, we also ensure it is capped
        accordingly.
        """
        return rfc_cv(
            n_estimators=int(n_estimators),
            min_samples_split=int(min_samples_split),
            max_features=max(min(max_features, 0.999), 1e-3),
            data=data,
            targets=targets,
        )

    optimizer = BayesianOptimization(
        f=rfc_crossval,
        pbounds={
            "n_estimators": (10, 250),
            "min_samples_split": (2, 25),
            "max_features": (0.1, 0.999),
        },
        random_state=1234,
        verbose=2
    )
    optimizer.maximize(n_iter=10)

    print("Final result:", optimizer.max)
    return optimizer

print(Colours.green("--- Optimizing Random Forest ---"))
optimize_rfc(X_train, y_train)
#----output----

--- Optimizing Random Forest ---
|   iter    |  target   | max_fe... | min_sa... | n_esti... |
-------------------------------------------------------------
|  1        | -0.5112   |  0.2722   |  16.31    |  115.1    |
|  2        | -0.5248   |  0.806    |  19.94    |  75.42    |
|  3        | -0.5075   |  0.3485   |  20.44    |  240.0    |
|  4        | -0.528    |  0.8875   |  10.23    |  130.2    |
|  5        | -0.5098   |  0.7144   |  18.39    |  98.86    |
|  6        | -0.51     |  0.999    |  25.0     |  176.7    |
|  7        | -0.5113   |  0.7731   |  24.94    |  249.8    |
|  8        | -0.5339   |  0.999    |  2.0      |  250.0    |
|  9        | -0.5107   |  0.9023   |  24.96    |  116.2    |
|  10       | -0.8284   |  0.1065   |  2.695    |  10.04    |
|  11       | -0.5235   |  0.1204   |  24.89    |  208.1    |
|  12       | -0.5181   |  0.1906   |  2.004    |  81.15    |
|  13       | -0.5203   |  0.1441   |  2.057    |  185.3    |
|  14       | -0.5257   |  0.1265   |  24.85    |  153.1    |
|  15       | -0.5336   |  0.9906   |  2.301    |  219.3    |
=============================================================
Final result: {'target': -0.5075247858575866, 'params': {'max_features': 0.34854136537364394, 'min_samples_split': 20.443060083305443, 'n_estimators': 239.95344488408924}}

Listing 4-24Random Search for Hyperparameter Tuning

```

### 时序物联网数据的降噪

在过去的十年里，技术的软件和硬件方面都有了巨大的增长，这催生了互联世界或物联网(IoT)的概念。这意味着物理设备、日常物品和硬件形式安装有微型传感器，以不同参数的形式持续捕获机器状态，并扩展到互联网连接，以便这些设备能够相互通信/交互，并可以远程监控/控制。预测分析是处理挖掘物联网数据以获得洞察力的 ML 领域。概括来说，有两个关键方面。一个是异常检测，这是一个尽早识别前兆故障特征(异常行为，也称为异常)的过程，以便可以计划必要的措施来避免阻碍故障，例如设备电压或温度的突然升高或降低。另一个是剩余使用寿命(RUL)预测。这是一个从异常检测的角度预测 RUL 或离即将发生的故障有多远的过程。我们可以使用回归模型来预测 RUL。

我们从传感器收集的数据集容易受到噪声的影响，尤其是高带宽测量，如振动或超声信号。傅立叶变换是一种众所周知的技术，它允许我们在频域或频谱域进行分析，以获得对高带宽信号(如振动测量曲线)的更深入了解。傅立叶是一系列正弦波，傅立叶变换实质上是将信号分解成单独的正弦波分量。然而，傅立叶变换的缺点是，如果信号的频谱分量随时间快速变化，它不能提供局部信息。傅立叶变换的主要缺点是，一旦信号从时域变换到频域，所有与时间相关的信息都会丢失。小波变换解决了傅里叶变换的主要缺点，是高带宽信号处理的理想选择。有大量的文献解释小波变换及其应用，所以在本书中你不会得到太多的细节。本质上，小波可以用来将信号分解成一系列系数。前几个系数代表最低频率，后几个系数代表最高频率。通过移除较高频率的系数，然后用截断的系数重构信号，我们可以平滑信号，而不用像移动平均那样平滑所有感兴趣的峰值。

小波变换将时间序列信号分解为两部分，一部分是低频或低通滤波器，用于平滑原始信号近似值，另一部分是高频或高通滤波器，用于产生详细的局部特性，例如异常，如图 [4-17](#Fig17) 所示。

![../images/434293_2_En_4_Chapter/434293_2_En_4_Fig17_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Fig17_HTML.jpg)

图 4-17。

小波分解的滤波概念

小波变换的优势之一是通过迭代获得多分辨率来执行多级分解过程的能力，其中近似值依次被连续分解，使得一个信号可以被分解成许多较低分辨率的分量。小波变换函数 f(x)是一个数列，其中称为母小波函数的小波基ψ用于分解。

![$$ f(x)=\frac{1}{\sqrt{M}}\ \sum \limits_k{W}_{\phi}\left({j}_0,k\right){\phi}_{j_0,k}(x)+\frac{1}{\sqrt{M}}\ \sum \limits_{j={j}_0}^{\infty}\sum \limits_k{W}_{\psi}\left(j,k\right){\psi}_{j,k}(x) $$](../images/434293_2_En_4_Chapter/434293_2_En_4_Chapter_TeX_IEq10.png)

其中， *j* <sub>*0*</sub> 是被称为近似或缩放系数的任意起始比例。*W*<sub>*ψ*</sub>*(j，k)称为细节或小波系数。*

```py
import pywt
from statsmodels.robust import mad
import pandas as pd
import numpy as np

df = pd.read_csv('Data/Temperature.csv')

# Function to denoise the sensor data using wavelet transform
def wp_denoise(df):
    for column in df:
        x = df[column]
        wp = pywt.WaveletPacket(data=x, wavelet="db7", mode="symmetric")
        new_wp = pywt.WaveletPacket(data=None, wavelet="db7", mode="sym")
        for i in range(wp.maxlevel):
            nodes = [node.path for node in wp.get_level(i, 'natural')]
            # Remove the high and low pass signals
            for node in nodes:
                sigma = mad(wp[node].data)
                uthresh = sigma * np.sqrt( 2*np.log( len( wp[node].data ) ) )
                new_wp[node] = pywt.threshold(wp[node].data, value=uthresh, mode="soft")
        y = new_wp.reconstruct(update=False)[:len(x)]
        df[column] = y
    return df

# denoise the sensor data
df_denoised = wp_denoise(df.iloc[:,3:4])
df['Date'] = pd.to_datetime(df['Date'])

plt.figure(1)
ax1 = plt.subplot(221)
df['4030CFDC'].plot(ax=ax1, figsize=(8, 8), title='Signal with noise')

ax2 = plt.subplot(222)
df_denoised['4030CFDC'].plot(ax=ax2, figsize=(8, 8), title='Signal without noise')
plt.tight_layout()
#----output----

Listing 4-25Wavelet Transform Implementation

```

![../images/434293_2_En_4_Chapter/434293_2_En_4_Figi_HTML.jpg](../images/434293_2_En_4_Chapter/434293_2_En_4_Figi_HTML.jpg)

## 摘要

在这一步中，我们已经了解了可能妨碍模型准确性的各种常见问题，例如没有为类创建、方差和偏差选择最佳概率截止点。我们还简要介绍了不同的模型调整技术，如 bagging、boosting、集成投票、网格搜索/随机搜索和贝叶斯优化技术，用于超参数调整。我们还研究了物联网数据的降噪技术。简而言之，我们只看了所讨论的每个主题中最重要的方面，以帮助您入门。然而，每种算法都有更多的优化选项，而且每种技术都在快速发展。所以我鼓励你留意他们各自的官方托管网页和 GitHub 资源库(表 [4-2](#Tab2) )。

表 4-2

额外资源

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

名字

 | 

网页

 | 

Github 知识库

 |
| --- | --- | --- |
| Scikit-learn | [T2`http://scikit-learn.org/stable/#`](http://scikit-learn.org/stable/) | [T2`https://github.com/scikit-learn/scikit-learn`](https://github.com/scikit-learn/scikit-learn) |
| Xgboost | [T2`https://xgboost.readthedocs.io/en/latest/`](https://xgboost.readthedocs.io/en/latest/) | [T2`https://github.com/dmlc/xgboost`](https://github.com/dmlc/xgboost) |
| 贝叶斯优化 | 不适用的 | [T2`https://github.com/fmfn/BayesianOptimization`](https://github.com/fmfn/BayesianOptimization) |
| 小波变换 | [T2`https://pywavelets.readthedocs.io/en/latest/#`](https://pywavelets.readthedocs.io/en/latest/) | [T2`https://github.com/PyWavelets/pywt`](https://github.com/PyWavelets/pywt) |

我们已经到达了步骤 4 的末尾，这意味着您已经通过了机器学习旅程的一半。在下一章，我们将学习文本挖掘技术。