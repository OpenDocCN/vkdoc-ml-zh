# 五、文本挖掘和推荐系统

人工智能的一个关键领域是自然语言处理(NLP)，或众所周知的文本挖掘，它涉及教计算机如何从文本中提取意义。在过去的 20 年里，随着互联网世界的爆炸和社交媒体的兴起，大量有价值的数据以文本的形式产生。从文本数据中挖掘出有意义的模式的过程称为文本挖掘。本章概述了高级文本挖掘过程、关键概念和涉及的常用技术。

除了 Scikit-learn 之外，还有许多已建立的面向 NLP 的库可供 Python 使用，而且数量还在不断增加。表 [5-1](#Tab1) 根据截至 2016 年的贡献者数量列出了最受欢迎的图书馆。

表 5-1

流行的 Python 文本挖掘库

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"> <col class="tcol4 align-left"></colgroup> 
| 

包名

 | 

贡献者数量(2019 年)

 | 

许可证

 | 

描述

 |
| --- | --- | --- | --- |
| 我是 NLTK | Two hundred and fifty-five | 街头流氓 | 它是最流行和最广泛使用的工具包，主要用于支持 NLP 的研究和开发 |
| 玄诗 | Three hundred and eleven | LGPL-2 突击步枪 | 主要用于大型语料库的主题建模、文档索引和相似性检索 |
| 宽大的 | Three hundred | 用它 | 使用 Python + Cython 构建，用于 NLP 概念的高效生产实现 |
| 文本 blob | Thirty-six | 用它 | 它是 NLTK 和模式库的包装器，便于访问它们的功能。适合快速原型制作 |
| 懂得多种语言的 | Twenty-two | GPL-3 | 这是一个多语言文本处理工具包，支持大量多语言应用程序。 |
| 模式 | Nineteen | BSD-3 | 这是一个 Python 的 web 挖掘模块，具有抓取、NLP、机器学习和网络分析/可视化功能。 |

### 注意

另一个著名的库是 Stanford CoreNLP，这是一套基于 Java 的工具包。有许多 Python 包装器可用于同样的目的；然而，到目前为止，这些包装器的贡献者数量还很少。

## 文本挖掘过程概述

整个文本挖掘过程可以大致分为以下四个阶段，如图 [5-1](#Fig1) 所示:

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig1_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig1_HTML.jpg)

图 5-1

文本挖掘过程概述

1.  文本数据汇编

2.  文本数据预处理

3.  数据探索或可视化

4.  模型结构

## 数据汇编(文本)

据观察，任何企业都有 70%的可用数据是非结构化的。第一步是整理来自不同来源(如开放式反馈)的非结构化数据；电话；电子邮件支持；在线聊天；以及 Twitter、LinkedIn 和脸书等社交媒体网络。汇集这些数据并应用挖掘/机器学习(ML)技术来分析它们，为组织提供了在客户体验中构建更多力量的宝贵机会。

有几个库可用于从所讨论的不同格式中提取文本内容。到目前为止，为多种格式提供简单、单一接口的最好的库是“textract”(开源 MIT 许可证)。请注意，到目前为止，这个库/包适用于 Linux 和 Mac OS，但不适用于 Windows。表 [5-2](#Tab2) 列出了支持的格式。

表 5-2

textract 支持的格式

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"> <col class="tcol3 align-left"></colgroup> 
| 

格式

 | 

支持方式

 | 

附加信息

 |
| --- | --- | --- |
| 。csv /.eml /。json /。odt /。txt / | Python 内置 |   |
| 。文件 | 反词 | [T2`www.winfield.demon.nl/`](http://www.winfield.demon.nl/) |
| 。文档 | Python-docx | [T2`https://python-docx.readthedocs.io/en/latest/`](https://python-docx.readthedocs.io/en/latest/) |
| 。电子书 | 电子书 | [T2`https://github.com/aerkalov/ebooklib`](https://github.com/aerkalov/ebooklib) |
| 。gif /.jpg /。jpeg /。png /。tiff /。tif(基准) | tessera CT ocr | [T2`https://github.com/tesseract-ocr`](https://github.com/tesseract-ocr) |
| 。html /。html 文件的后缀 | Beautifulsoup4 | [T2`http://beautiful-soup-4.readthedocs.io/en/latest/`](http://beautiful-soup-4.readthedocs.io/en/latest/) |
| . mp3 / .ogg / .wav 档案 | 演讲识别和 sox | URL 1:URL 2: [`http://sox.sourceforge.net/`](http://sox.sourceforge.net/) |
| 。味精 | 味精提取器 | [T2`https://github.com/mattgwwalker/msg-extractor`](https://github.com/mattgwwalker/msg-extractor) |
| 。可移植文档格式文件的扩展名（portable document format 的缩写） | pdftotext 和 pdfminer.six | URL 1:URL 2: [`https://github.com/pdfminer/pdfminer.six`](https://github.com/pdfminer/pdfminer.six) |
| 。附 | Python-pptx | [T2`https://python-pptx.readthedocs.io/en/latest/`](https://python-pptx.readthedocs.io/en/latest/) |
| 。著名图象处理软件 | PS2 文本 | [T2`http://pages.cs.wisc.edu/~ghost/doc/pstotext.htm`](http://pages.cs.wisc.edu/%257Eghost/doc/pstotext.htm) |
| 。普适文本格式 | Unrtf | [T2`www.gnu.org/software/unrtf/`](http://www.gnu.org/software/unrtf/) |
| . xlsx / .xls 档 | xlrd . xlrd . xlrd . xlrd . xlrd . xlrd . xlrd . xlrd . xlrd | [T2`https://pypi.python.org/pypi/xlrd`](https://pypi.python.org/pypi/xlrd) |

让我们看看商业世界中最普遍的格式的代码:pdf、jpg 和音频文件(清单 [5-1](#PC1) )。注意，从其他格式中提取文本也相对简单。

```py
# You can read/learn more about latest updates about textract on their official documents site at http://textract.readthedocs.io/en/latest/
import textract

# Extracting text from normal pdf
text = textract.process('Data/PDF/raw_text.pdf', language="eng")

# Extrcting text from two columned pdf
text = textract.process('Data/PDF/two_column.pdf', language="eng")

# Extracting text from scanned text pdf

text = textract.process('Data/PDF/ocr_text.pdf', method="tesseract", language="eng")

# Extracting text from jpg
text = textract.process('Data/jpg/raw_text.jpg', method="tesseract", language="eng")

# Extracting text from audio file
text = textract.process('Data/wav/raw_text.wav', language="eng")

Listing 5-1Example Code for Extracting Data from pdf, jpg, Audio

```

### 社会化媒体

你知道吗，在线新闻和社交网络服务提供商 Twitter 拥有 3.2 亿用户，平均每天有 4200 万条活跃推文！*(来源:smart insights 2016 年全球社交媒体研究总结)*

让我们了解如何探索社交媒体的丰富信息(我将考虑 Twitter 作为一个例子)，以探索关于一个选定的主题正在谈论什么(图 [5-2](#Fig2) )。大多数论坛都为开发者提供了访问帖子的 API。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig2_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig2_HTML.jpg)

图 5-2

提取 Twitter 帖子进行分析

步骤 1—获取访问密钥(一次性活动)。采取以下步骤来设置一个新的 Twitter 应用程序，以获得消费者/访问密钥、秘密和令牌(不要与未经授权的人共享密钥令牌)。

1.  前往 [`https://apps.twitter.com/`](https://apps.twitter.com/)

2.  点击“创建新应用”

3.  填写所需信息，然后点击“创建您的 Twitter 应用程序”

4.  您将在“密钥和访问令牌”选项卡下获得访问详细信息

第二步——获取推文。一旦有了授权秘密和访问令牌，就可以使用清单 [5-2](#PC2) 代码示例来建立连接。

```py
#Import the necessary methods from tweepy library
import tweepy
from tweepy.streaming import StreamListener
from tweepy import OAuthHandler
from tweepy import Stream

#provide your access details below
access_token = "Your token goes here"
access_token_secret = "Your token secret goes here"
consumer_key = "Your consumer key goes here"
consumer_secret = "Your consumer secret goes here"

# establish a connection
auth = tweepy.auth.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)

api = tweepy.API(auth)

Listing 5-2
Twitter Authentication

```

让我们假设你想了解关于 iPhone 7 及其相机功能的讨论。所以，我们来拉十个最近的帖子。

注意:根据帖子的数量，您最多只能获取 10 到 15 天内关于某个主题的历史用户帖子。

```py
#fetch recent 10 tweets containing words iphone7, camera
fetched_tweets = api.search(q=['iPhone 7','iPhone7','camera'], result_type="recent", lang="en", count=10)
print ("Number of tweets: ",len(fetched_tweets))
#----output----
Number of tweets:  10

# Print the tweet text
for tweet in fetched_tweets:
    print ('Tweet ID: ', tweet.id)
    print ('Tweet Text: ', tweet.text, '\n')
#----output----
Tweet ID:  825155021390049281
Tweet Text:  RT @volcanojulie: A Tau Emerald dragonfly. The iPhone 7 camera is exceptional!
#nature #insect #dragonfly #melbourne #australia #iphone7 #…

Tweet ID:  825086303318507520
Tweet Text:  Fuzzy photos? Protect your camera lens instantly with #iPhone7 Full Metallic Case. Buy now! https://t.co/d0dX40BHL6 https://t.co/AInlBoreht

```

您可以将有用的特征捕捉到数据帧中，以供进一步分析(清单 [5-3](#PC4) )。

```py
# function to save required basic tweets info to a dataframe
def populate_tweet_df(tweets):
    #Create an empty dataframe
    df = pd.DataFrame()

    df['id'] = list(map(lambda tweet: tweet.id, tweets))
    df['text'] = list(map(lambda tweet: tweet.text, tweets))
    df['retweeted'] = list(map(lambda tweet: tweet.retweeted, tweets))
    df['place'] = list(map(lambda tweet: tweet.user.location, tweets))
    df['screen_name'] = list(map(lambda tweet: tweet.user.screen_name, tweets))
    df['verified_user'] = list(map(lambda tweet: tweet.user.verified, tweets))
    df['followers_count'] = list(map(lambda tweet: tweet.user.followers_count, tweets))
    df['friends_count'] = list(map(lambda tweet: tweet.user.friends_count, tweets))

    # Highly popular user's tweet could possibly seen by large audience, so lets check the popularity of user
    df['friendship_coeff'] = list(map(lambda tweet: float(tweet.user.followers_count)/float(tweet.user.friends_count), tweets))
    return df

df = populate_tweet_df(fetched_tweets)
print df.head(10)
#---output----
                   id                                               text
0  825155021390049281  RT @volcanojulie: A Tau Emerald dragonfly. The...
1  825086303318507520  Fuzzy photos? Protect your camera lens instant...
2  825064476714098690  RT @volcanojulie: A Tau Emerald dragonfly. The...
3  825062644986023936  RT @volcanojulie: A Tau Emerald dragonfly. The...
4  824935025217040385  RT @volcanojulie: A Tau Emerald dragonfly. The...
5  824933631365779458  A Tau Emerald dragonfly. The iPhone 7 camera i...
6  824836880491483136  The camera on the IPhone 7 plus is fucking awe...
7  823805101999390720  'Romeo and Juliet' Ad Showcases Apple's iPhone...
8  823804251117850624  iPhone 7 Images Show Bigger Camera Lens - I ha...
9  823689306376196096  RT @computerworks5: Premium HD Selfie Stick &a...

  retweeted                          place      screen_name verified_user
0     False            Melbourne, Victoria        MonashEAE         False
1     False                California, USA        ShopNCURV         False
2     False    West Islip, Long Island, NY  FusionWestIslip         False
3     False  6676 Fresh Pond Rd Queens, NY  FusionRidgewood         False
4     False                                   Iphone7review         False
5     False   Melbourne; Monash University     volcanojulie         False
6     False                  Hollywood, FL       Hbk_Mannyp         False
7     False       Toronto.NYC.the Universe  AaronRFernandes         False
8     False                 Lagos, Nigeria    moyinoluwa_mm         False
9     False                                   Iphone7review         False

   followers_count  friends_count  friendship_coeff
0              322            388          0.829897
1              279            318          0.877358
2               13            193          0.067358
3               45            218          0.206422
4              199           1787          0.111360
5              398            551          0.722323
6               57             64          0.890625
7            18291              7       2613.000000
8              780            302          2.582781
9              199           1787          0.111360

Listing 5-3Save Features to Dataframe

```

除了主题，您还可以选择一个专注于某个主题的 screen_name。让我们看看(列表 [5-4](#PC5) )网名为 Iphone7review 的帖子。

```py
# For help about api look here http://tweepy.readthedocs.org/en/v2.3.0/api.html
fetched_tweets =  api.user_timeline(id='Iphone7review', count=5)

# Print the tweet text
for tweet in fetched_tweets:
    print 'Tweet ID: ', tweet.id
    print 'Tweet Text: ', tweet.text, '\n'
#----output----
Tweet ID:  825169063676608512
Tweet Text:  RT @alicesttu: iPhone 7S to get Samsung OLED display next year #iPhone https://t.co/BylKbvXgAG #iphone

Tweet ID:  825169047138533376
Tweet Text:  Nothing beats the Iphone7! Who agrees? #Iphone7 https://t.co/e03tXeLOao

Listing 5-4Example Code for Extracting Tweets Based on Screen Name

```

快速浏览这些帖子，人们通常可以得出结论，iPhone 7 的摄像头功能得到了积极的评价。

## 数据预处理(文本)

该步骤处理净化合并的文本以去除噪声，从而确保有效的句法、语义文本分析，以便从文本中获得有意义的见解。下面简要介绍一些常见的清洁步骤。

### 转换为小写并标记化

在这里，所有的数据都被转换成小写。这是为了防止像“like”或“LIKE”这样的词被解释为不同的词。Python 提供了一个函数 *lower()* 将文本转换成小写。

标记化是将一大组文本分解成更小的有意义的块，如句子、单词、短语的过程。

#### 句子标记化

NLTK(自然语言工具包)库提供 sent_tokenize 用于句子级标记化，它使用一个预训练的模型 PunktSentenceTokenize 来确定标点符号和标记欧洲语言句子结尾的字符(清单 [5-5](#PC6) )。

```py
import nltk
from nltk.tokenize import sent_tokenize

text='Statistics skills, and programming skills are equally important for analytics. Statistics skills and domain knowledge are important for analytics. I like reading books and traveling.'

sent_tokenize_list = sent_tokenize(text)
print(sent_tokenize_list)
#----output----
['Statistics skills, and programming skills are equally important for analytics.', 'Statistics skills, and domain knowledge are important for analytics.', 'I like reading books and travelling.']

Listing 5-5Example Code for Sentence Tokenizing

```

NLTK 总共支持 17 种欧洲语言的句子标记化。清单 [5-6](#PC7) 给出了为特定语言加载标记化模型的示例代码，作为 nltk.data 的一部分保存为 pickle 文件

```py
import nltk.data
spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')
spanish_tokenizer.tokenize('Hola. Esta es una frase espanola.')
#----output----
['Hola.', 'Esta es una frase espanola.']

Listing 5-6Sentence Tokenizing for European Languages

```

#### 单词标记化

NLTK 的 word_tokenize 函数是一个包装器函数，由 TreebankWordTokenizer 调用 tokenize(清单 [5-7](#PC8) )。

```py
from nltk.tokenize import word_tokenize
print word_tokenize(text)

# Another equivalent call method using TreebankWordTokenizer
from nltk.tokenize import TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
print (tokenizer.tokenize(text))
#----output----
['Statistics', 'skills', ',', 'and', 'programming', 'skills', 'are', 'equally', 'important', 'for', 'analytics', '.', 'Statistics', 'skills', ',', 'and', 'domain', 'knowledge', 'are', 'important', 'for', 'analytics', '.', 'I', 'like', 'reading', 'books', 'and', 'travelling', '.']

Listing 5-7Example Code for Word Tokenizing

```

### 消除噪音

你应该删除所有与文本分析无关的信息。这可以被视为文本分析的噪声。最常见的干扰是数字、标点符号、停用词、空格等。(列表 [5-8](#PC9) )。

*数字:*数字被删除，因为它们可能不相关并且不包含有价值的信息。

```py
def remove_numbers(text):
    return re.sub(r'\d+', ", text)

text = 'This is a     sample  English   sentence, \n with whitespace and numbers 1234!'
print ('Removed numbers: ', remove_numbers(text))
#----output----
Removed numbers:  This is a     sample  English   sentence,
 with whitespace and numbers!

Listing 5-8Example Code for Removing Noise from Text

```

*标点:*为了更好地识别每个单词，从数据集中删除标点字符，需要将其删除。例如:“like”和“like”或“coca-cola”和“CocaCola”会被解释为不同的单词，如果不去掉标点符号的话(清单 [5-9](#PC10) )。

```py
import string
# Function to remove punctuations
def remove_punctuations(text):
    words = nltk.word_tokenize(text)
    punt_removed = [w for w in words if w.lower() not in string.punctuation]
    return " ".join(punt_removed)

print (remove_punctuations('This is a sample English sentence, with punctuations!'))
#----output----
This is a sample English sentence with punctuations

Listing 5-9Example Code for Removing Punctuations from Text

```

*停用词:*像“the”、“and”和“or”这样的词是没有意义的，会给分析增加不必要的干扰。由于这个原因，它们被移除(列表 [5-10](#PC11) )。

```py
from nltk.corpus import stopwords

# Function to remove stop words
def remove_stopwords(text, lang="english"):
    words = nltk.word_tokenize(text)
    lang_stopwords = stopwords.words(lang)
    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]
    return " ".join(stopwords_removed)

print (remove_stopwords('This is a sample English sentence'))
#----output----
sample English sentence

Listing 5-10Example Code for Removing Stop Words from the Text

```

### 注意

*删除自己的停用词(如果需要)。*某些单词可能在特定领域中非常常用。除了英语停用词，我们还可以删除我们自己的停用词。我们自己的停用词的选择可能取决于话语的领域，并且可能直到我们做了一些分析之后才变得明显。

*空白:*通常在文本分析中，多余的空白(空格、制表符、回车、换行符)会被识别为一个单词。这种异常可以通过该步骤中的基本编程程序来避免(列表 [5-11](#PC12) )。

```py
# Function to remove whitespace
def remove_whitespace(text):
    return " ".join(text.split())
text = 'This is a     sample  English   sentence, \n with whitespace and numbers 1234!'
print ('Removed whitespace: ', remove_whitespace(text))
#----output----
Removed whitespace:  This is a sample English sentence, with whitespace and numbers 1234!

Listing 5-11Example Code for Removing Whitespace from Text

```

### 词性标注

词性标注是分配特定语言词性的过程，如名词、动词、形容词、副词等。，对于给定文本中的每个单词。

NLTK 支持多种 PoS 标记模型，默认的标记器是 maxent_treebank_pos_tagger，使用的是 Penn(宾夕法尼亚大学)treebank 语料库(表 [5-3](#Tab3) )。同样有 36 个可能的 PoS 标签。语法分析器将一个句子表示为一棵有三个子树的树:名词短语(NP)、动词短语(VP)和句号(。).树的根将是 s。清单 [5-12](#PC13) 和 [5-13](#PC14) 为您提供了词性标注和可视化句子树的示例代码。

表 5-3

NLTK PoS 标签

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

词性标注

 | 

简短描述

 |
| --- | --- |
| maxent_treebank_pos_tagger | 这是基于最大熵(ME)分类原则训练的华尔街日报子集的宾夕法尼亚树银行语料库 |
| 布里特格 | Brill 的基于规则的转换标记器 |
| CRFTagger | 条件随机场 |
| HiddenMarkovModelTagger | 隐马尔可夫模型(hmm)主要用于将正确的标签序列分配给序列数据，或者评估给定标签和数据序列的概率 |
| 洪博塔格 | 与 HunPos 开源 Pos 标记器接口的模块 |
| 感知标签 | 基于 Matthew Honnibal 提出的平均感知机技术 |
| 森纳塔格 | 使用神经网络架构的语义/句法提取 |
| SequentialBackoffTagger | 从左到右顺序标记句子的类 |
| 斯坦福·波斯塔格 | 斯坦福大学的研究者和开发者 |
| 三硝基甲苯 | Thorsten Brants 实现“TnT——统计词性标注器” |

```py
from nltk import chunk

tagged_sent = nltk.pos_tag(nltk.word_tokenize('This is a sample English sentence'))
print (tagged_sent)

tree = chunk.ne_chunk(tagged_sent)
tree.draw() # this will draw the sentence tree
#----output----
[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]

Listing 5-12Example Code for PoS Tagging the Sentence and Visualizing the Sentence Tree

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Figa_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Figa_HTML.jpg)

```py
# To use PerceptronTagger
from nltk.tag.perceptron import PerceptronTagger
PT = PerceptronTagger()
print (PT.tag('This is a sample English sentence'.split()))
#----output----
[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('English', 'JJ'), ('sentence', 'NN')]

# To get help about tags
nltk.help.upenn_tagset('NNP')
#----output----
NNP: noun, proper, singular

Listing 5-13Example Code for Using Perceptron Tagger and Getting Help on Tags

```

### 堵塞物

词干是转化为词根的过程。它使用一种算法来删除英语单词的常见词尾，如“ly”、“es”、“ed”和“s”。例如，假设在进行分析时，您可能希望将“小心”、“关心”、“关心”和“关心地”视为“关心”，而不是单独的单词。图 [5-3](#Fig3) 中列出了三种最广泛使用的词干算法。清单 [5-14](#PC15) 提供了词干提取的示例代码。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig3_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig3_HTML.jpg)

图 5-3

最流行的 NLTK 词干分析器

```py
from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer

# Function to apply stemming to a list of words
def words_stemmer(words, type="PorterStemmer", lang="english", encoding="utf8"):
    supported_stemmers = ["PorterStemmer","LancasterStemmer","SnowballStemmer"]
    if type is False or type not in supported_stemmers:
        return words
    else:
        stem_words = []
        if type == "PorterStemmer":
            stemmer = PorterStemmer()
            for word in words:
                stem_words.append(stemmer.stem(word).encode(encoding))
        if type == "LancasterStemmer":
            stemmer = LancasterStemmer()
            for word in words:
                stem_words.append(stemmer.stem(word).encode(encoding))
        if type == "SnowballStemmer":
            stemmer = SnowballStemmer(lang)
            for word in words:
                stem_words.append(stemmer.stem(word).encode(encoding))
        return " ".join(stem_words)

words =  'caring cares cared caringly carefully'

print ("Original: ", words)
print ("Porter: ", words_stemmer(nltk.word_tokenize(words), "PorterStemmer"))

print ("Lancaster: ", words_stemmer(nltk.word_tokenize(words), "LancasterStemmer"))
print ("Snowball: ", words_stemmer(nltk.word_tokenize(words), "SnowballStemmer"))
#----output----
Original:  caring cares cared caringly carefully
Porter:  care care care caringly care
Lancaster:  car car car car car
Snowball:  care care care care care

Listing 5-14Example Code for Stemming

```

### 词汇化

它是转换到字典基本形式的过程。为此，你可以使用 WordNet，这是一个大型的英语词汇数据库，通过它们的语义关系连接在一起。它就像一个词库:它根据单词的意思将单词组合在一起(列表 [5-15](#PC16) )。

```py
from nltk.stem import WordNetLemmatizer

wordnet_lemmatizer = WordNetLemmatizer()

# Function to apply lemmatization to a list of words
def words_lemmatizer(text, encoding="utf8"):
    words = nltk.word_tokenize(text)
    lemma_words = []
    wl = WordNetLemmatizer()
    for word in words:
        pos = find_pos(word)
        lemma_words.append(wl.lemmatize(word, pos).encode(encoding))
    return " ".join(lemma_words)

# Function to find part of speech tag for a word
def find_pos(word):
    # Part of Speech constants
    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'
    # You can learn more about these at http://wordnet.princeton.edu/wordnet/man/wndb.5WN.html#sect3
    # You can learn more about all the penn tree tags at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html
    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]
    # Adjective tags - 'JJ', 'JJR', 'JJS'
    if pos.lower()[0] == 'j':
        return 'a'
    # Adverb tags - 'RB', 'RBR', 'RBS'

    elif pos.lower()[0] == 'r':
        return 'r'
    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'
    elif pos.lower()[0] == 'v':
        return 'v'
    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'
    else:
        return 'n'

print ("Lemmatized: ", words_lemmatizer(words))
#----output----
Lemmatized:  care care care caringly carefully

In the preceding case,'caringly'/'carefully' are inflected forms of care and they are an entry word listed in WordNet Dictionary so they are retained in their actual form itself.

Listing 5-15Example Code for Lemmatization

```

NLTK 英语 WordNet 包括大约 155，287 个单词和 117，000 个同义词集。对于给定的单词，WordNet 包括/提供了定义、示例、同义词(一组相似的名词、形容词、动词)、反义词(意思与另一个相反)等。清单 [5-16](#PC17) 提供了 wordnet 的示例代码。

```py
from nltk.corpus import wordnet

syns = wordnet.synsets("good")
print "Definition: ", syns[0].definition()
print "Example: ", syns[0].examples()

synonyms = []
antonyms = []

# Print  synonums and antonyms (having opposite meaning words)
for syn in wordnet.synsets("good"):

    for l in syn.lemmas():
        synonyms.append(l.name())
        if l.antonyms():
            antonyms.append(l.antonyms()[0].name())

print ("synonyms: \n", set(synonyms))
print ("antonyms: \n", set(antonyms))
#----output----
Definition:  benefit
Example:  [u'for your own good', u"what's the good of worrying?"]
synonyms:
set([u'beneficial', u'right', u'secure', u'just', u'unspoilt', u'respectable', u'good', u'goodness', u'dear', u'salutary', u'ripe', u'expert', u'skillful', u'in_force', u'proficient', u'unspoiled', u'dependable', u'soundly', u'honorable', u'full', u'undecomposed', u'safe', u'adept', u'upright', u'trade_good', u'sound', u'in_effect', u'practiced', u'effective', u'commodity', u'estimable', u'well', u'honest', u'near', u'skilful', u'thoroughly', u'serious'])
antonyms:
set([u'bad', u'badness', u'ill', u'evil', u'evilness'])

Listing 5-16Example Code for Wordnet

```

### N-grams

文本挖掘中的一个重要概念是 n 元文法，它基本上是来自给定的大文本序列的 n 个项目的一组共现或连续序列。这里的项目可以是单词、字母和音节。让我们考虑一个例句，试着提取 n 的不同值的 n-grams(清单 [5-17](#PC18) )。

```py
from nltk.util import ngrams
from collections import Counter

# Function to extract n-grams from text
def get_ngrams(text, n):
    n_grams = ngrams(nltk.word_tokenize(text), n)
    return [ ' '.join(grams) for grams in n_grams]

text = 'This is a sample English sentence'
print ("1-gram: ", get_ngrams(text, 1))
print ("2-gram: ", get_ngrams(text, 2))
print ("3-gram: ", get_ngrams(text, 3))
print ("4-gram: ", get_ngrams(text, 4))
#----output----
1-gram:['This', 'is', 'a', 'sample', 'English', 'sentence']
2-gram:['This is', 'is a', 'a sample', 'sample English', 'English sentence']
3-gram:['This is a', 'is a sample', 'a sample English', 'sample English sentence']
4-gram: ['This is a sample', 'is a sample English', 'a sample English sentence']

Listing 5-17Example Code for Extracting n-grams from the Sentence

```

### 注意

1-gram 也称为 unigram 二元模型和三元模型分别是二元模型和三元模型。

N-gram 技术相对简单，简单的增加 n 的值会给我们更多的上下文。它在概率语言模型中广泛用于预测序列中的下一项。例如，当用户键入时，搜索引擎使用这种技术来预测/推荐序列中下一个字符/单词的可能性(清单 [5-18](#PC19) )。

```py
text = 'Statistics skills, and programming skills are equally important for analytics. Statistics skills and domain knowledge are important for analytics'

# remove punctuations
text = remove_punctuations(text)

# Extracting bigrams
result = get_ngrams(text,2)

# Counting bigrams
result_count = Counter(result)

# Converting the result to a data frame
import pandas as pd
df = pd.DataFrame.from_dict(result_count, orient="index")
df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index and column name
print (df)
#----output----
                      frequency
are equally                   1
domain knowledge              1
skills are                    1
knowledge are                 1
programming skills            1
are important                 1
skills and                    2
for analytics                 2
and domain                    1
important for                 2
and programming               1
Statistics skills             2
equally important             1
analytics Statistics          1

Listing 5-18Example Code for Extracting 2-grams from the Sentence and Storing in a Dataframe

```

### 一袋单词

文本必须用数字表示才能应用任何算法。单词包(BoW)是一种计算文档中单词出现次数的方法，而不考虑语法和单词顺序的重要性。这可以通过创建术语文档矩阵(TDM)来实现。它只是一个矩阵，以术语为行，以文档名为列，以词频计数为矩阵的单元(图 [5-4](#Fig4) )。让我们通过一个例子来学习创建 TDM:考虑三个包含一些文本的文本文档。Sklearn 在 feature_extraction.text 下提供了很好的函数，将一个文本文档集合转换成字数矩阵(清单 [5-19](#PC20) )。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig4_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig4_HTML.jpg)

图 5-4

术语文档矩阵

```py
import os
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Function to create a dictionary with key as file names and values as text for all files in a given folder
def CorpusFromDir(dir_path):
    result = dict(docs = [open(os.path.join(dir_path,f)).read() for f in os.listdir(dir_path)],
               ColNames = map(lambda x: x, os.listdir(dir_path)))
    return result

docs = CorpusFromDir('Data/')

# Initialize

vectorizer = CountVectorizer()
doc_vec = vectorizer.fit_transform(docs.get('docs'))

#create dataFrame
df = pd.DataFrame(doc_vec.toarray().transpose(), index = vectorizer.get_feature_names())

# Change column headers to be file names
df.columns = docs.get('ColNames')
print (df)
#----output----
             Doc_1.txt  Doc_2.txt  Doc_3.txt
analytics            1          1          0
and                  1          1          1
are                  1          1          0
books                0          0          1
domain               0          1          0
equally              1          0          0
for                  1          1          0
important            1          1          0
knowledge            0          1          0
like                 0          0          1
programming          1          0          0
reading              0          0          1
skills               2          1          0
statistics           1          1          0
travelling           0          0          1

Listing 5-19Creating a Term Document Matrix from a Corpus of Sample Documents

```

### 注意

术语文档矩阵(TDM)是术语文档矩阵的转置。在 TDM 中，行是文档名，列标题是术语。两者都是矩阵格式，有助于进行分析；然而，由于术语的数量通常比文档数大得多，所以通常使用 TDM。在这种情况下，拥有更多行比拥有大量列更好。

### 术语频率-逆文档频率(TF-IDF)

在信息检索领域，TF-IDF 是一种很好的统计方法，可以反映术语与文档集合或语料库中文档的相关性。我们来分解一下 TF_IDF，应用一个例子来更好的理解。

术语频率将告诉你一个给定术语出现的频率。

TF (term) = ![$$ \frac{Number\ of\ times\ term\ appears\ in\ a\ document}{Total\ number\ of\ term s\ in\ the\ document} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq1.png)

例如，考虑包含 100 个单词的文档，其中单词“ML”出现三次，则 TF (ML) = 3 / 100 = 0.03

文档频率会告诉你一个术语有多重要。

DF (term) = ![$$ \frac{d\ \left( number\ of\ documents\ containing\ a\  given\ term\right)}{D\ \left( the\ size\ of\ the\ collection\ of\ documents\right)} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq2.png)

假设我们有一千万个文档，单词 ML 出现在其中的一千个文档中，那么 DF (ML) = 1000 / 10，000，000 = 0.0001。

为了归一化，我们以 log (d/D)为例，log (0.0001) = -4

如前例所示，D > d 和 log (d/D)通常会给出负值。因此，为了解决这个问题，让我们对 log 表达式内部的比率进行反演，这就是所谓的逆文档频率(IDF)。本质上，我们正在压缩价值的尺度，以便可以平滑地比较非常大或非常小的数量。

IDF(期限)= ![$$ \log \left(\frac{Total\ number\ of\ documents}{Number\ of\ documents\ with\ a\  given\ term\ in\  it}\right) $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq3.png)

继续前面的例子，IDF(ML) = log(10，000，000 / 1，000) = 4。

TF-IDF 是数量的重量乘积；对于前面的例子，TF-IDF (ML) = 0.03 * 4 = 0.12。Sklearn 提供了一个函数 TfidfVectorizer，为文本计算 TF-IDF；然而，默认情况下，它使用 L2 归一化对术语向量进行归一化，并且通过将文档频率加 1 来平滑 IDF，以防止零划分(列表 [5-20](#PC21) )。

```py
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()

doc_vec = vectorizer.fit_transform(docs.get('docs'))
#create dataFrame
df = pd.DataFrame(doc_vec.toarray().transpose(), index = vectorizer.get_feature_names())

# Change column headers to be file names
df.columns = docs.get('ColNames')
print (df)
#----output----
             Doc_1.txt  Doc_2.txt  Doc_3.txt
analytics     0.276703   0.315269   0.000000
and           0.214884   0.244835   0.283217
are           0.276703   0.315269   0.000000
books         0.000000   0.000000   0.479528
domain        0.000000   0.414541   0.000000
equally       0.363831   0.000000   0.000000
for           0.276703   0.315269   0.000000
important     0.276703   0.315269   0.000000
knowledge     0.000000   0.414541   0.000000
like          0.000000   0.000000   0.479528
programming   0.363831   0.000000   0.000000
reading       0.000000   0.000000   0.479528
skills        0.553405   0.315269   0.000000
statistics    0.276703   0.315269   0.000000
travelling    0.000000   0.000000   0.479528

Listing 5-20Create a Term Document Matrix (TDM) with TF-IDF

```

## 数据探索(文本)

在这一阶段，探索语料库以理解共同的关键词、内容、关系以及噪声的存在和水平。这可以通过创建基本统计数据和采用可视化技术来实现，如词频计数、词共现或相关图等。，这将有助于我们发现隐藏的模式，如果有的话。

### 频率表

这个可视化呈现了一个条形图，其长度对应于特定单词出现的频率。让我们为 Doc_1.txt 文件绘制一个频率图(清单 [5-21](#PC22) )。

```py
words = df.index
freq = df.ix[:,0].sort(ascending=False, inplace=False)

pos = np.arange(len(words))
width=1.0
ax=plt.axes(frameon=True)
ax.set_xticks(pos)
ax.set_xticklabels(words, rotation="vertical", fontsize=9)
ax.set_title('Word Frequency Chart')
ax.set_xlabel('Words')
ax.set_ylabel('Frequency')
plt.bar(pos, freq, width, color="b")
plt.show()
#----output----

Listing 5-21Example Code for Frequency Chart

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Figb_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Figb_HTML.jpg)

### 词云

这是文本数据的可视化表示，有助于从数据中的重要关键词出现的角度获得高层次的理解。wordcloud 包可以用来生成字体大小与其频率相关的单词(清单 [5-22](#PC23) )。

```py
from wordcloud import WordCloud

# Read the whole text.
text = open('Data/Text_Files/Doc_1.txt').read()

# Generate a word cloud image
wordcloud = WordCloud().generate(text)

# Display the generated image:
# the matplotlib way:
import matplotlib.pyplot as plt
plt.imshow(wordcloud.recolor(random_state=2017))
plt.title('Most Frequent Words')
plt.axis("off")
plt.show()
#----output----

Listing 5-22Example Code for the Word Cloud

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Figc_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Figc_HTML.jpg)

从上面的图表中我们可以看出，相对而言,“技能”出现的次数最多。

### 词汇离差图

这个图有助于确定一个单词在一系列文本句子中的位置。x 轴上是单词偏移量，y 轴上的每一行代表整个文本，标记表示感兴趣的单词的一个实例(清单 [5-23](#PC24) )。

```py
from nltk import word_tokenize

def dispersion_plot(text, words):
    words_token = word_tokenize(text)
    points = [(x,y) for x in range(len(words_token)) for y in range(len(words)) if words_token[x] == words[y]]

    if points:
        x,y=zip(*points)
    else:
        x=y=()

    plt.plot(x,y,"rx",scalex=.1)
    plt.yticks(range(len(words)),words,color="b")
    plt.ylim(-1,len(words))
    plt.title("Lexical Dispersion Plot")
    plt.xlabel("Word Offset")
    plt.show()

text = 'statistics skills and programming skills are equally important for analytics. statistics skills and domain knowledge are important for analytics'

dispersion_plot(text, ['statistics', 'skills', 'and', 'important'])
#----output----

Listing 5-23Example Code for Lexical Dispersion Plot

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Figd_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Figd_HTML.jpg)

### 共生矩阵

计算文本序列中单词之间的共现将有助于解释单词之间的关系。共现矩阵告诉我们每个单词与当前单词共现了多少次。将这个矩阵进一步绘制成热图是一个强大的可视化工具，可以有效地发现单词之间的关系(清单 [5-24](#PC25) )。

```py
import statsmodels.api as sm
import scipy.sparse as sp

# default unigram model
count_model = CountVectorizer(ngram_range=(1,1))
docs_unigram = count_model.fit_transform(docs.get('docs'))

# co-occurrence matrix in sparse csr format
docs_unigram_matrix = (docs_unigram.T * docs_unigram)

# fill same word cooccurence to 0
docs_unigram_matrix.setdiag(0)

# co-occurrence matrix in sparse csr format
docs_unigram_matrix = (docs_unigram.T * docs_unigram) docs_unigram_matrix_diags = sp.diags(1./docs_unigram_matrix.diagonal())

# normalized co-occurence matrix
docs_unigram_matrix_norm = docs_unigram_matrix_diags * docs_unigram_matrix

# Convert to a dataframe

df = pd.DataFrame(docs_unigram_matrix_norm.todense(), index = count_model.get_feature_names())
df.columns = count_model.get_feature_names()

# Plot
sm.graphics.plot_corr(df, title='Co-occurrence Matrix', xnames=list(df.index))
plt.show()
#----output----

Listing 5-24Example Code for Cooccurrence Matrix

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Fige_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fige_HTML.jpg)

## 模型结构

您现在可能已经很熟悉了，建模是理解和建立变量之间关系的过程。到目前为止，您已经学习了如何从各种来源提取文本内容，预处理以消除噪声，以及执行探索性分析以获得关于手头文本数据的基本理解/统计。现在，您将学习对处理后的数据应用 ML 技术来构建模型。

### 文本相似度

这是指示两个对象有多相似的度量，通过用对象的特征(这里是文本)表示的维度的距离度量来描述。较小的距离表示高度相似，反之亦然。请注意，相似性是高度主观的，取决于领域或应用。对于文本相似性，选择合适的距离度量来获得更好的结果是很重要的。有各种各样的距离度量，欧几里德度量是最常见的，它是两点之间的直线距离。然而，在文本挖掘领域已经进行了大量的研究，以了解余弦距离更适合于文本相似性。

让我们看一个简单的例子(表 [5-4](#Tab4) )来更好地理解相似性。考虑包含某些简单文本关键词的三个文档，并假设前两个关键词是“事故”和“纽约”目前，忽略其他关键字，让我们基于这两个关键字的频率来计算文档的相似性。

表 5-4

样本术语文档矩阵

<colgroup><col class="tcol1 align-center"> <col class="tcol2 align-center"> <col class="tcol3 align-center"></colgroup> 
| 

文档编号

 | 

“事故”计数

 | 

“纽约”伯爵

 |
| --- | --- | --- |
| one | Two | eight |
| Two | three | seven |
| three | seven | three |

图 [5-5](#Fig5) 描绘了在二维图表上绘制文档词向量点。请注意，余弦相似性方程是两个数据点之间角度的表示，而欧几里德距离是数据点之间直线差的平方根。余弦相似性方程将产生一个介于 0 和 1 之间的值。余弦角越小，余弦值越大，表示相似度越高。在这种情况下，欧几里得距离将导致零。让我们将这些值放入公式中，找出文档 1 和文档 2 之间的相似性。

欧几里德距离(doc1，doc2) = ![$$ \sqrt{\left(2-3\right)\hat 2+\left(8-7\right)\hat 2}=\sqrt{\left(1+1\right)} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq4.png) = 1.41 = 0

余弦(doc1，doc2) = ![$$ \frac{62}{8.24\ast 7.61} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq5.png) = 0.98，其中

文档 1 = (2，8)

文档 2 = (3，7)

1 号文件。doc 2 =(2 * 3+8 * 7)=(56+6)= 62

||doc1|| = ` ![$$ \sqrt{\left(2\ast 2\right)+\left(8\ast 8\right)} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq6.png) = 8.24

||doc2|| = ![$$ \sqrt{\left(3\ast 3\right)+\left(7\ast 7\right)} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq7.png) = 7.61

同样，让我们找出文档 1 和 3 之间的相似之处(图 [5-5](#Fig5) )。

欧几里德距离(doc1，doc3) = ![$$ \sqrt{\left(2-7\right)\hat 2+\left(8-3\right)\hat 2}=\sqrt{\left(25+25\right)}=7.07 $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq8.png) = 0

余弦(doc1，doc3)= ![$$ \frac{38}{8.24\ast 7.61} $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_IEq9.png) = 0.60

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig5_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig5_HTML.jpg)

图 5-5

欧几里德与余弦

根据余弦方程，文件 1 和文件 2 有 98%相似；这可能意味着这两个文档更多地讨论了纽约，而文档 3 可以被认为更多地关注于“事故”然而，有几次提到了纽约，导致文档 1 和 3 之间有 60%的相似性。

清单 [5-25](#PC26) 为图 [5-5](#Fig5) 中给出的例子提供了计算余弦相似度的示例代码。

```py
from sklearn.metrics.pairwise import cosine_similarity

print "Similarity b/w doc 1 & 2: ", cosine_similarity([df['Doc_1.txt']], [df['Doc_2.txt']])
print "Similarity b/w doc 1 & 3: ", cosine_similarity([df['Doc_1.txt']], [df['Doc_3.txt']])
print "Similarity b/w doc 2 & 3: ", cosine_similarity([df['Doc_2.txt']], [df['Doc_3.txt']])
#----output----
Similarity b/w doc 1 & 2:  [[ 0.76980036]]
Similarity b/w doc 1 & 3:  [[ 0.12909944]]
Similarity b/w doc 2 & 3:  [[ 0.1490712]]

Listing 5-25Example Code for Calculating Cosine Similarity for Documents

```

### 文本聚类

例如，我们将使用 20 个新闻组数据集，它由 20 个主题的 18，000 多篇新闻组帖子组成。您可以在 [`http://qwone.com/~jason/20Newsgroups/`](http://qwone.com/%257Ejason/20Newsgroups/) 了解更多关于数据集的信息。让我们加载数据并检查主题名称(清单 [5-26](#PC27) )。

```py
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import Normalizer
from sklearn import metrics
from sklearn.cluster import KMeans, MiniBatchKMeans
import numpy as np

# load data and print topic names
newsgroups_train = fetch_20newsgroups(subset='train')
print(list(newsgroups_train.target_names))
#----output----
['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']

Listing 5-26Example Code for Text Clustering

```

为了简单起见，我们只过滤三个主题。假设我们不知道题目。让我们运行聚类算法，检查每个聚类的关键字。

```py
categories = ['alt.atheism', 'comp.graphics', 'rec.motorcycles']

dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=2017)

print("%d documents" % len(dataset.data))
print("%d categories" % len(dataset.target_names))

labels = dataset.target

print("Extracting features from the dataset using a sparse vectorizer")
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(dataset.data)
print("n_samples: %d, n_features: %d" % X.shape)
#----output----
2768 documents
3 categories
Extracting features from the dataset using a sparse vectorizer
n_samples: 2768, n_features: 35311

```

#### 潜在语义分析(LSA)

LSA 是一种数学方法，它试图揭示文档集合中的潜在关系。它不是孤立地查看每个文档，而是将所有文档作为一个整体来查看，并查看其中的术语来确定关系。让我们通过对数据运行奇异值分解(SVD)来执行 LSA，以降低维数。

矩阵 A 的奇异值分解= U * ∑ * V <sup>T</sup>

r =矩阵 X 的秩

U =列正交 m * r 矩阵

∑ =对角 r∫r 矩阵，奇异值按降序排序

V =列正交 r∫n 矩阵

在我们的例子中，我们有三个主题、2768 个文档和 35311 个单词的词汇表(图 [5-6](#Fig6) )。

*原始矩阵= 2768*35311 ~ 10 <sup>8</sup>

* SVD = 3 * 2768+3+3 * 35311 ~ 10<sup>5.3</sup>

得到的 SVD 占用的空间比原始矩阵少大约 460 倍。清单 [5-27](#PC29) 提供了通过 SVD 实现 LSA 的示例代码。

### 注意

潜在语义分析(LSA)和潜在语义索引(LSI)是同一个东西，后者的名称有时被用来特指为搜索(信息检索)而索引一组文档。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig6_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig6_HTML.jpg)

图 5-6

奇异值分解

```py
from sklearn.decomposition import TruncatedSVD

# Lets reduce the dimensionality to 2000
svd = TruncatedSVD(2000)
lsa = make_pipeline(svd, Normalizer(copy=False))

X = lsa.fit_transform(X)

explained_variance = svd.explained_variance_ratio_.sum()
print("Explained variance of the SVD step: {}%".format(int(explained_variance * 100)))
#----output----
Explained variance of the SVD step: 95%

Listing 5-27Example Code for LSA Through SVD

```

清单 [5-28](#PC30) 是在 SVD 输出上运行 k-means 聚类的示例代码。

```py
from __future__ import print_function

km = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1)

# Scikit learn provides MiniBatchKMeans to run k-means in batch mode suitable for a very large corpus
# km = MiniBatchKMeans(n_clusters=5, init='k-means++', n_init=1, init_size=1000, batch_size=1000)

print("Clustering sparse data with %s" % km)
km.fit(X)

print("Top terms per cluster:")
original_space_centroids = svd.inverse_transform(km.cluster_centers_)
order_centroids = original_space_centroids.argsort()[:, ::-1]

terms = vectorizer.get_feature_names()
for i in range(3):
    print("Cluster %d:" % i, end=“)
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind], end=“)
    print()
#----output----
Top terms per cluster:
Cluster 0: edu graphics university god subject lines organization com posting uk
Cluster 1: com bike edu dod ca writes article sun like organization
Cluster 2: keith sgi livesey caltech com solntze wpd jon edu sandvik

Listing 5-28k-means Clustering on SVD Dataset

```

清单 [5-29](#PC31) 是在 SVD 数据集上运行层次聚类的示例代码。

```py
from sklearn.metrics.pairwise import cosine_similarity
dist = 1 - cosine_similarity(X)

from scipy.cluster.hierarchy import ward, dendrogram

linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances

fig, ax = plt.subplots(figsize=(8, 8)) # set size
ax = dendrogram(linkage_matrix, orientation="right")

plt.tick_params(axis= 'x', which="both")

plt.tight_layout() #show plot with tight layout
plt.show()
#----output----

Listing 5-29Hierarchical Clustering on SVD Dataset

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Figf_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Figf_HTML.jpg)

### 主题建模

主题建模算法使您能够在大量文档中发现隐藏的主题模式或主题结构。最流行的主题建模技术是 LDA 和 NMF。

#### 潜在狄利克雷分配

LDA 是由大卫·布雷、吴恩达和迈克尔·乔丹在 2003 年提出的图形模型(图 [5-7](#Fig7) )。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig7_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig7_HTML.jpg)

图 5-7

LDA 图模型

LDA 是由 *P* ( *d* 、*w*=*p*(*d*)**p*(θ)

其中，φ<sup>(z)</sup>=主题的词分布，

α =在每个文档主题分布之前的狄利克雷参数，

β `=`狄利克雷参数在每文档单词分布之前，

θ<sup>(d)</sup>=文档的主题分布。

LDA 的目标是最大化投影主题均值之间的分离，最小化每个投影主题内的方差。因此，LDA 通过执行如下所述的三个步骤将每个主题定义为一个单词包(图 [5-8](#Fig8) )。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig8_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig8_HTML.jpg)

图 5-8

潜在狄利克雷分配

步骤 1:初始化 k 个簇，并将每个文档中的每个单词分配给 k 个主题中的一个。

步骤 2:根据 a)一个文档的单词与一个主题的比例，以及 b)一个主题在所有文档中的比例，将单词重新分配给一个新的主题。

第三步:重复第二步，直到产生连贯的主题。

清单 [5-30](#PC32) 提供了实现 LDA 的示例代码。

```py
from sklearn.decomposition import LatentDirichletAllocation

# continuing with the 20 newsgroup dataset and 3 topics
total_topics = 3
lda = LatentDirichletAllocation(n_components=total_topics,
                                max_iter=100,
                                learning_method='online',
                                learning_offset=50.,
                                random_state=2017)
lda.fit(X)

feature_names = np.array(vectorizer.get_feature_names())

for topic_idx, topic in enumerate(lda.components_):
    print("Topic #%d:" % topic_idx)
    print(" ".join([feature_names[i] for i in topic.argsort()[:-20 - 1:-1]]))
#----output----
Topic #0:
edu com writes subject lines organization

article posting university nntp host don like god uk ca just bike know graphics
Topic #1:
anl elliptical maier michael_maier qmgate separations imagesetter 5298 unscene appreshed linotronic l300 iici amnesia glued veiw halftone 708 252 dot
Topic #2:
hl7204 eehp22 raoul vrrend386 qedbbs choung qed daruwala ims kkt briarcliff kiat philabs col op_rows op_cols keeve 9327 lakewood gans

Listing 5-30Example Code for LDA

```

#### 非负矩阵分解

NMF 是一种用于多元数据的分解方法，由 V = MH 给出，其中 V 是矩阵 W 和 H 的乘积。W 是特征中单词等级的矩阵，H 是系数矩阵，每行是一个特征。这三个矩阵没有负元素(列表 [5-31](#PC33) )。

```py
from sklear.n.decomposition import NMF

nmf = NMF(n_components=total_topics, random_state=2017, alpha=.1, l1_ratio=.5)
nmf.fit(X)

for topic_idx, topic in enumerate(nmf.components_):
    print("Topic #%d:" % topic_idx)
    print(" ".join([feature_names[i] for i in topic.argsort()[:-20 - 1:-1]]))
#----output----
Topic #0:
edu com god writes article don subject lines organization just university bike people posting like know uk ca think host
Topic #1:
sgi livesey keith solntze wpd jon caltech morality schneider cco moral com allan edu objective political cruel atheists gap writes
Topic #2:
sun east green ed egreen com cruncher microsystems ninjaite 8302 460 rtp 0111 nc 919 grateful drinking pixel biker showed

Listing 5-31Example Code for Nonnegative Matrix Factorization

```

### 文本分类

将文本特征表示为数字的能力为运行分类 ML 算法提供了机会。让我们使用 20 个新闻组数据的子集来构建一个分类模型并评估其准确性(清单 [5-32](#PC34) )。

```py
categories = ['alt.atheism', 'comp.graphics', 'rec.motorcycles', 'sci.space', 'talk.politics.guns']

newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=2017, remove=('headers', 'footers', 'quotes'))
newsgroups_test = fetch_20newsgroups(subset='test', categories=categories,
shuffle=True, random_state=2017, remove=('headers', 'footers', 'quotes'))

y_train = newsgroups_train.target
y_test = newsgroups_test.target

vectorizer = TfidfVectorizer(sublinear_tf=True, smooth_idf = True, max_df=0.5,  ngram_range=(1, 2), stop_words="english")
X_train = vectorizer.fit_transform(newsgroups_train.data)
X_test = vectorizer.transform(newsgroups_test.data)

print("Train Dataset")
print("%d documents" % len(newsgroups_train.data))
print("%d categories" % len(newsgroups_train.target_names))
print("n_samples: %d, n_features: %d" % X_train.shape)

print("Test Dataset")
print("%d documents" % len(newsgroups_test.data))
print("%d categories" % len(newsgroups_test.target_names))
print("n_samples: %d, n_features: %d" % X_test.shape)
#----output----
Train Dataset
2801 documents
5 categories
n_samples: 2801, n_features: 241036
Test Dataset
1864 documents
5 categories
n_samples: 1864, n_features: 241036

Listing 5-32Example Code Text Classification on 20 News Groups Dataset

```

让我们构建一个简单的朴素贝叶斯分类模型，并评估其准确性。本质上，我们可以用任何其他分类算法来代替朴素贝叶斯，或者使用集成模型来建立一个有效的模型(清单 [5-33](#PC35) )。

```py
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

clf = MultinomialNB()
clf = clf.fit(X_train, y_train)

y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

print ('Train accuracy_score: ', metrics.accuracy_score(y_train, y_train_pred))
print ('Test accuracy_score: ',metrics.accuracy_score(newsgroups_test.target, y_test_pred))

print ("Train Metrics: ", metrics.classification_report(y_train, y_train_pred))
print ("Test Metrics: ", metrics.classification_report(newsgroups_test.target, y_test_pred))
#----output----
Train accuracy_score:  0.9760799714387719
Test accuracy_score:  0.8320815450643777
Train Metrics:       precision    recall  f1-score   support

           0              1.00      0.97      0.98       480
           1              1.00      0.97      0.98       584
           2              0.91      1.00      0.95       598
           3              0.99      0.97      0.98       593
           4              1.00      0.97      0.99       546

   micro avg              0.98      0.98      0.98      2801
   macro avg              0.98      0.98      0.98      2801
weighted avg              0.98      0.98      0.98      2801

Test Metrics:        precision    recall  f1-score   support

           0              0.91      0.62      0.74       319
           1              0.90      0.90      0.90       389
           2              0.81      0.90      0.86       398
           3              0.80      0.84      0.82       394
           4              0.78      0.86      0.82       364

   micro avg              0.83      0.83      0.83      1864
   macro avg              0.84      0.82      0.83      1864
weighted avg              0.84      0.83      0.83      1864

Listing 5-33Example Code Text Classification Using Multinomial Naïve Bayes

```

### 情感分析

发现和分类一段文本(如评论/反馈文本)中表达的观点的过程被称为情感分析。这种分析的预期结果是确定作者对某个主题、产品、服务等的态度。是中性的、正的或负的(列表 [5-34](#PC36) )。

```py
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
data = pd.read_csv('Data/customer_review.csv')

SIA = SentimentIntensityAnalyzer()
data['polarity_score']=data.Review.apply(lambda x:SIA.polarity_scores(x)['compound'])
data['neutral_score']=data.Review.apply(lambda x:SIA.polarity_scores(x)['neu'])
data['negative_score']=data.Review.apply(lambda x:SIA.polarity_scores(x)['neg'])
data['positive_score']=data.Review.apply(lambda x:SIA.polarity_scores(x)['pos'])
data['sentiment']=“
data.loc[data.polarity_score>0,'sentiment']='POSITIVE'
data.loc[data.polarity_score==0,'sentiment']='NEUTRAL'
data.loc[data.polarity_score<0,'sentiment']='NEGATIVE'
data.head()

data.sentiment.value_counts().plot(kind='bar',title="sentiment analysis")
plt.show()
#----output----
   ID                                             Review  polarity_score
0   1  Excellent service my claim was dealt with very...          0.7346
1   2  Very sympathetically dealt within all aspects ...         -0.8155
2   3  Having received yet another ludicrous quote fr...          0.9785
3   4  Very prompt and fair handling of claim. A mino...          0.1440
4   5  Very good and excellent value for money simple...          0.8610

   neutral_score  negative_score  positive_score sentiment
0          0.618           0.000           0.382  POSITIVE
1          0.680           0.320           0.000  NEGATIVE
2          0.711           0.039           0.251  POSITIVE
3          0.651           0.135           0.214  POSITIVE
4          0.485           0.000           0.515  POSITIVE

Listing 5-34Example Code for Sentiment Analysis

```

![img/434293_2_En_5_Chapter/434293_2_En_5_Figg_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Figg_HTML.jpg)

### 深度自然语言处理(DNLP)

首先，让我澄清一下，DNLP 不会被误认为是深度学习 NLP。诸如主题建模之类的技术通常被称为浅层 NLP，其中您试图通过语义或句法分析方法从文本中提取知识(即，试图通过保留相似的词并在句子/文档中占据较高权重来形成组)。浅 NLP 比 n-gram 噪音小；然而，关键的缺点是，它没有指定项目在句子中的作用。相比之下，DNLP 侧重于语义方法。也就是说，它检测句子内的关系，并且进一步地，它可以被表示或表达为形式的复杂结构，例如句法分析的句子中的主语:谓语:宾语(称为三元组或三元组),以保留上下文。句子由参与者、动作、对象和命名实体(人、组织、地点、日期等)的任意组合组成。).例如，考虑句子“爆胎被司机换了。”这里“司机”是主语(演员)，“被替换”是谓语(动作)，“爆胎”是宾语。所以这个三元组就是 driver:replaced:tire，它抓住了句子的上下文。请注意，三元组是广泛使用的形式之一，您可以根据手头的领域或问题形成类似的复杂结构。

对于 ae 演示，我将使用 sopex 包，它使用了斯坦福核心 NLP 树解析器(清单 [5-35](#PC37) )。

```py
from chunker import PennTreebackChunker
from extractor import SOPExtractor

# Initialize chunker
chunker = PennTreebackChunker()
extractor = SOPExtractor(chunker)

# function to extract triples
def extract(sentence):
    sentence = sentence if sentence[-1] == '.' else sentence+'.'
    global extractor
    sop_triplet = extractor.extract(sentence)
    return sop_triplet

sentences = [
  'The quick brown fox jumps over the lazy dog.',
  'A rare black squirrel has become a regular visitor to a suburban garden',
  'The driver did not change the flat tire',
  "The driver crashed the bike white bumper"
]

#Loop over sentence and extract triples
for sentence in sentences:
    sop_triplet = extract(sentence)
    print sop_triplet.subject + ':' + sop_triplet.predicate + ':' + sop_triplet.object

#----output----
fox:jumps:dog
squirrel:become:visitor
driver:change:tire
driver:crashed:bumper

Listing 5-35Example Code for Deep NLP

```

## Word2Vec

谷歌的托马斯·米科洛夫(Tomas Mikolov)领导的团队在 2013 年创建了 Word2Vec(单词到向量)模型，该模型使用文档来训练神经网络模型，以最大化给定单词的上下文的条件概率。

它使用两种模型:CBOW 和 skip-gram。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig9_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig9_HTML.jpg)

图 5-9

2 窗口的跳过程序

1.  连续单词袋(CBOW)模型从周围上下文单词的窗口中预测当前单词，或者在给定一组上下文单词的情况下，预测该上下文中可能出现的缺失单词。CBOW 的训练速度比 skip-gram 更快，对频繁出现的单词的准确率更高。

2.  连续跳格模型使用当前单词预测上下文单词的周围窗口，或者给定一个单词，预测在该上下文中可能出现在它附近的其他单词的概率。众所周知，Skip-gram 对常用词和生僻字都有很好的效果。让我们看一个例句，为 2 的窗口创建一个跳转程序(图 [5-9](#Fig9) )。用黄色突出显示的单词是输入单词。

你可以为 Word2Vec 下载谷歌的预训练模型(从以下链接),它包括从谷歌新闻数据集中的 1000 亿个单词中提取的 300 万个单词/短语。

URL: [`https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit`](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit)

清单 [5-36](#PC38) 提供了 Word2Vec 实现的示例代码。

```py
import gensim

# Load Google's pre-trained Word2Vec model.
model = gensim.models. KeyedVectors.load_word2vec_format('Data/GoogleNews-vectors-negative300.bin', binary=True)

model.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)
#----output----
[(u'queen', 0.7118192911148071),
 (u'monarch', 0.6189674139022827),
 (u'princess', 0.5902431607246399),
 (u'crown_prince', 0.5499460697174072),
 (u'prince', 0.5377321243286133)]

model.most_similar(['girl', 'father'], ['boy'], topn=3)
#----output----
[(u'mother', 0.831214427947998),
 (u'daughter', 0.8000643253326416),
 (u'husband', 0.769158124923706)]

model.doesnt_match("breakfast cereal dinner lunch".split())
#----output----
'cereal'

Listing 5-36Example Code for Word2Vec

```

可以在自己的数据集上训练一个 Word2Vec 模型。需要记住的关键模型参数是尺寸、窗口、最小计数和 sg(列表 [5-37](#PC39) )。

大小:向量的维数。较大的大小值需要更多的训练数据，但可以产生更精确的模型。

对于 CBOW 模型 sg = 0，对于 skip-gram 模型 SG = 1。

min_count:忽略总频率低于此的所有单词。

窗口:句子中当前单词和预测单词之间的最大距离。

```py
sentences = [['cigarette','smoking','is','injurious', 'to', 'health'],['cigarette','smoking','causes','cancer'],['cigarette','are','not','to','be','sold','to','kids']]

# train word2vec on the two sentences
model = gensim.models.Word2Vec(sentences, min_count=1, sg=1, window = 3)

model.most_similar(positive=['cigarette', 'smoking'], negative=['kids'], topn=1)
#----output----
[('injurious', 0.16142114996910095)]

Listing 5-37Example Code for Training word2vec on Your Own Dataset

```

## 推荐系统

用户体验的个性化已经成为一个高度优先的事项，并成为以消费者为中心的行业的新口号。你可能已经观察到电子商务公司为你投放个性化广告，建议你购买什么，阅读哪些新闻，观看哪个视频，在哪里/吃什么，以及你可能有兴趣在社交媒体网站上与谁(朋友/专业人士)交往。推荐系统是核心的信息过滤系统，旨在预测用户偏好，并帮助推荐正确的项目，以创建用户特定的个性化体验。推荐系统有两种:1)基于内容的过滤和 2)协同过滤(图 [5-10](#Fig10) )。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig10_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig10_HTML.jpg)

图 5-10

推荐系统

### 基于内容的过滤

这种类型的系统侧重于项目的相似性属性来给你推荐。通过一个例子可以很好地理解这一点:如果一个用户购买了某一特定类别的商品，来自同一类别的其他类似商品会被推荐给该用户(参见图 [5-10](#Fig10) )。

基于项目的相似性推荐算法可以表示为:

![$$ {\hat{x}}_{k,m}=\frac{\sum \limits_{i_b} si{m}_i\left({i}_m,{i}_b\right)\left({x}_{k,b}\right)}{\sum \limits_{i_b}\mid si{m}_i\left({i}_m,{i}_b\right)\mid } $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_Equa.png)

### 协同过滤(CF)

CF 侧重于用户的相似性属性，即它基于一个相似性度量从大的用户群中找到具有相似品味的人。实践中有两种类型的 CF 实现:基于内存的和基于模型的。

基于记忆的类型主要基于相似度算法；该算法查看相似人喜欢的项目，以创建推荐的排序列表。然后，您可以对排名列表进行排序，向用户推荐前 n 个项目。

基于用户的相似性推荐算法可以表示为:

![$$ p{r}_{x,k}={m}_x+\frac{\sum \limits_{u_y\in {N}_x}\left({r}_{y,k}-{m}_y\right)\kern0.125em \mathrm{sim}\kern0.125em \left({u}_x,{u}_y\right)}{\sum \limits_{u_y\in {N}_x}\mid \mathrm{sim}\kern0.125em \left({u}_x,{u}_y\right)\mid } $$](img/434293_2_En_5_Chapter/434293_2_En_5_Chapter_TeX_Equb.png)

让我们考虑一个电影评级的示例数据集(图 [5-11](#Fig11) )，并应用基于项目和基于用户的推荐来获得更好的理解。清单 [5-38](#PC40) 提供了一个推荐系统的示例代码。

![img/434293_2_En_5_Chapter/434293_2_En_5_Fig11_HTML.jpg](img/434293_2_En_5_Chapter/434293_2_En_5_Fig11_HTML.jpg)

图 5-11

电影分级样本数据集

```py
import numpy as np
import pandas as pd

df = pd.read_csv('Data/movie_rating.csv')

n_users = df.userID.unique().shape[0]
n_items = df.itemID.unique().shape[0]
print ('\nNumber of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))
#----output----
Number of users = 7 | Number of movies = 6

# Create user-item similarity matrices
df_matrix = np.zeros((n_users, n_items))
for line in df.itertuples():
    df_matrix[line[1]-1, line[2]-1] = line[3]

from sklearn.metrics.pairwise import pairwise_distances

user_similarity = pairwise_distances(df_matrix, metric="cosine")
item_similarity = pairwise_distances(df_matrix.T, metric="cosine")

# Top 3 similar users for user id 7
print ("Similar users for user id 7: \n", pd.DataFrame(user_similarity).loc[6,pd.DataFrame(user_similarity).loc[6,:] > 0].sort_values(ascending=False)[0:3])
#----output----
Similar users for user id 7:
3    8.000000

0    6.062178
5    5.873670

# Top 3 similar items for item id 6
print ("Similar items for item id 6: \n", pd.DataFrame(item_similarity).loc[5,pd.DataFrame(item_similarity).loc[5,:] > 0].sort_values(ascending=False)[0:3])
#----output----
0    6.557439
2    5.522681
3    4.974937

Listing 5-38Example Code for a Recommender System

```

让我们将基于用户的预测和基于项目的预测公式构建为一个函数。应用此函数来预测评级，并使用均方根误差(RMSE)来评估模型性能(列表 [5-39](#PC41) )。

```py
# Function for item based rating prediction
def item_based_prediction(rating_matrix, similarity_matrix):
    return rating_matrix.dot(similarity_matrix) / np.array([np.abs(similarity_matrix).sum(axis=1)])

# Function for user based rating prediction
def user_based_prediction(rating_matrix, similarity_matrix):
    mean_user_rating = rating_matrix.mean(axis=1)
    ratings_diff = (rating_matrix - mean_user_rating[:, np.newaxis])
    return mean_user_rating[:, np.newaxis] + similarity_matrix.dot(ratings_diff) / np.array([np.abs(similarity_matrix).sum(axis=1)]).T

item_based_prediction = item_based_prediction(df_matrix, item_similarity)
user_based_prediction = user_based_prediction(df_matrix, user_similarity)

# Calculate the RMSE

from sklearn.metrics import mean_squared_error
from math import sqrt
def rmse(prediction, actual):
    prediction = prediction[actual.nonzero()].flatten()
    actual = actual[actual.nonzero()].flatten()
    return sqrt(mean_squared_error(prediction, actual))

print ('User-based CF RMSE: ' + str(rmse(user_based_prediction, df_matrix)))
print ('Item-based CF RMSE: ' + str(rmse(item_based_prediction, df_matrix)))
#----output----
User-based CF RMSE: 1.0705767849
Item-based CF RMSE: 1.37392288971

y_user_based = pd.DataFrame(user_based_prediction)

# Predictions for movies that the user 6 hasn't rated yet
predictions = y_user_based.loc[6,pd.DataFrame(df_matrix).loc[6,:] == 0]
top = predictions.sort_values(ascending=False).head(n=1)
recommendations = pd.DataFrame(data=top)
recommendations.columns = ['Predicted Rating']
print (recommendations)
#----output----
   Predicted Rating
1          2.282415

y_item_based = pd.DataFrame(item_based_prediction)

# Predictions for movies that the user 6 hasn't rated yet
predictions = y_item_based.loc[6,pd.DataFrame(df_matrix).loc[6,:] == 0]
top = predictions.sort_values(ascending=False).head(n=1)
recommendations = pd.DataFrame(data=top)
recommendations.columns = ['Predicted Rating']
print (recommendations)
#----output----
   Predicted Rating

5          2.262497

Listing 5-39Example Code for Recommender System and Accuracy Evaluation

```

基于每个用户，推荐电影《星际穿越》(索引号 5)。根据项目，推荐的电影是《复仇者联盟》(索引号 1)。

基于模型的 CF 是基于矩阵分解(MF)的，如奇异值分解和 NMF 等。让我们看看如何使用 SVD 实现(清单 [5-40](#PC42) )。

```py
# calculate sparsity level
sparsity=round(1.0-len(df)/float(n_users∗n_items),3)
print 'The sparsity level of is ' +  str(sparsity∗100) + '%'

import scipy.sparse as sp
from scipy.sparse.linalg import svds

# Get SVD components from train matrix. Choose k.
u, s, vt = svds(df_matrix, k = 5)
s_diag_matrix=np.diag(s)
X_pred = np.dot(np.dot(u, s_diag_matrix), vt)
print ('User-based CF MSE: ' + str(rmse(X_pred, df_matrix)))
#----output----
The sparsity level of is 0.0%

User-based CF MSE: 0.015742898995

Listing 5-40Example Code for Recommender System Using SVD

```

注意，在我们的例子中，数据集很小，因此稀疏度是 0%。推荐你在 MovieLens 100k 数据集上试试这个方法，可以从 [`https://grouplens.org/datasets/movielens/100k/`](https://grouplens.org/datasets/movielens/100k/) 下载。

## 摘要

在这一步中，您学习了文本挖掘过程的基础，以及从各种文件格式中提取文本的不同工具/技术。您还了解了从数据中去除噪声的基本文本预处理步骤，以及更好地理解手头语料库的不同可视化技术。然后，您学习了各种模型，这些模型可以用来理解关系并从数据中获得洞察力。

我们还学习了两种重要的推荐系统方法，如基于内容的过滤和协同过滤。