# 三、神经网络

第 [2](2.html) 章介绍了机器学习(ML ),这是人工智能研究的一个子领域。深度学习(DL)是 ML 的一个子集。它试图解决人工智能领域的具体问题，如推理、规划和知识表示。

深度学习以*人工神经网络*(ann)为中心，这些网络功能强大，可扩展，用途广泛。谷歌图像使用人工神经网络来大规模分类数十亿张图像。从数学的角度来看，*神经网络* (NN)可以看作是一个非线性回归模型，对模型函数有特定的选择。线性模型被输入到非线性函数中，然后表达式被多次嵌套。这些模型来源于生物神经网络[18]。它们代表的图形让人想起由突触连接的神经元。

Note

本章从应用的角度看经典人工神经网络的基础。它还着眼于深度学习技术，如学习、方差、偏差、神经网络，以及通过使用公共可用数据类型的一些动手练习来接触它。代码可以从该书的网站上下载。神经网络本身是一个庞大的领域，不可能在一章中包括该主题的所有方面。我们在阅读这本书时会重温这些概念，并看看它们如何在数据驱动的预测中与量子理论相适应。

人工神经网络的灵感来自于将人脑的逻辑功能映射到计算系统以创造智能机器的愿望。马文·明斯基是人工智能的创始人之一。他在 1954 年提交给普林斯顿大学的博士论文题目是“神经网络和大脑模型问题”1951 年，他建立了一个空军科学研究办公室资助的机器，见证了 SNARC(随机神经模拟强化计算器)的实现。当 A. C. Clarke 的传奇科幻小说《2001:太空漫游》被拍成电影时，他是导演斯坦利·库布里克的顾问。

在分类过程中，算法上只进行一次*正向传递*。正向传递只是输入通过神经网络时发生的计算的总和。前馈的逆过程被称为*反向传播*。

建立一个神经网络需要一个结构化的方法，并由以下整体组件组成。

*   关于网络层数的决定

*   输入的大小(通常与输入层中神经元的数量相同)

*   隐藏层中的神经元数量(如果有)

*   输出层中的神经元数量

*   权重的初始值

*   偏差的初始值

## 感知器

从数学上讲，神经元的学习过程包括通过反向传播在训练过程中修改权重和偏差。其思想是在分类时测量网络误差，然后通过修改权重来最小化该误差。弗兰克·罗森布拉特于 1962 年发明的感知机是神经网络的组成部分。感知器由一个*二进制阈值神经元*组成，定义如下。

![$$ z=b+\sum \limits_i{w}_i{x}_i $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equa.png)

(3.1)

![$$ y=\left\{\begin{array}{c}1,\kern0.5em z\ge 0\\ {}0,\kern0.5em otherwise\end{array}\right. $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equb.png)

(3.2)

其中，*x*<sub>T3】IT5】为输入， *w* <sub>*i*</sub> 为权重， *b* 为偏差， *z* 为 *logit* 或*加权和*。输入和输出要么是实数，要么是二进制数。等式 [3.2](#Equb) 定义了用非线性得出的决策。可以将等式 [3.1](#Equa) 中的偏差吸收为权重，使得可以仅使用更新权重的规则。如图 [3-1](#Fig1) 所示。</sub>

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig1_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig1_HTML.jpg)

图 3-1

偏置吸收

为了实现这一点，输入*x*T2 0 被添加到等式 [3.1](#Equa) 中，值为 1，偏差作为其权重。

![$$ z=b+\sum \limits_i{w}_i{x}_i={w}_0{x}_0+{w}_1{x}_1+{w}_2{x}_2+\dots $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equc.png)

用*b*=*w*<sub>0</sub>*x*<sub>0</sub>

在前面的等式中， *b* 可以是 *x* <sub>0</sub> 而 *w* <sub>0</sub> = 1 或者 *b* 可以是 *w* <sub>0</sub> 而 *x* <sub>0</sub> = 1。

输入永远不会改变，我们要做的是改变偏差。因此，输入被视为权重。这个过程被称为*偏置吸收*。

下面描述感知器训练规则的基础。

1.  选择一个训练场景。

2.  如果预测输出与输出标签匹配，则无需采取任何措施。

3.  如果感知器在应该预测 1 的时候预测了 0，则输入向量被添加到权重向量。

4.  如果感知器在应该预测 0 的时候预测了 1，则从权重向量中减去输入向量。

通常，一个神经网络由 *N 个*输入组成，比如说*I*<sub>T5】N</sub>和 *N* 权值如图 [3-2](#Fig2) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig2_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig2_HTML.jpg)

图 3-2

简单神经网络

在图 [3-2](#Fig2) 所示的简单 NN 中，输入为*二进制*(开/关)。在该模型中，输入和权重相加，通常为阶跃函数的*阈值函数*给出输出。以下是这种模式的局限性。

*   输入和输出是二进制的

*   没有自动训练重量的方法；因此，权重的设置是手动完成的

与图 [3-2](#Fig2) 所示的简单神经网络相比，图 [3-3](#Fig3) 所示的感知器模型更加通用，因为输入可以是数字，而*不必是二进制的*。感知器模型是前馈神经网络的最简单形式。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig3_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig3_HTML.jpg)

图 3-3

简单感知器

图 [3-3](#Fig3) 所示的广义感知器模型有输入和权重，然后是方程 [3.1](#Equa) 和 [3.2](#Equb) 描述的偏置吸收。如果我们重温方程 [3.1](#Equa) ，感知器的计算涉及由

![$$ z=f\left(x;w\right)=\phi \left({\mathbf{w}}^{\mathbf{T}}.\mathbf{x}\right) $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equd.png)

(3.3)给出的模型函数

其中 **w** <sup>**T**</sup> 是权重乘以输入向量 **x** 的矩阵表示。 *ϕ* 是依赖于输入和权重的函数。 *f* 与模型功能 *z* 相同。这里，输入和输出要么是实数，要么是二进制数。非线性函数 *ϕ* 被称为*激活函数*(稍后讨论)。输入馈入后，求和在过程开始时完成。

在过程的末尾，应用阶跃函数。以下是常见的阶跃函数。

![$$ z=\left\{\begin{array}{c}0,\kern0.5em z&lt;0\\ {}1,\kern0.5em z\ge 0\end{array}\right. $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_IEq1.png)，![$$ \mathit{\operatorname{sgn}}\ (z)=\left\{\begin{array}{c}-1,\kern0.5em z&lt;0\kern0.5em \\ {}0,z=0\\ {}1,z&gt;0\end{array}\kern10.5em 3.4\right. $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_IEq2.png)

其中 *sgn* 是*符号*函数。当一层中的神经元连接到它之前层中的所有其他神经元(即输入神经元)时，该层是*完全连接的*。

感知器的一个简单例子是如果一个人要去看板球比赛。去或不去的决定取决于几个因素，或者在这种情况下，输入。

*   天气:如果下雨，板球比赛将不会举行

*   由于没有汽车，场馆靠近公共交通

*   如果有朋友也可以加入提供优质服务的公司

*   如果喜爱的明星玩家正在参与游戏

问题的感知器如图 [3-4](#Fig4) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig4_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig4_HTML.jpg)

图 3-4

板球比赛的简单感知器示例

通过给每个输入分配权重来计算输出。根据预先确定的决策标准，输出是二进制的(是/否),如果最终得分高于 60，则为“是”,否则为“否”。权重会根据“如果朋友不能加入怎么办”等问题的答案影响决策或者“如果下雨，但只是一个小时呢？”或者“如果明星球员不上场呢？”。再次注意，这里所有的权重操作都是手动完成的。

### 激活功能

神经网络中的激活函数定义了如何将输入的加权和转换成网络层中一个或多个节点的输出。为了理解激活功能，让我们假设我们有一组狗的图像和另一组不包含狗的图像。假设每个神经元从图像中单个像素的值接收输入。当计算机处理这些图像时，我们希望我们的神经元调整其权重和偏差，使错误识别的图像越来越少。这种方法看起来很直观，但它要求权重或偏差有很小的变化，从而只引起输出有很小的变化。因为，如果我们在输出上有一个大的跳跃，我们就失去了学习曲线中的“平滑度”,并且不能渐进地学习。用人类对儿童学习过程的类比来说，学习是一小步一小步地完成的(或者说，一点一点地)。不幸的是，感知器并没有表现出这种“小步学习”的行为。感知器要么是 0，要么是 1，这是一个巨大的跳跃，对学习没有帮助，如图 [3-5](#Fig5) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig5_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig5_HTML.jpg)

图 3-5

感知器输出

为了纠正这种渐进式学习的缺乏，需要“某种东西”来平滑图 [3-5](#Fig5) 中 0 和 1 之间的陡峭步骤。基本上，我们需要一个从 0 到 1 连续变化的函数。从数学上来说，我们需要的是一个连续函数，它允许我们计算导数。在数学中，导数是函数在连续函数的给定点上如何变化。对于输入由实数给出的函数，导数是图上一点切线的斜率。

在图 [3-4](#Fig4) 的板球比赛的例子中，我们在神经元输出之后的*激活函数中定义了一个大于 60 的阈值。这个激活函数用于在我们的板球比赛示例中注入“平滑度”。*

没有激活函数，感知器的输出将是线性的，并且线性函数趋于简单。他们很难解决复杂的问题。神经网络(NNs)被视为*通用函数逼近器*，因为它们可以计算和学习几乎任何函数。因此，需要*非线性*，以便神经网络可以计算复杂函数。因此，需要*激活功能*。

以下是一些最常见的激活功能。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig6_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig6_HTML.jpg)

图 3-6

线性激活函数

*   **线性**:感知器的线性输出中，函数的导数为零，因为函数是线性的。线性激活函数常用于*线性回归*。因此，没有激活函数的神经网络通常被视为线性回归模型。图 [3-6](#Fig6) 描绘了一个线性激活函数。

*   **Sigmoid**: The sigmoid function is one of the most widely used nonlinear activation functions that transform the values between the range 0 and 1\. Mathematically, the Sigmoid function is continuous, and a typical sigmoid function, represented in Figure [3-7](#Fig7), is given by

    ![$$ \sigma (z)=\frac{1}{1+{e}^{-z}} $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Eque.png)

对于(∞、∞)之间的输入变化，sigmoid 会在范围(0，1)内产生较小的输出变化。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig7_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig7_HTML.jpg)

图 3-7

ReLU、tanh 和 sigmoid 函数(来源[273])

sigmoid 函数始终具有明确定义的非零导数；因此，可以使用梯度下降。从历史上看，乙状结肠函数非常受欢迎。最近，ReLU 函数越来越受欢迎。

*   **Tanh**: Tanh (see Figure [3-7](#Fig7)) is another popular activation function mathematically defined as

    ![$$ \tanh z=2\sigma (2z)-1=\frac{e^z-{e}^{-z}}{e^z+{e}^{-z}} $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equf.png)

Tanh 和乙状结肠密切相关，tanh 被认为是“拉伸”的乙状结肠。像 sigmoid 一样，tanh 是 S 形的、连续的、可微分的。Tanh 围绕零对称，范围从–1 到+1，而 sigmoid 范围从 0 到+1。

*   **ReLU**: A *rectified linear unit* is another nonlinear activation function that has gained popularity in deep learning. The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons simultaneously. ReLU (see Figure [3-7](#Fig7)) is mathematically defined as follows.

    ![$$ \mathrm{ReLU}(z)=\max \left(0,z\right) $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equg.png)

ReLU 是一个简单的函数，因为它有助于解决使用 sigmoid 观察到的一些优化问题而变得流行起来。该函数对于负值为零，对于正值线性增长，如图 [3-7](#Fig7) 所示。ReLU 的实现也非常简单，而 sigmoid 的实现要复杂得多。

有关其他激活功能的更多信息，请阅读古德菲勒等人在[`www.deeplearningbook.org`](http://www.deeplearningbook.org)【274】发表的*深度学习*。

### 隐藏层

当我们讨论 SVM 特征和核的类型时，您看到了多项式核提供了更多的灵活性，因为与线性核相比，多项式核的决策边界是曲线。类似地，在神经网络中，通常需要灵活的决策边界。一个*隐藏层*是实现这一点的一种方式。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig8_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig8_HTML.jpg)

图 3-8

具有单个隐藏层的 NN(使用 TensorFlow 操场创建，参考文献[27])

图 [3-8](#Fig8) 中标记的红色区域显示了用 tensor flow Playground【27】创建的前馈网络中的一个*单隐层*。多个隐藏层允许更复杂的决策边界。尽管取决于要求，在一个神经网络中可以有几个隐藏层；通常，一个隐层就足以解决大多数非线性问题。最近的研究表明，任何函数都可以用一个具有一个隐层的足够大的神经网络来表示(Lin，Jegelka，2018)[19]；然而，训练这个网络可能是一个挑战。随着整个行业对功能集的要求和需求的增长，考虑多个隐藏层已经成为一种常见的做法。

### 反向传播

反向传播是前馈的逆过程。一个简单的逻辑回归网络，严格来说，不需要任何反向传播。然而，随着层数的增加，这些计算的数量也会增加。这些层不一定会增加计算的复杂性，但会增加计算量。

NN 的经验法则之一是*误差的反向传播是梯度下降*。梯度下降是前馈网络最常见的训练算法之一，数学上这就是反向传播，定义为

![$$ {w}_{ij}^{n+1}={w}_{ij}^n-\eta \mathbf{\nabla}E={w}_{ij}^n-\eta \frac{\partial C\left({w}_1,{w}_2\right)}{\partial {w}_{ij}} $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equh.png)

(3.5)

其中，当前权重![$$ {w}_{ij}^{n+1} $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_IEq3.png)是更新后的权重，![$$ {w}_{ij}^n $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_IEq4.png)是朝着最陡梯度方向向下的前一步的权重， *η* 是学习率， *E* 是成本函数， *C* 是以微分形式的 *E* 的数学等价物，对*w*<sub>*ij*</sub>求导以获得梯度。

### 实验操作:带 TensorFlow 操场的 NN

TensorFlow [20]是 Google 发明的，通过利用张量在机器学习中高效地使用图形模型，并迅速成为非常受欢迎的 ML 库。TensorFlow 是谷歌为快速机器学习任务制作的免费开源软件库。它可以用于一系列任务，但侧重于深度神经网络的训练和推理。TensorFlow 是一个基于数据流和可微分编程的符号数学库。TensorFlow 的文档可在 [`www.tensorflow.org`](http://www.tensorflow.org) 获得。由于其利用图形属性的特性，许多 TensorFlow 实现在 GPU(图形处理单元)上运行比在 CPU 上运行更高效。Google 推出了 TPUs(张量处理单元)来优化 TensorFlow 的代码性能。在写这本书的时候，TensorFlow 的版本是 2.x。

Keras 是另一个流行的 ML 工具。Keras 是一个高级 API，它允许我们构建、评估、训练和执行所有类型的神经网络。Keras 的原始版本可以在 [`https://github.com/keras-team/keras`](https://github.com/keras-team/keras) 获得。Keras 的多个其他实现已经发布。三个最受欢迎的开源深度学习库是 TensorFlow、微软认知工具包(CNTK)和 Theano。在本书中，我们使用与 TensorFlow 捆绑在一起的 Keras 实现。

为了体验神经网络及其工作方式，在我们开始详细编码之前，有一个资源可以让我们体验神经网络的性能和架构，并使其可视化。它被称为 TensorFlow 游乐场[27]。

以下步骤模拟神经网络。

**第一步**:将浏览器指向 [`http://playground.tensorflow.org`](http://playground.tensorflow.org) 。如图 [3-9](#Fig9) 所示，这应该会打开我们用来*玩*一些概念的游戏的 GUI。这个神经网络模拟器帮助用户习惯各种神经网络算法的概念。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig9_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig9_HTML.jpg)

图 3-9

tensorflow 游乐场 GUI

对于接下来的步骤，请参考图 [3-10](#Fig10) 。相应的步骤用深蓝色数字标记。

**第二步**:选择数据。

**第三步**:选择特征。

**第四步**:设计神经网络。

**第五步**:调整参数。

**第六步**:运行。

**第七步**:检查结果。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig10_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig10_HTML.jpg)

图 3-10

TensorFlow 游乐场配置

**隐藏图层**:选择**无隐藏图层**。这将改变顶部隐藏层标题旁边的 2 为 0，并相应地改变 NN，如图 [3-11](#Fig11) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig11_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig11_HTML.jpg)

图 3-11

没有隐藏层的 TensorFlow 游乐场配置

通过消除隐藏层来观察输出的变化。输出中的域边界变得线性地更加明确，复杂性降低。

#### 线性回归:

**第一步**:在右上角点击**问题类型**下拉菜单，选择**回归**。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figa_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figa_HTML.jpg)

**第二步**:在左下方选择**数据**作为数据集。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figb_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figb_HTML.jpg)

**第三步**:在顶部点击下拉菜单，选择**学习率**的**最低设置**。学习率是用于梯度下降的“步长”。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figc_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figc_HTML.jpg)

**第四步** : **激活**是添加到输出的功能。选项有线性、Sigmoid、Tanh 和 ReLU。选择**直线**。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figd_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figd_HTML.jpg)

**第五步** : **正则化率**指的是*正则化*，减少过拟合。该选项可以用 0、L1 和 L2 之间的值进行试验，以估计如果有任何过度拟合需要多少正则化。

**第六步**:点击运行/开始/播放按钮。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fige_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fige_HTML.jpg)

**第七步**:观察输出曲线。它似乎在慢慢收敛。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figf_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figf_HTML.jpg)

延迟的原因与学习率的值有关。这个模型似乎学得太慢了。目标是使用尽可能少的时期尽可能快地达到收敛。输出中提到的**损耗**就是**误差**。在这种情况下，它是均方根误差(RMSE)。

**步骤 7** :点击学习率下拉菜单，**增加**学习率，再次点击运行。算法运行速度有变化吗？

**步骤 8** :再次点击学习率下拉菜单，**增加**学习率到**高值，**再次点击**运行**。输出测试损耗显示为 **NaN** 或*而非数字*。学习率的大值会导致过冲。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figg_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figg_HTML.jpg)

为了找到最佳学习率，我们需要确定学习率的值，该值使用最少的时期使我们最接近零。

**分类** **:**

**第一步**:模拟分类，选择以下参数。

1.  在**问题类型**下拉菜单中选择**分类**。

2.  在**激活**下拉菜单中选择 Tanh

3.  在**学习率**下拉菜单中选择 **0.01** 。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figh_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figh_HTML.jpg)

**步骤 2** :选择双斑点数据集。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figi_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figi_HTML.jpg)

**步骤 3** :分离数据集的输出可能如下图所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figj_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figj_HTML.jpg)

**步骤 4** :零损耗可能无法实现，尤其是在模型中引入一些噪声的情况下

**第 5 步**:和之前的线性回归实验室一样，挑战在于调整*学习率*以在尽可能少的时期内达到最小损失。

鼓励读者在 TensorFlow 游乐场玩一玩，花一些时间更改和调整隐藏图层、回归和分类的参数，同时在左侧添加新功能。随着顶部隐藏图层的数量发生变化，该工具会添加更多的隐藏图层。同样，单击“特征”下的方框会向 NN 模型添加新特征。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figk_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figk_HTML.jpg)

更改数据下的数据集类型，如棘手的**螺旋**数据集，以熟悉各种 NN 模型随参数变化的行为。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figl_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figl_HTML.jpg)

接下来，让我们进入神经网络架构的世界。

### 神经网络体系结构

我们已经了解了神经网络的基本原理、灵活性和可扩展性，以及感知器是如何构建神经网络的。从体系结构的角度来看，构建神经网络有几种选择。下面是一些现在使用的比较常见的方法。

*   卷积神经网络(CNN)

*   前馈神经网络

*   循环神经网络(RNN)

### 卷积神经网络(CNN)

1981 年诺贝尔生理学和医学奖授予了大卫·H·哈贝尔和托尔斯滕·魏塞尔，以表彰他们在 1968 年发表的论文“猴子纹状皮层的感受野和功能结构”[20]。他们研究了动物的视觉皮层，发现视野中的小区域的活动与大脑中一个小的、明确的区域的活动相关联。他们表明，识别负责部分视野的神经元是可能的。这导致了*感受野*的发现。

通常，在图像分析中，使用的技术之一是*展平*图像，其中各层被组合或堆叠在彼此之上。前面讨论的逻辑回归技术允许我们将图像向量输入到一个完成所有处理工作的单个神经元中。这两种技术(展平和逻辑回归)与感受野的概念相结合，为我们提供了如何构建卷积神经网络(CNN)的指南。为了在 CNN 中有效使用，逻辑回归部分使用不同的激活函数，但结构保持不变。

*展平*允许将图像合并到一个图层中。这使得我们可以在一个层中拥有所有可见的信息。这是构建神经网络的重要部分，因为全连接层的输入由一维数组组成。展平的概念如图 [3-12](#Fig12) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig12_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig12_HTML.jpg)

图 3-12

扁平化(medium.com 提供[21])

卷积层获取图像，并对整个图像进行逻辑回归。因此，第一个输入由展平向量的分量组成，第二个输入由接下来的分量组成，依此类推。该过程如图 [3-13](#Fig13) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig13_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig13_HTML.jpg)

图 3-13

卷积神经网络

CNN 有多层。CNN 是专门为处理图像而设计的。卷积层神经元扫描图像的特定区域，这是它们的*视野* (FOV)，并拾取图像中的图案，如眼睛、耳朵、鼻子、车轮等等。例如，可以让 CNN 处理一个大小为 10 的图像，其所有三层都有一个 3 × 3 的感受野。假设第一层具有随机分配偏差和权重的特性，则它会获取 10X10 的图像，并生成大小为 8 × 8 的图像。然后将这个大小赋予第二个 CNN 层。第二层可以有自己的感受野，具有随机初始化的偏差和权重，但对于这个例子，让我们假设它也是 3 × 3。这产生 6 × 6 的输出，然后传递到具有第三局部感受野的第三层。第三层 CNN 产生一个 4 × 4 的图像。然后，该图像被展平为 16 维向量，并被馈送到具有一个输出神经元并使用非线性逻辑函数的标准全连接层。逻辑函数给出 0 和 1 之间的输出，并比较输出和图像标签。然后，它计算误差，反向传播误差，并对数据集中经过整个网络训练的每幅图像重复这一过程。

训练 CNN 包括训练局部感受野以及全连接层的权重和偏差。逻辑回归和局部感受野之间的结构差异在于前者使用逻辑函数。但是，在后者中，可以使用任何激活功能。广泛使用的激活功能之一是*整流线性单元*或 ReLU。输入 *x* 的 ReLU 由 *x* 和 0 之间的最大值给出(即，如果输入为负，则返回 0，否则返回原始输入)。CNN 广泛应用于计算机视觉领域。

### 前馈神经网络

如前所述，单层神经网络可以解决简单的线性问题，但难以解决复杂性。为了解决更复杂的问题，引入了非线性。*前馈神经网络* (FFNN)是一种实现解决复杂性所需的非线性的方法。它也被称为*多层感知器* (MLP)和*深度前馈神经网络* (DFNN)。

图 [3-11](#Fig11) 所示的前馈神经网络具有以下形式

![$$ f\left(\mathbf{x},{\mathbf{w}}_{\mathbf{1}},{\mathbf{w}}_{\mathbf{2}},\dots \right)=\dots {\phi}_2\left({\mathbf{w}}_{\mathbf{2}}{\phi}_1\left({\mathbf{w}}_{\mathbf{1}}\mathbf{x}\right)\right) $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equi.png)

(3.6)

其中，*<sub>1</sub>， *ϕ* <sub>2</sub> 等为非线性激活函数。 *w* <sub>*i*</sub> 是定义为权重矩阵的参数。模型函数由级联的线性模型和函数形成。这种连接可以重复几次。这是神经网络威力的一个例子:它们利用了线性代数的数据处理能力与非线性动力学的灵活性的结合。*

 *![img/502577_1_En_3_Chapter/502577_1_En_3_Fig14_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig14_HTML.jpg)

图 3-14

单隐层简单前馈神经网络

图 [3-14](#Fig14) 显示了一个具有两个隐藏层的前馈神经网络。前馈神经网络是通过连接多层感知器构成的，使得每层的每个单元都连接到下一层的单元。第一层由输入*x*<sub>T5】I</sub>组成，下一层是由 *h* <sub>*i*</sub> 单元组成的隐藏层，最后一层有输出 *y* <sub>*i*</sub> 。有一个激活函数来更新每个神经元，每一层都在前一层之后更新，从而形成一个*前馈*神经网络。以下来自方程 [3.6](#Equi) ，图 [3-11](#Fig11) 的单隐层前馈神经网络模型。

![$$ f\left(\mathbf{x},{\mathbf{w}}_{\mathbf{1}},{\mathbf{w}}_{\mathbf{2}}\right)={\phi}_2\left({\mathbf{w}}_{\mathbf{2}}{\phi}_1\left({\mathbf{w}}_{\mathbf{1}}\mathbf{x}\right)\right) $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equj.png)

前馈 NN 可以与许多可能的*激活函数* ( *ϕ* )组合，例如 *tanh* 、 *sigmoid* 和 *ReLU* 。关于激活函数的详细讨论可以在参考文献[15]中找到，我们将在接下来的章节中根据需要对其进行扩展。

以下是前馈神经网络的典型特征。

*   它由多层组成。

*   每一层都有几个神经元。

*   每个神经元都与前一层的神经元相连。

*   信息只通过一种方式流动；因此前馈。

*   它由*输入*、*输出、*和*隐藏*层组成。如果需要，可以有多个隐藏层。

以下是设计 FFNN 时一些基本的*尺寸指南*。

*   输入层
    *   大小等于输入维度的数量。

    *   它可能需要一个额外的额外神经元作为偏置项。

    *   对于许多稀疏维，通常考虑宽而深的神经网络(具有许多层)。

*   隐藏层
    *   隐藏层的大小和数量取决于训练样本、输入特征、输出和问题的复杂性。

    *   深度学习使用多个隐藏层。

*   输出层
    *   对于回归，只需要单个神经元。

    *   对于二进制分类，只有一个神经元作为输出是二进制的(0/1，真/假…)。

    *   对于多类分类，softmax 图层在其他图层中使用。

    *   大小建议是每个类标签一个节点。

### 实验操作:使用 MNIST 数据集进行影像分析

本实验练习着眼于图像分类。我们创建并训练一个神经网络模型，在训练之后，该模型可以以很高的准确度从手写图像中预测数字。您应该对神经网络如何工作以及 TensorFlow 如何与 Keras 一起工作有一个基本的了解。

本实验是使用 MNIST 数据集完成的。MNIST 是由高中生和美国人口普查局员工手写的 7 万个数字小图像组成的集合。每个图像都标有它所代表的数字。这个集合已经被研究了很多，所以它经常被称为机器学习的“你好，世界”。每当人们开发一种新的分类算法时，他们都很好奇它在 MNIST 上的表现如何，任何学习机器学习的人迟早都会处理这个数据集。

图 [3-15](#Fig15) 描述了该任务:创建并训练一个模型，该模型将一个手写数字的图像作为输入，并预测该数字的类别；也就是说，它预测输入图像的数字或类别。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig15_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig15_HTML.jpg)

图 3-15

图像分类任务

打开名为`TF_Keras_imageClassifiaction.ipynb`的 Jupyter 笔记本文件，可从该书的网站下载。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figm_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figm_HTML.jpg)

**第一步**:导入 TensorFlow，验证版本。

```py
import tensorflow as tf
tf.logging.set_verbosity(tf.logging.ERROR)
print('Using TensorFlow version', tf.__version__)

```

**第二步**:导入 MNIST 数据。

```py
mnist=tf.keras.datasets.mnist
(x_train, y_train),(x_test, y_test)= mnist.load_data()

```

**第三步**:塑造数据。

```py
print('x_train:', x_train.shape)
print('y_train:', y_train.shape)
print('x_test:', x_test.shape)
print('y_test:', y_test.shape)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fign_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fign_HTML.jpg)

**第四步**:看一张图片。

```py
from matplotlib import pyplot as plt
%matplotlib inline
plt.imshow(x_train[0], cmap='binary')
plt.show()

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figo_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figo_HTML.jpg)

**第五步**:显示标签。

```py
y_train[0]

```

**步骤 6** :在这一步中，我们将利用热编码。每个标签被转换成具有 10 个元素的列表，并且对应类的索引元素被设置为 1，其余的都被设置为 0，例如

![img/502577_1_En_3_Chapter/502577_1_En_3_Figp_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figp_HTML.jpg)

```py
from tensorflow.keras.utils import to_categorical
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)

```

**第七步**:验证形状。

```py
print('y_train_encoded:', y_train_encoded.shape)
print('y_test_encoded:', y_test_encoded.shape)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figq_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figq_HTML.jpg)

**步骤 8** :显示编码后的标签。

```py
y_train_encoded[1]

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figr_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figr_HTML.jpg)

**步骤 9** :参见图 [3-16](#Fig16) ，显示了神经网络架构。正如在关于感知器和偏差吸收的讨论中所解释的那样，该图表示了以下等式。

![$$ z=b+w1\ast x1+w2\ast x2+w3\ast x3 $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equk.png)

其中 *w* 1、 *w* 2、 *w* 3 为权重。 *b* 是截距项，也叫*偏差*。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig16_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig16_HTML.jpg)

图 3-16

神经网络架构。测试意味着转置。这给出了我们想要的结果的点积

**第十步**:建立神经网络。

我们有一个有 784 个特征的单个神经元。这就构成了一个具有两个隐层的神经网络，如图 [3-17](#Fig17) 所示。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig17_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig17_HTML.jpg)

图 3-17

具有两个隐层的神经网络

**步骤 11** :对实例进行预处理。展开 *N* 维数组为向量。

```py
import numpy as np
x_train_reshaped = np.reshape(x_train,(60000, 784))
x_test_reshaped = np.reshape(x_test, (10000, 784))
print('x_train_reshaped:', x_train_reshaped.shape)
print('x_test_reshaped:', x_test_reshaped.shape)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figs_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figs_HTML.jpg)

**第十二步**:创建模型。

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
model = Sequential([
    Dense(128, activation='relu', input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

```

**步骤 13** :激活功能:ReLU(整流器线性单元)。为了在节点中施加激活函数，第一步是定义输入的线性和。【T2![$$ Y=W.X+b $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equl.png)

第二步是定义激活函数输出。

![$$ Z=f(Y) $$](img/502577_1_En_3_Chapter/502577_1_En_3_Chapter_TeX_Equm.png)T2】

图 [3-18](#Fig18) 显示了该功能。

![img/502577_1_En_3_Chapter/502577_1_En_3_Fig18_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Fig18_HTML.jpg)

图 3-18

具有两个隐层的神经网络

编译模型。

```py
model.compile(
    optimizer='sgd',
    loss='categorical_crossentropy', metrics=['accuracy']
)
model.summary()

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figt_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figt_HTML.jpg)

**第 14 步**:训练模型。

```py
model.fit(x_train_norm, y_train_encoded, epochs=3)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figu_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figu_HTML.jpg)

**第十五步**:评估模型。

```py
loss, accuracy = model.evaluate(x_test_norm, y_test_encoded)
print('Test Set Accuracy %', accuracy*100)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figv_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figv_HTML.jpg)

**第十六步**:预测。

```py
preds = model.predict(x_test_norm)
print ('Shape of predictions:', preds.shape)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figw_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figw_HTML.jpg)

**第 17 步**:绘制结果。

```py
plt.figure(figsize=(12, 12))
start_index = 0
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.grid(False)
    plt.xticks([])
    plt.yticks([])
    pred = np.argmax(preds[start_index+i])
    gt = y_test[start_index+i]
        col = 'b'
    if pred != gt:
        col = 'r'
            plt.xlabel('i={},pred={}, gt={}'.format(start_index+i, pred, gt), color = col)
    plt.imshow(x_test[start_index+i], cmap= 'binary')
plt.show()

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figx_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figx_HTML.jpg)

**第 18 步**:绘制结果。

```py
plt.plot(preds[8])
plt.show()

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figy_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figy_HTML.jpg)

### 实验操作:使用 Iris 数据集的深度神经网络分类器

本实验室主要使用 Iris 数据集进行深度神经网络分类。对于这个实验室，我们利用 Google Colab 云计算环境。

Google Colaboratory 是由 Google 提供的完全托管的免费服务，允许我们加载和运行用于数据科学、机器学习和深度学习的笔记本。除了 CPU，该环境还提供了利用 GPU 和 TPU 的选项。

这个实验室使用 TensorBoard，这是 TensorFlow 的可视化工具包[23]。它提供了机器学习实验所需的可视化和工具，如下所示。

*   跟踪和可视化损失和准确性等指标

*   可视化模型图(操作和层)

*   查看权重、偏差或其他张量随时间变化的直方图

*   将嵌入投影到低维空间

*   显示图像、文本和音频数据

*   剖析张量流程序

**第一步**:在 [`https://colab.research.google.com`](https://colab.research.google.com) 访问谷歌合作实验室。

该链接应打开以下页面。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figz_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figz_HTML.jpg)

**步骤 2** :点击欢迎来到合作实验室链接，访问更广阔的环境。观看介绍视频，熟悉 Google Colab 环境。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figaa_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figaa_HTML.jpg)

**第三步**:到左上角点击**文件➤上传笔记本**。您会看到以下屏幕。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figab_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figab_HTML.jpg)

**第四步**:上传 DNNIris.ipynb 笔记本文件:Chrome 调出一个你的操作系统特有的文件请求器:Finder (macOS)和 Windows Explorer (Windows)。导航到您保存的位置(例如，您的桌面)。从您的中选择下载并解压缩的实验室文件夹。zip 文件(不是。zip 文件本身)。然后使用 run 按钮或每个代码单元格左侧的箭头按钮运行它。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figac_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figac_HTML.jpg)

接下来，我们通过运行代码继续 hnads-on 练习:

**步骤 1** :第一段代码测试环境并将必要的库导入到 Google Colab 环境中。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figad_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figad_HTML.jpg)

```py
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
from tensorflow import keras

```

**第二步**:读取数据。

```py
import os
data = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
data_local = keras.utils.get_file(fname=os.path.basename(data),
                                           origin=data)
iris = pd.read_csv(data_local)
iris.columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm','Species']

iris

```

输出列出了 Iris 数据集的一个示例。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figae_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figae_HTML.jpg)

**第三步**:塑造数据。

```py
input_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']
x = iris [input_columns]
y = iris[['Species']]

print (x.head())
print('-----')
print (y.head())

```

输出显示了选定的列。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figaf_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figaf_HTML.jpg)

**步骤 4** :对标签进行编码。输出标签是类似 Iris-setosa 或 Iris-virginica 的字符串。这些被称为*分类变量*。它们需要被转换成数字，就像我们对 SVM 所做的那样。这叫做*编码*。

```py
from sklearn.preprocessing import LabelEncoder
encoder =  LabelEncoder()
y1 = encoder.fit_transform(y.values) ## need y.values which is an array
print(y1)

```

**第五步**:将数据分成训练/测试:80%用于训练，20%用于测试。

```py
from sklearn.model_selection import train_test_split
# 'y1' (encoded labels)
x_train,x_test, y_train,y_test = train_test_split(x,y1,test_size=0.2,random_state=0)
print ("x_train.shape : ", x_train.shape)
print ("y_train.shape : ", y_train.shape)
print ("x_test.shape : ", x_test.shape)
print ("y_test.shape : ", y_test.shape)

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figag_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figag_HTML.jpg)

**第六步**:建立深度 NN 模型。由于这是一个分类器，神经网络将看起来如下。

输出层中的神经元=输出类(此处为 3)输出激活是“softmax”

1.  输入层中的神经元=输入维度(此处为 4)

2.  输出层中的神经元=输出类(此处为 3)

3.  输出激活为“`softmax`”。

4.  使用 Adam optimizer，这是一种优化算法，可用来代替经典的随机梯度下降过程，根据训练数据迭代更新网络权重。

```py
input_dim = len(input_columns)
output_clases = 3
print ("input_dim : ", input_dim, ", output classes : ", output_clases)
model = tf.keras.Sequential([
            tf.keras.layers.Dense(units=64, activation=tf.nn.relu, input_dim=input_dim),
            tf.keras.layers.Dense(units=32, activation=tf.nn.relu),
            tf.keras.layers.Dense(units=output_clases,  activation=tf.nn.softmax)
            ])
# loss = 'sparse_categorical_crossentropy'
model.compile(loss='sparse_categorical_crossentropy',
                 optimizer=tf.keras.optimizers.Adam(),
                 metrics=['accuracy'])
print (model.summary())
tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)

```

下面的 NN 模型是代码片段的输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figah_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figah_HTML.jpg)

第 7 步:tensor board GUI 可能会在第一次出现 HTML 错误。**使用 TensorBoard GUI 右上角的刷新图标刷新页面**。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figai_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figai_HTML.jpg)

```py
import datetime
import os
import shutil
app_name = 'classification-iris'
tb_top_level_dir= '/tmp/tensorboard-logs'
tb_app_dir = os.path.join (tb_top_level_dir, app_name)

tb_logs_dir = os.path.join (tb_app_dir, datetime.datetime.now().strftime("%H-%M-%S"))
print ("Saving TB logs to : " , tb_logs_dir)
#clear out old logs
shutil.rmtree ( tb_app_dir, ignore_errors=True )
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_logs_dir, histogram_freq=1)

## This will embed Tensorboard right here in jupyter
%load_ext tensorboard
%tensorboard --logdir $tb_logs_dir

```

下面显示了 TensorBoard 视图中的输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figaj_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figaj_HTML.jpg)

**步骤 8** :训练数据(这可能需要一段时间，取决于纪元值)。

```py
%%time
epochs = 100  ## experiment 100, 500, 1000
print ("training starting ...")
history = model.fit(
              x_train, y_train,
              epochs=epochs, validation_split = 0.2, verbose=1,
              callbacks=[tensorboard_callback])
print ("training done.")

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figak_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figak_HTML.jpg)

**第九步**:绘制历史。

```py
%matplotlib inline
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='train_accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.legend()
plt.show()

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figal_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figal_HTML.jpg)

将此图与 TensorBoard 的图进行比较，以确定训练和验证的准确性。

**第十步**:获取预测。

```py
np.set_printoptions(formatter={'float': '{: 0.3f}'.format})
predictions = model.predict(x_test)
predictions

```

在步骤 10 的输出中，对于每个测试输入，softmax 层产生三个数字。这些数字是概率。如果把它们加起来，总数是 1.0。我们希望选择概率最高的输出。

例如,( 0.03086184，0.33362046，0.6355177)表示

*   1 类有 0.03 或 3%的概率。

*   2 类有 0.33 或 33%的概率。

*   3 类有 0.63 或 63%的概率。

因此，我们选择概率最高的类别作为预测:类别 3

**步骤 11** :评估模型。

```py
metric_names = mo

del.metrics_names
print ("model metrics : " , metric_names)
metrics = model.evaluate(x_test, y_test, verbose=0)
for idx, metric in enumerate(metric_names):
    print ("Metric : {} = {:,.2f}".format (metric_names[idx], metrics[idx]))

```

下面显示了输出。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figam_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figam_HTML.jpg)

准确度显示非常接近 1.0，因为损耗仅为 0.12。

**步骤 12** :由于这是一个分类问题，混淆矩阵是评估模型的有效方法。我们再次利用`seaborn`库进行可视化，就像我们在 SVM 做的一样。

```py
from sklearn.metrics import confusion_matrix
import seaborn as sns
cm = confusion_matrix(y_test, y_pred, labels = [0,1,2])
cm
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize = (8,5))
sns.heatmap(cm, annot=True, cmap="YlGnBu", fmt='d').plot()

from sklearn.metrics import classification_report
from pprint import pprint

pprint(classification_report(y_test, y_pred, output_dict=True))

```

以下是热图。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figan_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figan_HTML.jpg)

以下是混淆矩阵中的指标。

![img/502577_1_En_3_Chapter/502577_1_En_3_Figao_HTML.jpg](img/502577_1_En_3_Chapter/502577_1_En_3_Figao_HTML.jpg)

回到第 [2](2.html) 章中关于混淆矩阵的讨论，你能从这个输出中得出什么结论？

### 摘要

第 3 章着眼于人工神经网络，它将人脑的逻辑功能映射到计算系统，以创建智能机器。基于神经网络，深度学习解决人工智能中的特定问题，如推理、规划和知识表示。下一章涵盖量子信息处理概念的本质。*