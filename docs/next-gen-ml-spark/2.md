# 2.Spark 和 Spark MLlib 简介

> *简单的模型和大量的数据胜过基于较少数据的更复杂的模型。*
> 
> —彼得·诺维格 [<sup>我</sup>](#Sec89)

Spark 是一个统一的大数据处理框架，用于处理和分析大型数据集。Spark 提供 Scala、Python、Java 和 R 的高级 API，具有强大的库，包括用于机器学习的 MLlib、用于 SQL 支持的 Spark SQL、用于实时流的 Spark Streaming 和用于图形处理的 GraphX。 [<sup>ii</sup>](#Sec89) Spark 由马泰·扎哈里亚(Matei Zaharia)在加州大学伯克利分校的 AMPLab 创立，后捐赠给阿帕奇软件基金会，于 2014 年 2 月 24 日成为顶级项目。 [<sup>iii</sup>](#Sec89) 第一版于 2017 年 5 月 30 日发布。 [<sup>iv</sup>](#Sec89)

## 概观

Spark 的开发是为了解决 Hadoop 最初的数据处理框架 MapReduce 的局限性。Matei Zaharia 在加州大学伯克利分校和脸书分校(他在那里实习)看到了 MapReduce 的局限性，并试图创建一个更快、更通用、多用途的数据处理框架，以处理迭代和交互式应用程序。 [<sup>v</sup>](#Sec89) 它提供了一个统一的平台(图 [2-1](#Fig1) )，支持流式、交互式、图形处理、机器学习、批处理等多种类型的工作负载。 [<sup>vi</sup>](#Sec89) Spark 作业的运行速度比同等的 MapReduce 作业快好几倍，这是因为它具有快速的内存功能和高级 DAG(定向非循环图)执行引擎。Spark 是用 Scala 编写的，因此它是 Spark 事实上的编程接口。我们将在整本书中使用 Scala。我们将在第 [7](7.html) 章中使用 PySpark，这是用于 Spark 的 Python API，用于分布式深度学习。本章是我上一本书*下一代大数据*(2018 年出版)中第 [5](5.html) 章的更新版本。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig1_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig1_HTML.png)

图 2-1

Apache Spark 生态系统

### 集群管理器

集群管理器管理和分配集群资源。Spark 支持 Spark(独立调度程序)、YARN、Mesos 和 Kubernetes 附带的独立集群管理器。

## 体系结构

在高层次上，Spark 将 Spark 应用程序任务的执行分布在集群节点上(图 [2-2](#Fig2) )。每个 Spark 应用程序在其驱动程序中都有一个 SparkContext 对象。SparkContext 表示到集群管理器的连接，集群管理器为 Spark 应用程序提供计算资源。在连接到集群之后，Spark 在您的 worker 节点上获取执行器。然后 Spark 将您的应用程序代码发送给执行器。一个应用程序通常会运行一个或多个作业来响应一个 Spark 动作。然后，Spark 将每个作业划分为更小的阶段或任务的有向无环图(DAG)。然后，每个任务被分发并发送给工作节点上的执行器来执行。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig2_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig2_HTML.png)

图 2-2

Apache Spark 架构

每个 Spark 应用程序都有自己的一组执行器。因为来自不同应用程序的任务在不同的 JVM 中运行，所以一个 Spark 应用程序不会干扰另一个 Spark 应用程序。这也意味着，如果不使用外部数据源，比如 HDFS 或 S3，Spark 应用程序很难共享数据。使用 Tachyon(又名 Alluxio)等堆外内存存储可以使数据共享更快更容易。我将在这一章的后面更详细地讨论 Alluxio。

## 执行 Spark 应用程序

您可以使用交互式 shell (spark-shell 或 pyspark)或提交应用程序(spark-submit)来执行 spark 应用程序。一些人更喜欢使用基于网络的交互式笔记本，如 Apache Zeppelin 和 Jupyter，来与 Spark 进行交互。Databricks 和 Cloudera 等商业供应商也提供了他们自己的交互式笔记本环境。我将在整章中使用火花壳。在带有集群管理器(如 YARN)的环境中，启动 Spark 应用程序有两种部署模式。

### 集群模式

在集群模式下，驱动程序运行在由 YARN 管理的主应用程序中。客户端可以退出而不影响应用程序的执行。以集群模式启动应用程序或 spark-shell:

```py
spark-shell --master yarn --deploy-mode cluster

spark-submit --class mypath.myClass --master yarn --deploy-mode cluster

```

### 客户端模式

在客户端模式下，驱动程序在客户端运行。应用程序主机仅用于向 YARN 请求资源。要在客户端模式下启动应用程序或 spark-shell:

```py
spark-shell --master yarn --deploy-mode client

spark-submit --class mypath.myClass --master yarn --deploy-mode client

```

## 火花壳简介

您通常使用交互式 shell 进行特定的数据分析或探索。也是学习 Spark API 的好工具。Spark 的交互 shell 有 Spark 或者 Python 两种版本。在下面的示例中，我们将创建一个城市 RDD，并将它们全部转换为大写字母。当您启动 spark-shell 时，会自动创建一个名为“spark”的 SparkSession，如清单 [2-1](#PC3) 所示。

```py
spark-shell

Spark context Web UI available at http://10.0.2.15:4041
Spark context available as 'sc' (master = local[*], app id = local-1574144576837).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_212)
Type in expressions to have them evaluated.
Type :help for more information.

scala>val myCities = sc.parallelize(List(
                                   "tokyo",
                                   "new york",
                                   "sydney",
                                   "san francisco"))

scala>val uCities = myCities.map {x =>x.toUpperCase}

scala>uCities.collect.foreach(println)
TOKYO
NEW YORK
SYDNEY
SAN FRANCISCO

Listing 2-1Introduction to spark-shell

```

### 火花会议

如图 [2-2](#Fig2) 所示，SparkContext 支持访问所有 Spark 特性和功能。驱动程序使用 SparkContext 来访问其他上下文，如 StreamingContext、SQLContext 和 HiveContext。从 Spark 2.0 开始，SparkSession 提供了与 Spark 交互的单一入口点。Spark 1.x 中通过 SparkContext、SQLContext、HiveContext 和 StreamingContext 提供的所有功能现在都可以通过 SparkSession 访问。 [<sup>vii</sup>](#Sec89) 你可能仍然会遇到用 Spark 1.x 编写的代码。

```py
val sparkConf = new SparkConf().setAppName("MyApp").setMaster("local")

val sc = new SparkContext(sparkConf).set("spark.executor.cores", "4")

val sqlContext = new org.apache.spark.sql.SQLContext(sc)

```

在 Spark 2.x 中，您不必显式创建 SparkConf、SparkContext 或 SQLContext，因为它们的所有功能都已经包含在 SparkSession 中。

```py
val spark = SparkSession.
builder().
appName("MyApp").
config("spark.executor.cores", "4").
getOrCreate()

```

### 弹性分布式数据集(RDD)

RDD 是一个有弹性的不可变分布式对象集合，跨集群中的一个或多个节点进行分区。rdd 可以通过两种类型的操作并行处理和操作:转换和操作。

Note

RDD 是 Spark 1.x 中 Spark 的主要编程接口。从 Spark 2.0 开始，数据集已经取代 RDD 成为主要的 API。由于更丰富的编程界面和更好的性能，建议用户从 RDD 切换到数据集/数据框架。我将在本章后面讨论数据集和数据帧。

#### 创建 RDD

创建 RDD 非常简单。你可以从现有的 Scala 集合中创建一个 RDD，或者从存储在 HDFS 或 S3 的外部文件中读取。

##### 平行放置

并行化从 Scala 集合创建一个 RDD。

```py
val data = (1 to 5).toList
val rdd = sc.parallelize(data)
val cities = sc.parallelize(List("tokyo","new york","sydney","san francisco"))

```

##### 文本文件

文本文件从储存在 HDFS 或 S3 的文本文件创建 RDD。

```py
val rdd = sc.textFile("hdfs://master01:9000/files/mydirectory")

val rdd = sc.textFile("s3a://mybucket/files/mydata.csv")

```

请注意，RDD 是不可变的。数据转换会产生另一个 RDD，而不是修改当前的 RDD。RDD 操作可以分为两类:转换和行动。

#### 转换

转换是创建新 RDD 的操作。我描述了一些最常见的转换。有关完整的列表，请参考在线 Spark 文档。

##### 地图

Map 对 RDD 中的每个元素执行一个函数。它创建并返回结果的新 RDD。地图的返回类型不必与原始 RDD 的类型相同。

```py
val cities = sc.parallelize(List("tokyo","new york","paris","san francisco"))
val upperCaseCities = myCities.map {x =>x.toUpperCase}
upperCaseCities.collect.foreach(println)
TOKYO
NEW YORK
PARIS
SAN FRANCISCO

```

让我们展示另一个地图的例子。

```py
val lines = sc.parallelize(List("Michael Jordan", "iPhone"))

val words = lines.map(line =>line.split(" "))

words.collect

res2: Array[Array[String]] = Array(Array(Michael, Jordan), Array(iPhone))

```

##### 平面地图

FlatMap 对 RDD 中的每个元素执行一个函数，然后对结果进行拼合。

```py
val lines = sc.parallelize(List("Michael Jordan", "iPhone"))

val words = lines.flatMap(line =>line.split(" "))

words.collect

res3: Array[String] = Array(Michael, Jordan, iPhone)

```

##### 过滤器

Filter 返回一个仅包含与指定条件匹配的元素的 RDD。

```py
val lines = sc.parallelize(List("Michael Jordan", "iPhone","Michael Corleone"))

val words = lines.map(line =>line.split(" "))

val results = words.filter(w =>w.contains("Michael"))

results.collect

res9: Array[Array[String]] = Array(Array(Michael, Jordan), Array(Michael, Corleone))

```

##### 明显的

Distinct 只返回不同的值。

```py
val cities1 = sc.parallelize(List("tokyo","tokyo","paris","sydney"))

val cities2 = sc.parallelize(List("perth","tokyo","canberra","sydney"))

val cities3 = cities1.union(cities2)

cities3.distinct.collect.foreach(println)

sydney
perth
canberra
tokyo
paris

```

##### ReduceByKey

ReduceByKey 使用指定的 reduce 函数将值与同一个键组合在一起。

```py
val pairRDD = sc.parallelize(List(("a", 1), ("b",2), ("c",3), ("a", 30), ("b",25), ("a",20)))
val sumRDD = pairRDD.reduceByKey((x,y) =>x+y)
sumRDD.collect
res15: Array[(String, Int)] = Array((b,27), (a,51), (c,3))

```

##### 键

Keys 返回只包含键的 RDD。

```py
val rdd = sc.parallelize(List(("a", "Larry"), ("b", "Curly"), ("c", "Moe")))

val keys = rdd.keys

keys.collect.foreach(println)

a
b
c

```

##### 价值观念

值返回仅包含值的 RDD。

```py
val rdd = sc.parallelize(List(("a", "Larry"), ("b", "Curly"), ("c", "Moe")))

val value = rdd.values

value.collect.foreach(println)

Larry
Curly
Moe

```

##### 内部连接

内部连接基于连接谓词返回两个 RDD 中所有元素的 RDD。

```py
val data = Array((100,"Jim Hernandez"), (101,"Shane King"))
val employees = sc.parallelize(data)

val data2 = Array((100,"Glendale"), (101,"Burbank"))
val cities = sc.parallelize(data2)

val data3 = Array((100,"CA"), (101,"CA"), (102,"NY"))
val states = sc.parallelize(data3)

val record = employees.join(cities).join(states)

record.collect.foreach(println)

(100,((Jim Hernandez,Glendale),CA))
(101,((Shane King,Burbank),CA))

```

##### RightOuterJoin 和 LeftOuterJoin

RightOuterJoin 返回右 RDD 中元素的 RDD，即使左 RDD 中没有匹配的行。LeftOuterJoin 等效于列顺序不同的 RightOuterJoin。

```py
val record = employees.join(cities).rightOuterJoin(states)

record.collect.foreach(println)

(100,(Some((Jim Hernandez,Glendale)),CA))
(102,(None,NY))
(101,(Some((Shane King,Burbank)),CA))

```

##### 联盟

Union 返回包含两个或更多 RDD 组合的 RDD。

```py
val  data = Array((103,"Mark Choi","Torrance","CA"), (104,"Janet Reyes","RollingHills","CA"))
val employees = sc.parallelize(data)
val  data = Array((105,"Lester Cruz","VanNuys","CA"), (106,"John White","Inglewood","CA"))
val employees2 = sc.parallelize(data)
val rdd = sc.union([employees, employees2])
rdd.collect.foreach(println)
(103,MarkChoi,Torrance,CA)
(104,JanetReyes,RollingHills,CA)
(105,LesterCruz,VanNuys,CA)
(106,JohnWhite,Inglewood,CA)

```

##### 减去

Subtract 返回仅包含第一个 RDD 中的元素的 RDD。

```py
val data = Array((103,"Mark Choi","Torrance","CA"),  (104,"Janet Reyes","Rolling Hills","CA"),(105,"Lester Cruz","Van Nuys","CA"))

val rdd = sc.parallelize(data)

val data2 = Array((103,"Mark Choi","Torrance","CA"))
val rdd2 = sc.parallelize(data2)

val employees = rdd.subtract(rdd2)

employees.collect.foreach(println)

(105,LesterCruz,Van Nuys,CA)
(104,JanetReyes,Rolling Hills,CA)

```

##### 联合

联合减少了 RDD 中的分区数量。在大型 RDD 上执行过滤后，您可能需要使用合并。虽然过滤减少了新 RDD 消耗的数据量，但它继承了原始 RDD 的分区数量。如果新的 RDD 比原来的 RDD 小得多，它可能会有成百上千个小分区，这可能会导致性能问题。

当您想减少 Spark 在写入 HDFS 时生成的文件数量，防止可怕的“小文件”问题时，Coalesce 也很有用。每个分区作为单独的文件写入 HDFS。请注意，使用 coalesce 时，您可能会遇到性能问题，因为在写入 HDFS 时，您会有效地降低并行度。如果发生这种情况，请尝试增加分区的数量。在下面的例子中，我们只将一个拼花文件写入 HDFS。

```py
df.coalesce(1).write.mode("append").parquet("/user/hive/warehouse/Mytable")

```

##### 再分

重新分区可以减少或增加 RDD 中的分区数量。减少分区时通常会使用联合，因为它比重新分区更有效。增加分区数量有助于提高写入 HDFS 时的并行度。在下面的例子中，我们向 HDFS 写了六个拼花文件。

```py
df.repartition(6).write.mode("append").parquet("/user/hive/warehouse/Mytable")

```

Note

合并通常比重新分区快。重新分区将执行完全洗牌，创建新分区并在工作节点之间平均分配数据。通过使用现有分区，联合最大限度地减少了数据移动并避免了完全洗牌。

#### 行动

动作是向驱动程序返回值的 RDD 操作。我列出了一些最常见的动作。请参考在线 Spark 文档，了解完整的操作列表。

##### 收集

Collect 将整个数据集作为数组返回给驱动程序。

```py
val myCities = sc.parallelize(List("tokyo","new york","paris","san francisco"))
myCities.collect
res2: Array[String] = Array(tokyo, new york, paris, san francisco)

```

##### 数数

Count 返回数据集中元素的数量。

```py
val myCities = sc.parallelize(List("tokyo","new york","paris","san francisco"))
myCities.count
res3: Long = 4

```

##### 拿

Take 将数据集的前 *n* 个元素作为数组返回。

```py
val myCities = sc.parallelize(List("tokyo","new york","paris","san francisco"))
myCities.take(2)
res4: Array[String] = Array(tokyo, new york)

```

##### 为每一个

Foreach 对数据集的每个元素执行一个函数。

```py
val myCities = sc.parallelize(List("tokyo","new york","paris","san francisco"))

myCities.collect.foreach(println)

tokyo
newyork
paris
sanFrancisco

```

#### 懒惰评估

Spark 支持惰性求值，这对于大数据处理至关重要。Spark 中的所有转换都是延迟计算的。Spark 不会立即执行转换。您可以继续定义更多的转换。当您最终想要最终结果时，您执行一个动作，这将导致转换被执行。

#### 贮藏

默认情况下，每次运行操作时，都会重新执行每个转换。您可以使用 cache 或 persist 方法在内存中缓存 RDD，以避免多次重复执行转换。

#### 蓄电池

累加器是只被“添加”的变量。它们通常用于实现计数器。在示例中，我使用累加器将数组的元素相加:

```py
val accum = sc.longAccumulator("Accumulator 01")

sc.parallelize(Array(10, 20, 30, 40)).foreach(x =>accum.add(x))

accum.value
res2: Long = 100

```

#### 广播变量

广播变量是存储在每个节点内存中的只读变量。Spark 使用高速广播算法来减少复制广播变量的网络延迟。使用广播变量在每个节点上存储数据集的副本是一种更快的方法，而不是将数据存储在 HDFS 或 S3 这样的慢速存储引擎中。

```py
val broadcastVar = sc.broadcast(Array(10, 20, 30))

broadcastVar.value
res0: Array[Int] = Array(10, 20, 30)

```

## Spark SQL、数据集和数据框架 API

开发 Spark SQL 是为了使处理和分析结构化数据变得更加容易。数据集类似于 RDD，因为它支持强类型，但在后台数据集有一个更有效的引擎。从 Spark 2.0 开始，数据集 API 现在是主要的编程接口。DataFrame 只是一个带有命名列的数据集，类似于关系表。Spark SQL 和 DataFrames 一起为处理和分析结构化数据提供了强大的编程接口。这里有一个关于如何使用 DataFrames API 的简单例子。

```py
val jsonDF = spark.read.json("/jsondata/customers.json")

jsonDF.show
+---+------+--------------+-----+------+-----+
|age|  city|          name|state|userid|  zip|
+---+------+--------------+-----+------+-----+
| 35|Frisco| Jonathan West|   TX|   200|75034|
| 28|Dallas|Andrea Foreman|   TX|   201|75001|
| 69| Plano|  Kirsten Jung|   TX|   202|75025|
| 52| Allen|Jessica Nguyen|   TX|   203|75002|
+---+------+--------------+-----+------+-----+

jsonDF.select ("age","city").show

+---+------+
|age|  city|
+---+------+
| 35|Frisco|
| 28|Dallas|
| 69| Plano|
| 52| Allen|
+---+------+

jsonDF.filter($"userid" < 202).show()
+---+------+--------------+-----+------+-----+
|age|  city|          name|state|userid|  zip|
+---+------+--------------+-----+------+-----+
| 35|Frisco| Jonathan West|   TX|   200|75034|
| 28|Dallas|Andrea Foreman|   TX|   201|75001|
+---+------+--------------+-----+------+-----+

jsonDF.createOrReplaceTempView("jsonDF")

val df = spark.sql("SELECT userid, zip FROM jsonDF")

df.show
+------+-----+
|userid|  zip|
+------+-----+
|   200|75034|
|   201|75001|
|   202|75025|
|   203|75002|
+------+-----+

```

Note

Spark 2.0 中统一了数据帧和数据集 API。DataFrame 现在只是行数据集的类型别名，其中行是通用的非类型化对象。相比之下，Dataset 是强类型对象 Dataset[T]的集合。Scala 支持强类型和非类型 API，而在 Java 中，Dataset[T]是主要的抽象。DataFrames 是 R 和 Python 的主要编程接口，因为它缺乏对编译时类型安全的支持。

## Spark 数据源

读写不同的文件格式和数据源是最常见的数据处理任务之一。在我们的示例中，我们将同时使用 RDD 和 DataFrames API。

### 战斗支援车

Spark 为您提供了从 CSV 文件中读取数据的不同方法。您可以先将数据读入 RDD，然后将其转换为 DataFrame。

```py
val dataRDD = sc.textFile("/sparkdata/customerdata.csv")
val parsedRDD = dataRDD.map{_.split(",")}
case class CustomerData(customerid: Int, name: String, city: String, state: String, zip: String)
val dataDF = parsedRDD.map{ a =>CustomerData (a(0).toInt, a(1).toString, a(2).toString,a(3).toString,a(4).toString) }.toDF

```

从 Spark 2.0 开始，CSV 连接器已经内置。

```py
val dataDF = spark.read.format("csv")
             .option("header", "true")
             .load("/sparkdata/customerdata.csv")

```

### 可扩展置标语言

Databricks 有一个 Spark XML 包，可以轻松读取 XML 数据。

```py
cat users.xml

<userid>100</userid><name>Wendell Ryan</name><city>San Diego</city><state>CA</state><zip>92102</zip>
<userid>101</userid><name>Alicia Thompson</name><city>Berkeley</city><state>CA</state><zip>94705</zip>
<userid>102</userid><name>Felipe Drummond</name><city>Palo Alto</city><state>CA</state><zip>94301</zip>
<userid>103</userid><name>Teresa Levine</name><city>Walnut Creek</city><state>CA</state><zip>94507</zip>

hadoop fs -mkdir /xmldata
hadoop fs -put users.xml /xmldata

spark-shell --packages  com.databricks:spark-xml_2.10:0.4.1

```

使用 Spark XML 创建一个数据框架。在本例中，我们指定了行标记和 XML 文件所在的 HDFS 路径。

```py
import com.databricks.spark.xml._

val xmlDF = spark.read
            .option("rowTag", "user")
            .xml("/xmldata/users.xml");

xmlDF: org.apache.spark.sql.DataFrame = [city: string, name: string, state: string, userid: bigint, zip: bigint]

```

我们也来看看数据。

```py
xmlDF.show

+------------+---------------+-----+------+-----+
|        city|           name|state|userid|  zip|
+------------+---------------+-----+------+-----+
|   San Diego|   Wendell Ryan|   CA|   100|92102|
|    Berkeley|Alicia Thompson|   CA|   101|94705|
|   Palo Alto|Felipe Drummond|   CA|   102|94301|
|Walnut Creek|  Teresa Levine|   CA|   103|94507|
+------------+---------------+-----+------+-----+

```

### 数据

我们将创建一个 JSON 文件作为这个例子的样本数据。确保该文件位于 HDFS 名为/jsondata 的文件夹中。

```py
cat users.json

{"userid": 200, "name": "Jonathan West", "city":"Frisco", "state":"TX", "zip": "75034", "age":35}
{"userid": 201, "name": "Andrea Foreman", "city":"Dallas", "state":"TX", "zip": "75001", "age":28}
{"userid": 202, "name": "Kirsten Jung", "city":"Plano", "state":"TX", "zip": "75025", "age":69}
{"userid": 203, "name": "Jessica Nguyen", "city":"Allen", "state":"TX", "zip": "75002", "age":52}

```

从 JSON 文件创建一个数据帧。

```py
val jsonDF = spark.read.json("/jsondata/users.json")

jsonDF: org.apache.spark.sql.DataFrame = [age: bigint, city: string, name: string, state: string, userid: bigint, zip: string]

```

检查日期

```py
jsonDF.show

+---+------+--------------+-----+------+-----+
|age|  city|          name|state|userid|  zip|
+---+------+--------------+-----+------+-----+
| 35|Frisco| Jonathan West|   TX|   200|75034|
| 28|Dallas|Andrea Foreman|   TX|   201|75001|
| 69| Plano|  Kirsten Jung|   TX|   202|75025|
| 52| Allen|Jessica Nguyen|   TX|   203|75002|
+---+------+--------------+-----+------+-----+

```

### 关系数据库和 MPP 数据库

我们在这个例子中使用 MySQL，但也支持其他关系数据库和 MPP 引擎，如 Oracle、Snowflake、Redshift、Impala、Presto 和 Azure DW。通常，只要关系数据库有 JDBC 驱动程序，就应该可以从 Spark 访问它。性能取决于您的 JDBC 驱动程序对批处理操作的支持。请查看您的 JDBC 驱动程序文档以了解更多详细信息。

```py
mysql -u root -pmypassword

create databases salesdb;

use salesdb;

create table customers (
customerid INT,
name VARCHAR(100),
city VARCHAR(100),
state CHAR(3),
zip  CHAR(5));

spark-shell --driver-class-path mysql-connector-java-5.1.40-bin.jar

```

启动火花壳。

将 CSV 文件读入 RDD，并将其转换为数据帧。

```py
val dataRDD = sc.textFile("/home/hadoop/test.csv")
val parsedRDD = dataRDD.map{_.split(",")}

case class CustomerData(customerid: Int, name: String, city: String, state: String, zip: String)

val dataDF = parsedRDD.map{ a =>CustomerData (a(0).toInt, a(1).toString, a(2).toString,a(3).toString,a(4).toString) }.toDF

```

将数据框注册为临时表，以便我们可以对其运行 SQL 查询。

```py
dataDF.createOrReplaceTempView("dataDF")

```

让我们设置连接属性。

```py
val jdbcUsername = "myuser"
val jdbcPassword = "mypass"
val jdbcHostname = "10.0.1.112"
val jdbcPort = 3306
val jdbcDatabase ="salesdb"
val jdbcrewriteBatchedStatements = "true"
val jdbcUrl = s"jdbc:mysql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}?user=${jdbcUsername}&password=${jdbcPassword}&rewriteBatchedStatements=${jdbcrewriteBatchedStatements}"

val connectionProperties = new java.util.Properties()

```

这将允许我们指定正确的保存模式`–`追加、覆盖等等。

```py
import org.apache.spark.sql.SaveMode

```

将 SELECT 语句返回的数据插入到 MySQL salesdb 数据库中存储的 customer 表中。

```py
spark.sql("select * from dataDF")
          .write
          .mode(SaveMode.Append)
          .jdbc(jdbcUrl, "customers", connectionProperties)

```

让我们用 JDBC 读一个表格。让我们用一些测试数据填充 MySQL 中的 users 表。确保 salesdb 数据库中存在 users 表。

```py
mysql -u root -pmypassword

use salesdb;

describe users;
+--------+--------------+------+-----+---------+-------+
| Field  | Type         | Null | Key | Default | Extra |
+--------+--------------+------+-----+---------+-------+
| userid | bigint(20)   | YES  |     | NULL    |       |
| name   | varchar(100) | YES  |     | NULL    |       |
| city   | varchar(100) | YES  |     | NULL    |       |
| state  | char(3)      | YES  |     | NULL    |       |
| zip    | char(5)      | YES  |     | NULL    |       |
| age    | tinyint(4)   | YES  |     | NULL    |       |
+--------+--------------+------+-----+---------+-------+

select * from users;
Empty set (0.00 sec)

insert into users values (300,'Fred Stevens','Torrance','CA',90503,23);

insert into users values (301,'Nancy Gibbs','Valencia','CA',91354,49);

insert into users values (302,'Randy Park','Manhattan Beach','CA',90267,21);

insert into users values (303,'Victoria Loma','Rolling Hills','CA',90274,75);

select * from users;
+--------+---------------+-----------------+-------+-------+------+
| userid | name          | city            | state | zip   | age  |
+--------+---------------+-----------------+-------+-------+------+
|    300 | Fred Stevens  | Torrance        | CA    | 90503 |   23 |
|    301 | Nancy Gibbs   | Valencia        | CA    | 91354 |   49 |
|    302 | Randy Park    | Manhattan Beach | CA    | 90267 |   21 |
|    303 | Victoria Loma | Rolling Hills   | CA    | 90274 |   75 |
+--------+---------------+-----------------+-------+-------+------+

spark-shell --driver-class-path mysql-connector-java-5.1.40-bin.jar --jars mysql-connector-java-5.1.40-bin.jar

```

让我们设置 jdbcurl 和连接属性。

```py
val jdbcURL = s"jdbc:mysql://10.0.1.101:3306/salesdb?user=myuser&password=mypass"

val connectionProperties = new java.util.Properties()

```

我们可以从整个表中创建一个数据帧。

```py
val df = spark.read.jdbc(jdbcURL, "users", connectionProperties)

df.show

+------+-------------+---------------+-----+-----+---+
|userid|         name|           city|state|  zip|age|
+------+-------------+---------------+-----+-----+---+
|   300| Fred Stevens|       Torrance|   CA|90503| 23|
|   301|  Nancy Gibbs|       Valencia|   CA|91354| 49|
|   302|   Randy Park|Manhattan Beach|   CA|90267| 21|
|   303|Victoria Loma|  Rolling Hills|   CA|90274| 75|
+------+-------------+---------------+-----+-----+---+

```

### 镶木地板

在拼花地板上读写很简单。

```py
val df = spark.read.load("/sparkdata/employees.parquet")

df.select("id","firstname","lastname","salary")
          .write
          .format("parquet")
          .save("/sparkdata/myData.parquet")

You can run SELECT statements on Parquet files directly.

val df = spark.sql("SELECT * FROM parquet.`/sparkdata/myData.parquet`")

```

### 巴什

从 Spark 访问 HBase 有多种方式。例如，可以使用 SaveAsHadoopDataset 将数据写入 HBase。启动 HBase shell。

创建一个 HBase 表并用测试数据填充它。

```py
hbase shell

create 'users', 'cf1'

```

启动火花壳。

```py
spark-shell

val hconf = HBaseConfiguration.create()
val jobConf = new JobConf(hconf, this.getClass)
jobConf.setOutputFormat(classOf[TableOutputFormat])
jobConf.set(TableOutputFormat.OUTPUT_TABLE,"users")

val num = sc.parallelize(List(1,2,3,4,5,6))

val theRDD = num.filter.map(x=>{

      val rowkey = "row" + x

val put = new Put(Bytes.toBytes(rowkey))

      put.add(Bytes.toBytes("cf1"), Bytes.toBytes("fname"), Bytes.toBytes("my fname" + x))

    (newImmutableBytesWritable, put)
})
theRDD.saveAsHadoopDataset(jobConf)

```

您还可以使用 Spark 中的 HBase 客户端 API 来读写 HBase 中的数据。如前所述，Scala 可以访问所有 Java 库。

启动 HBase shell。创建另一个 HBase 表，并用测试数据填充它。

```py
hbase shell

create 'employees', 'cf1'

put 'employees','400','cf1:name', 'Patrick Montalban'
put 'employees','400','cf1:city', 'Los Angeles'
put 'employees','400','cf1:state', 'CA'
put 'employees','400','cf1:zip', '90010'
put 'employees','400','cf1:age', '71'
put 'employees','401','cf1:name', 'Jillian Collins'
put 'employees','401','cf1:city', 'Santa Monica'
put 'employees','401','cf1:state', 'CA'
put 'employees','401','cf1:zip', '90402'
put 'employees','401','cf1:age', '45'

put 'employees','402','cf1:name', 'Robert Sarkisian'
put 'employees','402','cf1:city', 'Glendale'
put 'employees','402','cf1:state', 'CA'
put 'employees','402','cf1:zip', '91204'
put 'employees','402','cf1:age', '29'

put 'employees','403','cf1:name', 'Warren Porcaro'
put 'employees','403','cf1:city', 'Burbank'
put 'employees','403','cf1:state', 'CA'
put 'employees','403','cf1:zip', '91523'
put 'employees','403','cf1:age', '62'

```

让我们验证数据是否成功地插入到我们的 HBase 表中。

```py
scan 'employees'

ROW       COLUMN+CELL
 400      column=cf1:age, timestamp=1493105325812, value=71
 400      column=cf1:city, timestamp=1493105325691, value=Los Angeles
 400      column=cf1:name, timestamp=1493105325644, value=Patrick Montalban
 400      column=cf1:state, timestamp=1493105325738, value=CA
 400      column=cf1:zip, timestamp=1493105325789, value=90010
 401      column=cf1:age, timestamp=1493105334417, value=45
 401      column=cf1:city, timestamp=1493105333126, value=Santa Monica
 401      column=cf1:name, timestamp=1493105333050, value=Jillian Collins
 401      column=cf1:state, timestamp=1493105333145, value=CA
 401      column=cf1:zip, timestamp=1493105333165, value=90402
 402      column=cf1:age, timestamp=1493105346254, value=29
 402      column=cf1:city, timestamp=1493105345053, value=Glendale
 402      column=cf1:name, timestamp=1493105344979, value=Robert Sarkisian
 402      column=cf1:state, timestamp=1493105345074, value=CA
 402      column=cf1:zip, timestamp=1493105345093, value=91204
 403      column=cf1:age, timestamp=1493105353650, value=62
 403      column=cf1:city, timestamp=1493105352467, value=Burbank
 403      column=cf1:name, timestamp=1493105352445, value=Warren Porcaro
 403      column=cf1:state, timestamp=1493105352513, value=CA
 403      column=cf1:zip, timestamp=1493105352549, value=91523

```

启动火花壳。

```py
spark-shell

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.HColumnDescriptor
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.client.Result;
import org.apache.hadoop.hbase.util.Bytes;
import java.io.IOException;

val configuration = HBaseConfiguration.create()

```

指定 HBase 表和行键。

```py
val table = new HTable(configuration, "employees");
val g = new Get(Bytes.toBytes("401"))
val result = table.get(g);

```

从表中提取值。

```py
val val2 = result.getValue(Bytes.toBytes("cf1"),Bytes.toBytes("name"));
val val3 = result.getValue(Bytes.toBytes("cf1"),Bytes.toBytes("city"));
val val4 = result.getValue(Bytes.toBytes("cf1"),Bytes.toBytes("state"));
val val5 = result.getValue(Bytes.toBytes("cf1"),Bytes.toBytes("zip"));
val val6 = result.getValue(Bytes.toBytes("cf1"),Bytes.toBytes("age"));

```

将这些值转换为适当的数据类型。

```py
val id = Bytes.toString(result.getRow())
val name = Bytes.toString(val2);
val city = Bytes.toString(val3);
val state = Bytes.toString(val4);
val zip = Bytes.toString(val5);
val age = Bytes.toShort(val6);

```

打印数值。

```py
println(" employee id: " + id + " name: " + name + " city: " + city + " state: " + state + " zip: " + zip + " age: " + age);

employee id: 401 name: Jillian Collins city: Santa Monica state: CA zip: 90402 age: 13365

```

让我们使用 HBase API 写入 HBase。

```py
val configuration = HBaseConfiguration.create()
val table = new HTable(configuration, "employees");

```

指定新的行键。

```py
val p = new Put(new String("404").getBytes());

```

用新值填充单元格。

```py
p.add("cf1".getBytes(), "name".getBytes(), new String("Denise Shulman").getBytes());
p.add("cf1".getBytes(), "city".getBytes(), new String("La Jolla").getBytes());
p.add("cf1".getBytes(), "state".getBytes(), new String("CA").getBytes());
p.add("cf1".getBytes(), "zip".getBytes(), new String("92093").getBytes());
p.add("cf1".getBytes(), "age".getBytes(), new String("56").getBytes());

```

写入 HBase 表。

```py
table.put(p);
table.close();

```

确认值已成功插入 HBase 表中。

启动 HBase shell。

```py
hbase shell

scan 'employees'

ROW       COLUMN+CELL
 400      column=cf1:age, timestamp=1493105325812, value=71
 400      column=cf1:city, timestamp=1493105325691, value=Los Angeles
 400      column=cf1:name, timestamp=1493105325644, value=Patrick Montalban
 400      column=cf1:state, timestamp=1493105325738, value=CA
 400      column=cf1:zip, timestamp=1493105325789, value=90010
 401      column=cf1:age, timestamp=1493105334417, value=45
 401      column=cf1:city, timestamp=1493105333126, value=Santa Monica
 401      column=cf1:name, timestamp=1493105333050, value=Jillian Collins
 401      column=cf1:state, timestamp=1493105333145, value=CA
 401      column=cf1:zip, timestamp=1493105333165, value=90402
 402      column=cf1:age, timestamp=1493105346254, value=29
 402      column=cf1:city, timestamp=1493105345053, value=Glendale
 402      column=cf1:name, timestamp=1493105344979, value=Robert Sarkisian

 402      column=cf1:state, timestamp=1493105345074, value=CA
 402      column=cf1:zip, timestamp=1493105345093, value=91204
 403      column=cf1:age, timestamp=1493105353650, value=62
 403      column=cf1:city, timestamp=1493105352467, value=Burbank
 403      column=cf1:name, timestamp=1493105352445, value=Warren Porcaro
 403      column=cf1:state, timestamp=1493105352513, value=CA
 403      column=cf1:zip, timestamp=1493105352549, value=91523
 404      column=cf1:age, timestamp=1493123890714, value=56
 404      column=cf1:city, timestamp=1493123890714, value=La Jolla
 404      column=cf1:name, timestamp=1493123890714, value=Denise Shulman
 404      column=cf1:state, timestamp=1493123890714, value=CA
 404      column=cf1:zip, timestamp=1493123890714, value=92093

```

虽然通常速度较慢，但也可以通过 SQL 查询引擎(如 Impala 或 Presto)访问 HBase。

### 亚马逊 S3

亚马逊 S3 是一个流行的对象存储，经常被用作临时集群的数据存储。它还是备份和冷数据的经济高效的存储方式。从 S3 读取数据就像从 HDFS 或任何其他文件系统读取数据一样。

阅读来自亚马逊 S3 的 CSV 文件。请确保您已经配置了 S3 凭据。

```py
val myCSV = sc.textFile("s3a://mydata/customers.csv")

```

将 CSV 数据映射到 RDD。

```py
import org.apache.spark.sql.Row

val myRDD = myCSV.map(_.split(',')).map(e ⇒ Row(r(0).trim.toInt, r(1), r(2).trim.toInt, r(3)))

```

创建一个模式。

```py
import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};

val mySchema = StructType(Array(
StructField("customerid",IntegerType,false),
StructField("customername",StringType,false),
StructField("age",IntegerType,false),
StructField("city",StringType,false)))

val myDF = spark.createDataFrame(myRDD, mySchema)

```

### 使用

您可以使用 SolrJ 从 Spark 与 Solr 进行交互。 [<sup>viii</sup>](#Sec89)

```py
import java.net.MalformedURLException;
import org.apache.solr.client.solrj.SolrServerException;
import org.apache.solr.client.solrj.impl.HttpSolrServer;
import org.apache.solr.client.solrj.SolrQuery;
import org.apache.solr.client.solrj.response.QueryResponse;
import org.apache.solr.common.SolrDocumentList;

val solr = new HttpSolrServer("http://master02:8983/solr/mycollection");

val query = new SolrQuery();

query.setQuery("*:*");
query.addFilterQuery("userid:3");
query.setFields("userid","name","age","city");
query.setStart(0);
query.set("defType", "edismax");

val response = solr.query(query);
val results = response.getResults();

println(results);

```

从 Spark 访问 Solr 集合的一个更简单的方法是通过 spark-solr 包。Lucidworks 启动了 spark-solr 项目来提供 Spark-Solr 集成。与 solrJ 相比，使用 spark-solr 要简单和强大得多，它允许你从 Solr 集合中创建数据帧。

首先从 spark-shell 导入 JAR 文件。

```py
spark-shell --jars spark-solr-3.0.1-shaded.jar

```

指定集合和连接信息。

```py
val options = Map( "collection" -> "mycollection","zkhost" -> "{ master02:8983/solr}")

```

创建一个数据框架。

```py
val solrDF = spark.read.format("solr")
      .options(options)
      .load

```

### 微软优越试算表

虽然我通常不推荐从 Spark 访问 Excel 电子表格，但是某些用例需要这种能力。一家名为 Crealytics 的公司开发了一个用于与 Excel 交互的 Spark 插件。该库需要 Spark 2.x。可以使用- packages 命令行选项添加该包。

```py
spark-shell --packages com.crealytics:spark-excel_2.11:0.9.12

```

从 Excel 工作表创建数据框架。

```py
val ExcelDF = spark.read
    .format("com.crealytics.spark.excel")
    .option("sheetName", "sheet1")
    .option("useHeader", "true")
    .option("inferSchema", "true")
    .option("treatEmptyValuesAsNulls", "true")
    .load("budget.xlsx")

```

将数据帧写入 Excel 工作表。

```py
ExcelDF2.write
  .format("com.crealytics.spark.excel")
  .option("sheetName", "sheet1")
  .option("useHeader", "true")
  .mode("overwrite")
  .save("budget2.xlsx")

```

你可以在他们的 GitHub 页面上找到更多的细节:github.com/crealytics.

### 安全 FTP

从 SFTP 服务器下载文件和向其写入数据帧也是一个流行的请求。SpringML 提供了一个 Spark SFTP 连接器库。该库需要 Spark 2.x 并利用 jsch，这是 SSH2 的一个 Java 实现。对 SFTP 服务器的读写将作为单个进程执行。

```py
spark-shell --packages com.springml:spark-sftp_2.11:1.1.

```

从 SFTP 服务器中的文件创建一个数据帧。

```py
val sftpDF = spark.read.
            format("com.springml.spark.sftp").
            option("host", "sftpserver.com").
            option("username", "myusername").
            option("password", "mypassword").
            option("inferSchema", "true").
            option("fileType", "csv").
            option("delimiter", ",").
            load("/myftp/myfile.csv")

```

将数据帧作为 CSV 文件写入 FTP 服务器。

```py
sftpDF2.write.
      format("com.springml.spark.sftp").
      option("host", "sftpserver.com").
      option("username", "myusername").
      option("password", "mypassword").
      option("fileType", "csv").
      option("delimiter", ",").
      save("/myftp/myfile.csv")

```

你可以在他们的 GitHub 页面上找到更多的细节:github.com/springml/spark-sftp.

## Spark MLlib 简介

机器学习是 Spark 的主要应用之一。Spark MLlib 包括用于回归、分类、聚类、协作过滤和频繁模式挖掘的流行机器学习算法。它还为构建管线、模型选择和调整以及特征选择、提取和转换提供了广泛的功能。

### Spark MLlib 算法

Spark MLlib 包括大量用于各种任务的机器学习算法。我们将在接下来的章节中介绍其中的大部分。

**分类**

*   逻辑回归(二项式和多项式)

*   决策图表

*   随机森林

*   梯度增强树

*   多层感知器

*   线性支持向量机

*   奈伊夫拜厄斯

*   一对一休息

**回归**

*   线性回归

*   决策图表

*   随机森林

*   梯度增强树

*   生存回归

*   保序回归

**聚类**

*   k 均值

*   平分 K-均值

*   高斯混合模型

*   潜在狄利克雷分配

**协同过滤**

*   交替最小二乘法

**频繁模式挖掘**

*   FP-增长

*   前缀 Span

## ML 管道

Spark MLlib 的早期版本只包含一个基于 RDD 的 API。基于数据框架的 API 现在是 Spark 的主要 API。一旦基于数据帧的 API 达到特性对等，基于 RDD 的 API 在 Spark 2.3 中将被弃用。 [<sup>x</sup>](#Sec89) 基于 RDD 的 API 将在 Spark 3.0 中被移除。基于 DataFrames 的 API 通过提供更高级别的抽象来表示类似于关系数据库表的表格数据，使转换功能变得容易，这使它成为实现管道的自然选择。

Spark MLlib API 引入了几个创建机器学习管道的概念。图 [2-3](#Fig3) 显示了一个用于处理文本数据的简单 Spark MLlib 管道。记号赋予器将文本分解成一个单词包，将单词附加到输出数据帧上。词频`–`逆文档频率(TF `–` IDF)将数据帧作为输入，将单词包转换为特征向量，并将它们添加到第三个数据帧中。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig3_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig3_HTML.png)

图 2-3

一个简单的 Spark MLlib 流水线

### 管道

流水线是创建机器学习工作流的一系列相连的阶段。一个阶段可以是一个转换器或估计器。

### 变压器

转换器将一个数据帧作为输入，并输出一个新的数据帧，其中附加了附加列。新数据帧包括来自输入数据帧的列和附加列。

### 估计量

估计器是一种机器学习算法，可以根据训练数据拟合模型。估计器接受训练数据并产生机器学习模型。

### ParamGridBuilder

ParamGridBuilder 用于构建参数网格。CrossValidator 执行网格搜索，并用参数网格中用户指定的超参数组合来训练模型。

### 交叉验证器

CrossValidator 交叉评估拟合的机器学习模型，并通过尝试用用户指定的超参数组合拟合底层估计器来输出最佳模型。使用 CrossValidator 或 TrainValidationSplit 估计器进行模型选择。

### 求值程序

评估者计算你的机器学习模型的性能。它输出精度和召回率等指标来衡量拟合模型的表现。赋值器的示例包括分别用于二进制和多类分类任务的 BinaryClassificationEvaluator 和 multiclasclassificationevaluator，以及用于回归任务的 RegressionEvaluator。

## 特征提取、变换和选择

大多数情况下，在使用原始数据拟合模型之前，需要进行额外的预处理。例如，基于距离的算法要求特征标准化。当分类数据被一键编码时，一些算法执行得更好。文本数据通常需要标记化和特征矢量化。对于非常大的数据集，可能需要降维。Spark MLlib 包含了一个针对这些任务类型的转换器和估算器的大集合。我将讨论 Spark MLlib 中一些最常用的变压器和估算器。

### StringIndexer

大多数机器学习算法不能直接处理字符串，需要数据为数字格式。StringIndexer 是一个将标签的字符串列转换为索引的估计器。它支持四种不同的方法来生成索引:alphabetDesc、alphabetAsc、frequencyDesc 和 frequencyAsc。默认值设置为 frequencyDesc，最频繁的标签设置为 0，结果按标签频率降序排序。

```py
import org.apache.spark.ml.feature.StringIndexer

val df = spark.createDataFrame(
  Seq((0, "car"), (1, "car"), (2, "truck"), (3, "van"), (4, "van"), (5, "van"))
).toDF("id", "class")

df.show
+---+-----+
| id|class|
+---+-----+
|  0|  car|
|  1|  car|
|  2|truck|
|  3|  van|
|  4|  van|
|  5|  van|
+---+-----+
val model = new StringIndexer()
            .setInputCol("class")
            .setOutputCol("classIndex")

val indexer = model.fit(df)

val indexed = indexer.transform(df)

indexed.show()

+---+-----+----------+
| id|class|classIndex|
+---+-----+----------+
|  0|  car|       1.0|
|  1|  car|       1.0|
|  2|truck|       2.0|
|  3|  van|       0.0|
|  4|  van|       0.0|
|  5|  van|       0.0|
+---+-----+----------+

```

### Tokenizer

当分析文本数据时，通常有必要将句子分成单独的术语或单词。记号赋予器正是这样做的。您可以使用 RegexTokenizer 的正则表达式执行更高级的标记化。标记化通常是机器学习 NLP 流水线的第一步。我将在第四章更详细地讨论自然语言处理(NLP)。

```py
import org.apache.spark.ml.feature.Tokenizer

val df = spark.createDataFrame(Seq(
  (0, "Mark gave a speech last night in Laguna Beach"),
  (1, "Oranges are full of nutrients and low in calories"),
  (2, "Eddie Van Halen is amazing")
)).toDF("id", "sentence")

 df.show(false)

+---+-------------------------------------------------+
|id |sentence                                         |
+---+-------------------------------------------------+
|0  |Mark gave a speech last night in Laguna Beach    |
|1  |Oranges are full of nutrients and low in calories|
|2  |Eddie Van Halen is amazing                       |
+---+-------------------------------------------------+

val tokenizer = new Tokenizer().setInputCol("sentence").setOutputCol("words")

val tokenized = tokenizer.transform(df)

tokenized.show(false)

+---+-------------------------------------------------+
|id |sentence                                         |
+---+-------------------------------------------------+
|0  |Mark gave a speech last night in Laguna Beach    |
|1  |Oranges are full of nutrients and low in calories|
|2  |Eddie Van Halen is amazing                       |
+---+-------------------------------------------------+

+-----------------------------------------------------------+
|words                                                      |
+-----------------------------------------------------------+
|[mark, gave, a, speech, last, night, in, laguna, beach]    |
|[oranges, are, full, of, nutrients, and, low, in, calories]|
|[eddie, van, halen, is, amazing]                           |
+-----------------------------------------------------------+

```

### 向量汇编器

Spark MLlib 算法要求将要素存储在单个向量列中。通常，训练数据会以表格格式出现，数据存储在单独的列中。VectorAssembler 是一个转换器，它将一组列合并成一个向量列。

```py
import org.apache.spark.ml.feature.VectorAssembler

val df = spark.createDataFrame(
  Seq((0, 50000, 7, 1))
).toDF("id", "income", "employment_length", "marital_status")

val assembler = new VectorAssembler()
.setInputCols(Array("income", "employment_length", "marital_status"))
.setOutputCol("features")

val df2 = assembler.transform(df)

df2.show(false)

+---+------+-----------------+--------------+-----------------+
|id |income|employment_length|marital_status|features         |
+---+------+-----------------+--------------+-----------------+
|0  |50000 |7                |1             |[50000.0,7.0,1.0]|
+---+------+-----------------+--------------+-----------------+

```

### 标准鞋匠

正如在第 [1](1.html) 章中所讨论的，一些机器学习算法需要将特征规范化才能正常工作。StandardScaler 是一种将要素归一化为单位标准差和/或零均值的估计器。它接受两个参数:*withst 和 withMean。用*将特征缩放到单位标准偏差。默认情况下，该参数设置为 true。将【带平均值的 T4】设置为“真”,则在缩放之前，数据以平均值为中心。默认情况下，该参数设置为 false。

```py
import org.apache.spark.ml.feature.StandardScaler
import org.apache.spark.ml.feature.VectorAssembler

val df = spark.createDataFrame(
  Seq((0, 186, 200, 56),(1, 170, 198, 42))
).toDF("id", "height", "weight", "age")

val assembler = new VectorAssembler()
.setInputCols(Array("height", "weight", "age"))
.setOutputCol("features")

val df2 = assembler.transform(df)

df2.show(false)

+---+------+------+---+------------------+
|id |height|weight|age|features          |
+---+------+------+---+------------------+
|0  |186   |200   |56 |[186.0,200.0,56.0]|
|1  |170   |198   |42 |[170.0,198.0,42.0]|
+---+------+------+---+------------------+

val scaler = new StandardScaler()
  .setInputCol("features")
  .setOutputCol("scaledFeatures")
  .setWithStd(true)
  .setWithMean(false)

val model = scaler.fit(df2)

val scaledData = model.transform(df2)

scaledData.select("features","scaledFeatures").show(false)
+------------------+------------------------------------------------------+
|features          |scaledFeatures                                        |
+------------------+------------------------------------------------------+
|[186.0,200.0,56.0]|[16.440232662587228,141.42135623730948,5.656854249492]|
|[170.0,198.0,42.0]|[15.026019100214134,140.0071426749364,4.2426406871192]|
+------------------+------------------------------------------------------+

```

用于重定数据比例的其他转换器包括 Normalizer、MinMaxScaler 和 MaxAbsScaler。请查看 Apache Spark 在线文档以了解更多详细信息。

### 停用词去除器

常用于文本分析，从字符串序列中删除停用词。停用词，如 I、the 和 a，对文档的意义没有太大贡献。

```py
import org.apache.spark.ml.feature.StopWordsRemover

val remover = new StopWordsRemover().setInputCol("data").setOutputCol("output")

val dataSet = spark.createDataFrame(Seq(
  (0, Seq("She", "is", "a", "cute", "baby")),
  (1, Seq("Bob", "never", "went", "to", "Seattle"))
)).toDF("id", "data")

val df = remover.transform(dataSet)

df.show(false)

+---+-------------------------------+---------------------------+
|id |data                           |output                     |
+---+-------------------------------+---------------------------+
|0  |[She, is, a, cute, baby]       |[cute, baby]               |
|1  |[Bob, never, went, to, Seattle]|[Bob, never, went, Seattle]|
+---+-------------------------------+---------------------------+

```

### n-克

当执行文本分析时，将术语组合成 n 元语法(文档中术语的组合)有时是有利的。创建 n 元语法有助于从文档中提取更有意义的信息。例如,“San”和“Diego”这两个词本身没有什么意义，但是将它们组合成一个词“San Diego”可以提供更多的上下文信息。我们将在第 4 章[的后面使用 n-gram。](4.html)

```py
import org.apache.spark.ml.feature.NGram

val df = spark.createDataFrame(Seq(
  (0, Array("Los", "Angeles", "Lobos", "San", "Francisco")),
  (1, Array("Stand", "Book", "Case", "Phone", "Mobile", "Magazine")),
  (2, Array("Deep", "Learning", "Machine", "Algorithm", "Pizza"))
)).toDF("id", "words")

val ngram = new NGram().setN(2).setInputCol("words").setOutputCol("ngrams")

val df2 = ngram.transform(df)

df2.select("ngrams").show(false)

+---------------------------------------------------------------------+
|ngrams                                                               |
+---------------------------------------------------------------------+
|[Los Angeles, Angeles Lobos, Lobos San, San Francisco]               |
|[Stand Book, Book Case, Case Phone, Phone Mobile, Mobile Magazine]   |
|[Deep Learning, Learning Machine, Machine Algorithm, Algorithm Pizza]|
+---------------------------------------------------------------------+

```

### onehotencoderestomator 口腔癌

独热编码将分类特征转换成二进制向量，该向量最多具有单个一值，表示所有特征集合中特定特征值的存在。[](#Sec89)一键编码分类变量是逻辑回归、支持向量机等很多机器学习算法的要求。OneHotEncoderEstimator 可以转换多个列，为每个输入列生成一个独热编码的向量列。

```py
import org.apache.spark.ml.feature.StringIndexer

val df = spark.createDataFrame(
  Seq((0, "Male"), (1, "Male"), (2, "Female"), (3, "Female"), (4, "Female"), (5, "Male"))
).toDF("id", "gender")

df.show()

+---+------+
| id|gender|
+---+------+
|  0|  Male|
|  1|  Male|
|  2|Female|
|  3|Female|
|  4|Female|
|  5|  Male|
+---+------+

val indexer = new StringIndexer()
              .setInputCol("gender")
              .setOutputCol("genderIndex")

val indexed = indexer.fit(df).transform(df)

indexed.show()

+---+------+-----------+
| id|gender|genderIndex|
+---+------+-----------+
|  0|  Male|        1.0|
|  1|  Male|        1.0|
|  2|Female|        0.0|
|  3|Female|        0.0|
|  4|Female|        0.0|
|  5|  Male|        1.0|
+---+------+-----------+

import org.apache.spark.ml.feature.OneHotEncoderEstimator

val encoder = new OneHotEncoderEstimator()
              .setInputCols(Array("genderIndex"))
              .setOutputCols(Array("genderEnc"))

val encoded = encoder.fit(indexed).transform(indexed)

encoded.show()

+---+------+-----------+-------------+
| id|gender|genderIndex|    genderEnc|
+---+------+-----------+-------------+
|  0|  Male|        1.0|    (1,[],[])|
|  1|  Male|        1.0|    (1,[],[])|
|  2|Female|        0.0|(1,[0],[1.0])|
|  3|Female|        0.0|(1,[0],[1.0])|
|  4|Female|        0.0|(1,[0],[1.0])|
|  5|  Male|        1.0|    (1,[],[])|
+---+------+-----------+-------------+

```

### SQL 转换器

SQLTransformer 允许您使用 SQL 执行数据转换。虚拟表“__THIS__”对应于输入数据集。

```py
import org.apache.spark.ml.feature.SQLTransformer

val df = spark.createDataFrame(
  Seq((0, 5.2, 6.7), (2, 25.5, 8.9))).toDF("id", "col1", "col2")

val transformer = new SQLTransformer().setStatement("SELECT ABS(col1 - col2) as c1, MOD(col1, col2) as c2 FROM __THIS__")

val df2 = transformer.transform(df)

df2.show()

+----+-----------------+
|  c1|               c2|
+----+-----------------+
| 1.5|              5.2|
|16.6|7.699999999999999|
+----+-----------------+

```

### 术语频率–逆文档频率(TF–IDF)

TF `–` IDF 或词频`–`逆文档频率是文本分析中常用的一种特征矢量化方法。它经常被用来表示一个术语或单词对语料库中的文档的重要性。转换器 HashingTF 使用特征散列将术语转换成特征向量。估计器 IDF 对 HashingTF(或 CountVectorizer)生成的向量进行缩放。我将在第 4 章更详细地讨论 TF `–` IDF。

```py
import org.apache.spark.ml.feature.{HashingTF, IDF, Tokenizer}

val df = spark.createDataFrame(Seq(
  (0, "Kawhi Leonard is the league MVP"),
  (1, "Caravaggio pioneered the Baroque technique"),
  (2, "Using Apache Spark is cool")
)).toDF("label", "sentence")

df.show(false)

+-----+------------------------------------------+
|label|sentence                                  |
+-----+------------------------------------------+
|0    |Kawhi Leonard is the league MVP           |
|1    |Caravaggio pioneered the Baroque technique|
|2    |Using Apache Spark is cool                |
+-----+------------------------------------------+

val tokenizer = new Tokenizer()
                .setInputCol("sentence")
                .setOutputCol("words")

val df2 = tokenizer.transform(df)

df2.select("label","words").show(false)
+-----+------------------------------------------------+
|label|words                                           |
+-----+------------------------------------------------+
|0    |[kawhi, leonard, is, the, league, mvp]          |
|1    |[caravaggio, pioneered, the, baroque, technique]|
|2    |[using, apache, spark, is, cool]                |
+-----+------------------------------------------------+

val hashingTF = new HashingTF()
                .setInputCol("words")
                .setOutputCol("features")
                .setNumFeatures(20)

val df3 = hashingTF.transform(df2)

df3.select("label","features").show(false)

+-----+-----------------------------------------------+
|label|features                                       |
+-----+-----------------------------------------------+
|0    |(20,[1,4,6,10,11,18],[1.0,1.0,1.0,1.0,1.0,1.0])|
|1    |(20,[1,5,10,12],[1.0,1.0,2.0,1.0])             |
|2    |(20,[1,4,5,15],[1.0,1.0,1.0,2.0])              |
+-----+-----------------------------------------------+
val idf = new IDF()
          .setInputCol("features")
          .setOutputCol("scaledFeatures")

val idfModel = idf.fit(df3)

val df4 = idfModel.transform(df3)

df4.select("label", "scaledFeatures").show(3,50)
+-----+--------------------------------------------------+
|label|                                    scaledFeatures|
+-----+--------------------------------------------------+
|    0|(20,[1,4,6,10,11,18],[0.0,0.28768207245178085,0...|
|    1|(20,[1,5,10,12],[0.0,0.28768207245178085,0.5753...|
|    2|(20,[1,4,5,15],[0.0,0.28768207245178085,0.28768...|
+-----+--------------------------------------------------+

```

### 主成分分析

主成分分析(PCA)是一种降维技术，它将相关特征组合成一组较小的线性不相关特征，称为主成分。PCA 在图像识别和异常检测等多个领域都有应用。我将在第 [4](4.html) 章更详细地讨论 PCA。

```py
import org.apache.spark.ml.feature.PCA
import org.apache.spark.ml.linalg.Vectors

val data = Array(
  Vectors.dense(4.2, 5.4, 8.9, 6.7, 9.1),
  Vectors.dense(3.3, 8.2, 7.0, 9.0, 7.2),
  Vectors.dense(6.1, 1.4, 2.2, 4.3, 2.9)
)
val df = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")

val pca = new PCA()
          .setInputCol("features")
          .setOutputCol("pcaFeatures")
          .setK(2)
          .fit(df)

val result = pca.transform(df).select("pcaFeatures")

result.show(false)

+---------------------------------------+
|pcaFeatures                            |
+---------------------------------------+
|[13.62324332562565,3.1399510055159445] |
|[14.130156836243236,-1.432033103462711]|
|[3.4900743524527704,0.6866090886347056]|
+---------------------------------------+

```

### 卡方选择器

ChiSqSelector 使用卡方独立性检验进行特征选择。卡方检验是一种检验两个分类变量之间关系的方法。 *numTopFeatures* 是默认的选择方法。它返回一组基于卡方检验的特征，或最具预测影响的特征。其他选择方法包括*百分位数、fpr、fdr 和 fwe* 。

```py
import org.apache.spark.ml.feature.ChiSqSelector
import org.apache.spark.ml.linalg.Vectors

val data = Seq(
  (0, Vectors.dense(5.1, 2.9, 5.6, 4.8), 0.0),
  (1, Vectors.dense(7.3, 8.1, 45.2, 7.6), 1.0),
  (2, Vectors.dense(8.2, 12.6, 19.5, 9.21), 1.0)
)

val df = spark.createDataset(data).toDF("id", "features", "class")

val selector = new ChiSqSelector()
               .setNumTopFeatures(1)
               .setFeaturesCol("features")
               .setLabelCol("class")
               .setOutputCol("selectedFeatures")

val df2 = selector.fit(df).transform(df)

df2.show()

+---+--------------------+-----+----------------+
| id|            features|class|selectedFeatures|
+---+--------------------+-----+----------------+
|  0|   [5.1,2.9,5.6,4.8]|  0.0|           [5.1]|
|  1|  [7.3,8.1,45.2,7.6]|  1.0|           [7.3]|
|  2|[8.2,12.6,19.5,9.21]|  1.0|           [8.2]|
+---+--------------------+-----+----------------+

```

### 相互关系

相关性评估两个变量之间线性关系的强度。对于线性问题，您可以使用相关性来选择相关要素(要素类相关性)和识别冗余要素(要素内相关性)。Spark MLlib 支持皮尔逊和斯皮尔曼的相关性。在下面的示例中，correlation 计算输入向量的相关矩阵。

```py
import org.apache.spark.ml.linalg.{Matrix, Vectors}
import org.apache.spark.ml.stat.Correlation
import org.apache.spark.sql.Row

val data = Seq(
  Vectors.dense(5.1, 7.0, 9.0, 6.0),
  Vectors.dense(3.2, 1.1, 6.0, 9.0),
  Vectors.dense(3.5, 4.2, 9.1, 3.0),
  Vectors.dense(9.1, 2.6, 7.2, 1.8)
)

val df = data.map(Tuple1.apply).toDF("features")

+-----------------+
|         features|
+-----------------+
|[5.1,7.0,9.0,6.0]|
|[3.2,1.1,6.0,9.0]|
|[3.5,4.2,9.1,3.0]|
|[9.1,2.6,7.2,1.8]|
+-----------------+

val Row(c1: Matrix) = Correlation.corr(df, "features").head

c1: org.apache.spark.ml.linalg.Matrix =
1.0                  -0.01325851107237613  -0.08794286922175912 -0.6536434849076798
-0.01325851107237613  1.0                   0.8773748081826724  -0.1872850762579899
-0.08794286922175912  0.8773748081826724    1.0                 -0.46050932066780714
-0.6536434849076798  -0.1872850762579899   -0.46050932066780714  1.0

val Row(c2: Matrix) = Correlation.corr(df, "features", "spearman").head

c2: org.apache.spark.ml.linalg.Matrix =
1.0                   0.399999999999999    0.19999999999999898  -0.8000000000000014
0.399999999999999     1.0                  0.8000000000000035   -0.19999999999999743
0.19999999999999898   0.8000000000000035   1.0                  -0.39999999999999486
-0.8000000000000014  -0.19999999999999743  -0.39999999999999486  1.0

```

还可以计算 DataFrame 列中存储的值的相关性，如下所示。

```py
dataDF.show

+------------+-----------+------------+-----------+-----------+-----+
|sepal_length|sepal_width|petal_length|petal_width|      class|label|
+------------+-----------+------------+-----------+-----------+-----+
|         5.1|        3.5|         1.4|        0.2|Iris-setosa|  0.0|
|         4.9|        3.0|         1.4|        0.2|Iris-setosa|  0.0|
|         4.7|        3.2|         1.3|        0.2|Iris-setosa|  0.0|
|         4.6|        3.1|         1.5|        0.2|Iris-setosa|  0.0|
|         5.0|        3.6|         1.4|        0.2|Iris-setosa|  0.0|
|         5.4|        3.9|         1.7|        0.4|Iris-setosa|  0.0|
|         4.6|        3.4|         1.4|        0.3|Iris-setosa|  0.0|
|         5.0|        3.4|         1.5|        0.2|Iris-setosa|  0.0|
|         4.4|        2.9|         1.4|        0.2|Iris-setosa|  0.0|
|         4.9|        3.1|         1.5|        0.1|Iris-setosa|  0.0|
|         5.4|        3.7|         1.5|        0.2|Iris-setosa|  0.0|
|         4.8|        3.4|         1.6|        0.2|Iris-setosa|  0.0|
|         4.8|        3.0|         1.4|        0.1|Iris-setosa|  0.0|
|         4.3|        3.0|         1.1|        0.1|Iris-setosa|  0.0|
|         5.8|        4.0|         1.2|        0.2|Iris-setosa|  0.0|
|         5.7|        4.4|         1.5|        0.4|Iris-setosa|  0.0|
|         5.4|        3.9|         1.3|        0.4|Iris-setosa|  0.0|
|         5.1|        3.5|         1.4|        0.3|Iris-setosa|  0.0|
|         5.7|        3.8|         1.7|        0.3|Iris-setosa|  0.0|
|         5.1|        3.8|         1.5|        0.3|Iris-setosa|  0.0|
+------------+-----------+------------+-----------+-----------+-----+

dataDF.stat.corr("petal_length","label")
res48: Double = 0.9490425448523336

dataDF.stat.corr("petal_width","label")
res49: Double = 0.9564638238016178

dataDF.stat.corr("sepal_length","label")
res50: Double = 0.7825612318100821

dataDF.stat.corr("sepal_width","label")
res51: Double = -0.41944620026002677

```

## 评估指标

如第 [1](1.html) 章所述，精确度、召回率和准确度是评估模型性能的重要评估指标。然而，它们可能并不总是某些问题的最佳度量。

### 受试者工作特性下的面积(AUROC)

接受者操作特征下的面积(AUROC)是用于评估二元分类器的常见性能度量。受试者工作特性(ROC)是绘制真阳性率与假阳性率的图表。曲线下面积(AUC)是 ROC 曲线下的面积。AUC 可以解释为模型对随机正例的排序高于随机负例的概率。 [<sup>xii</sup>](#Sec89) 曲线下面积越大(AUROC 越接近 1.0)，模型表现越好。AUROC 为 0.5 的模型是无用的，因为它的预测准确性与随机猜测一样好。

```py
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val evaluator = new BinaryClassificationEvaluator()
                .setMetricName("areaUnderROC")
                .setRawPredictionCol("rawPrediction")
                .setLabelCol("label")

```

### F1 度量

F1 测量值或 F1 分数是精确度和召回率的调和平均值或加权平均值。这是评估多类分类器的一个常见性能指标。当存在不均匀的阶级分布时，这也是一个很好的衡量标准。F1 成绩最好的是 1，最差的是 0。一个好的 F1 测量意味着你有很低的假阴性和假阳性。F1 度量的公式为:*F1-Measure = 2∫(精度∫召回)/(精度+召回)*。

```py
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator

val evaluator = new MulticlassClassificationEvaluator()
                .setMetricName("f1")
                .setLabelCol("label")
                .setPredictionCol("prediction")

```

### 均方根误差(RMSE)

均方根误差(RMSE)是回归任务中最常见的指标。RMSE 就是均方误差(MSE)的平方根。MSE 表示回归线与一组数据点的接近程度，方法是将这些点到回归线的距离或“误差”取平方。[<sup>XIII</sup>](#Sec89)MSE 越小，拟合越好。但是，MSE 与原始数据的单位不匹配，因为该值是平方的。RMSE 与输出的单位相同。

```py
import org.apache.spark.ml.evaluation.RegressionEvaluator

val evaluator = new RegressionEvaluator()
                .setLabelCol("label")
                .setPredictionCol("prediction")
                .setMetricName("rmse")

```

我将在后续章节中介绍其他评估指标，如误差平方和(WSSSE)和轮廓系数。有关 Spark MLlib 支持的所有评估指标的完整列表，请参考 Spark 的在线文档。

## 模型持久性

Spark MLlib 允许您保存模型并在以后加载它们。如果您想要将您的模型与第三方应用程序集成，或者与团队的其他成员共享它们，这将特别有用。

保存单个随机森林模型

```py
rf = RandomForestClassifier(numBin=10,numTrees=30)
model = rf.fit(training)
model.save("modelpath")

```

加载单个随机森林模型

```py
val model2 = RandomForestClassificationModel.load("modelpath")

```

保存完整的管道

```py
val pipeline = new Pipeline().setStages(Array(labelIndexer,vectorAssembler, rf))
val cv = new CrossValidator().setEstimator(pipeline)
val model = cv.fit(training)
model.save("modelpath")

```

加载完整的管道

```py
val model2 = CrossValidatorModel.load("modelpath")

```

## Spark MLlib 示例

让我们来看一个例子。我们将使用来自 UCI 机器学习知识库的心脏病数据集 [<sup>xiv</sup>](#Sec89) 来预测心脏病的存在。这些数据是由罗伯特·德特拉诺医学博士和他的团队在弗吉尼亚医学中心、长滩和克利夫兰诊所基金会收集的。历史上，克利夫兰数据集一直是众多研究的主题，因此我们将使用该数据集。原始数据集有 76 个属性，但其中只有 14 个用于 ML 研究(表 [2-1](#Tab1) )。我们将进行二项式分类，确定患者是否患有心脏病(列表 [2-2](#PC95) )。

表 2-1

克利夫兰心脏病数据集属性信息

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

属性

 | 

描述

 |
| --- | --- |
| 年龄 | 年龄 |
| 性 | 性 |
| 丙酸纤维素 | 胸痛型 |
| treatbps | 静息血压 |
| 胆固醇 | 血清胆固醇(毫克/分升) |
| 前沿系统 | 空腹血糖> 120 毫克/分升 |
| 尊重 | 静息心电图结果 |
| 塔尔巴赫 | 达到最大心率 |
| 考试 | 运动诱发的心绞痛 |
| 旧峰 | 相对于静息运动诱发的 ST 段压低 |
| 倾斜 | 运动 ST 段峰值的斜率 |
| 大约 | 荧光镜染色的主要血管数量(0-3) |
| 塔尔 | 铊压力测试结果 |
| 数字 | 预测属性——心脏病的诊断 |

我们开始吧。下载文件，并将其复制到 HDFS。

```py
wget http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/cleveland.data

head -n 10 processed.cleveland.data

63.0,1.0,1.0,145.0,233.0,1.0,2.0,150.0,0.0,2.3,3.0,0.0,6.0,0
67.0,1.0,4.0,160.0,286.0,0.0,2.0,108.0,1.0,1.5,2.0,3.0,3.0,2
67.0,1.0,4.0,120.0,229.0,0.0,2.0,129.0,1.0,2.6,2.0,2.0,7.0,1
37.0,1.0,3.0,130.0,250.0,0.0,0.0,187.0,0.0,3.5,3.0,0.0,3.0,0
41.0,0.0,2.0,130.0,204.0,0.0,2.0,172.0,0.0,1.4,1.0,0.0,3.0,0
56.0,1.0,2.0,120.0,236.0,0.0,0.0,178.0,0.0,0.8,1.0,0.0,3.0,0
62.0,0.0,4.0,140.0,268.0,0.0,2.0,160.0,0.0,3.6,3.0,2.0,3.0,3
57.0,0.0,4.0,120.0,354.0,0.0,0.0,163.0,1.0,0.6,1.0,0.0,3.0,0
63.0,1.0,4.0,130.0,254.0,0.0,2.0,147.0,0.0,1.4,2.0,1.0,7.0,2
53.0,1.0,4.0,140.0,203.0,1.0,2.0,155.0,1.0,3.1,3.0,0.0,7.0,1

hadoop fs -put processed.cleveland.data /tmp/data

```

我们使用 spark-shell 来交互式地训练我们的模型。

```py
spark-shell

val dataDF = spark.read.format("csv")
             .option("header", "true")
             .option("inferSchema", "true")
             .load(d("/tmp/data/processed.cleveland.data")
             .toDF("id","age","sex","cp","trestbps","chol","fbs","restecg",
             "thalach","exang","oldpeak","slope","ca","thal","num")

dataDF.printSchema
root
 |-- id: string (nullable = false)
 |-- age: float (nullable = true)
 |-- sex: float (nullable = true)
 |-- cp: float (nullable = true)
 |-- trestbps: float (nullable = true)
 |-- chol: float (nullable = true)
 |-- fbs: float (nullable = true)
 |-- restecg: float (nullable = true)
 |-- thalach: float (nullable = true)
 |-- exang: float (nullable = true)
 |-- oldpeak: float (nullable = true)
 |-- slope: float (nullable = true)
 |-- ca: float (nullable = true)
 |-- thal: float (nullable = true)
 |-- num: float (nullable = true)

val myFeatures = Array("age", "sex", "cp", "trestbps", "chol", "fbs",
      "restecg", "thalach", "exang", "oldpeak", "slope",
      "ca", "thal", "num")

import org.apache.spark.ml.feature.VectorAssembler

val assembler = new VectorAssembler()
                .setInputCols(myFeatures)
                .setOutputCol("features")

val dataDF2 = assembler.transform(dataDF)

import org.apache.spark.ml.feature.StringIndexer

val labelIndexer = new StringIndexer()
                   .setInputCol("num")
                   .setOutputCol("label")

val dataDF3 = labelIndexer.fit(dataDF2).transform(dataDF2)

val dataDF4 = dataDF3.where(dataDF3("ca").isNotNull)
              .where(dataDF3("thal").isNotNull)
              .where(dataDF3("num").isNotNull)

val Array(trainingData, testData) = dataDF4.randomSplit(Array(0.8, 0.2), 101)

import org.apache.spark.ml.classification.RandomForestClassifier

val rf = new RandomForestClassifier()
         .setFeatureSubsetStrategy("auto")
         .setSeed(101)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator

val evaluator = new BinaryClassificationEvaluator().setLabelCol("label")

import org.apache.spark.ml.tuning.ParamGridBuilder

val pgrid = new ParamGridBuilder()
      .addGrid(rf.maxBins, Array(10, 20, 30))
      .addGrid(rf.maxDepth, Array(5, 10, 15))
      .addGrid(rf.numTrees, Array(20, 30, 40))
      .addGrid(rf.impurity, Array("gini", "entropy"))
      .build()

import org.apache.spark.ml.Pipeline

val pipeline = new Pipeline().setStages(Array(rf))

import org.apache.spark.ml.tuning.CrossValidator

val cv = new CrossValidator()
      .setEstimator(pipeline)
      .setEvaluator(evaluator)
      .setEstimatorParamMaps(pgrid)
      .setNumFolds(3)

Listing 2-2Performing Binary Classification Using Random Forest

```

我们现在可以拟合模型了。

```py
val model = cv.fit(trainingData)

```

对测试数据进行预测。

```py
val prediction = model.transform(testData)

```

我们来评价一下模型。

```py
import org.apache.spark.ml.param.ParamMap

val pm = ParamMap(evaluator.metricName -> "areaUnderROC")

val aucTestData = evaluator.evaluate(prediction, pm)

```

## 图形处理

Spark 包括一个名为 GraphX 的图形处理框架。有一个独立的包叫做 GraphFrames，它基于 DataFrames。GraphFrames 目前不是核心 Apache Spark 的一部分。在撰写本文时，GraphX 和 GraphFrames 仍在积极开发中。[<sup>XV</sup>T3【我盖 GraphX】第六章](#Sec89)[第六章](6.html)。

## 超越 Spark MLlib:第三方机器学习集成

由于无数开源贡献者以及微软和谷歌等公司，Spark 可以访问第三方框架和库的丰富生态系统。虽然我涵盖了核心 Spark MLlib 算法，但这本书专注于更强大的下一代算法和框架，如 XGBoost、LightGBM、Isolation Forest、Spark NLP 和分布式深度学习。我将在接下来的章节中介绍它们。

## 使用 Alluxio 优化 Spark 和 Spark MLlib

Alluxio，原名 Tachyon，是加州大学伯克利分校 AMPLab 的一个开源项目。Alluxio 是一个以内存为中心的分布式存储系统，最初是由李皓原在 2012 年作为一个研究项目开发的，当时他是 AMPLab 的一名博士生和 Apache Spark 创始人。 [<sup>xvi</sup>](#Sec89) 该项目是 Berkeley 数据分析栈(BDAS)的存储层。2015 年，李创立了 Alluxio，Inc .以实现 Alluxio 的商业化，并获得了安德森·霍洛维茨(Andre essen Horowitz)750 万美元的现金注入。如今，Alluxio 拥有来自英特尔、IBM、雅虎和 Red Hat 等全球 50 个组织的 200 多名贡献者。几家知名公司目前正在生产中使用 Alluxio，如百度、阿里巴巴、Rackspace 和巴克莱。[T5】XVIIT7】](#Sec89)

Alluxio 可用于优化 Spark 机器学习和深度学习工作负载，方法是将超快大数据存储到超大数据集。由 Alluxio 进行的深度学习基准测试显示，当从 Alluxio 而不是 S3 读取数据时，性能有显著提高。[<sup>XVIII</sup>T3】](#Sec89)

### 体系结构

Alluxio 是一个以内存为中心的分布式存储系统，旨在成为大数据事实上的存储统一层。它提供了一个虚拟化层，统一了对不同存储引擎(如本地文件系统、HDFS、S3 和 NFS)和计算框架(如 Spark、MapReduce、Hive 和 Presto)的访问。图 [2-4](#Fig4) 给你一个 Alluxio 架构的概述。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig4_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig4_HTML.png)

图 2-4

Alluxio 架构概述

Alluxio 是协调数据共享和指导数据访问的中间层，同时为计算框架和大数据应用程序提供高性能低延迟的内存速度。Alluxio 与 Spark 和 Hadoop 无缝集成，只需要少量的配置更改。通过利用 Alluxio 的统一命名空间功能，应用程序只需连接到 Alluxio 即可访问存储在任何受支持的存储引擎中的数据。Alluxio 有自己的原生 API 以及 Hadoop 兼容的文件系统接口。便利类使用户能够执行最初为 Hadoop 编写的代码，而无需任何代码更改。REST API 提供了对其他语言的访问。我们将在本章的后面探讨 API。

Alluxio 的统一命名空间特性不支持关系数据库和 MPP 引擎，如 Redshift 或 Snowflake，也不支持文档数据库，如 MongoDB。当然，支持向 Alluxio 和上面提到的存储引擎写入数据。开发人员可以使用 Spark 等计算框架从红移表创建数据帧，并以 Parquet 或 CSV 格式存储在 Alluxio 文件系统中，反之亦然(图 [2-5](#Fig5) )。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig5_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig5_HTML.png)

图 2-5

Alluxio 技术架构

## 为什么要用 Alluxio？

### 显著提高大数据处理性能和可扩展性

这些年来，内存变得越来越便宜，而其性能却变得越来越快。与此同时，硬盘驱动器的性能只是略有改善。毫无疑问，在内存中处理数据比在磁盘上处理数据快一个数量级。在几乎所有的编程范例中，我们都被建议在内存中缓存数据以提高性能。Apache Spark 优于 MapReduce 的一个主要优势是它能够缓存数据。Alluxio 将这一点提升到了一个新的水平，为大数据应用程序提供的不仅仅是一个缓存层，而是一个成熟的分布式高性能以内存为中心的存储系统。

百度正在运营世界上最大的 Alluxio 集群之一，1000 个工作节点处理超过 2PB 的数据。借助 Alluxio，百度在查询和处理时间方面的性能平均提高了 10 倍，最高可达 30 倍，显著提高了百度做出重要业务决策的能力。 [<sup>xix</sup>](#Sec89) 巴克莱发表文章描述了他们与 Alluxio 的经历。巴克莱数据科学家 Gianmario Spacagna 和高级分析主管 Harry Powell 能够使用 Alluxio 将他们的 Spark 工作从数小时调整到数秒。中国最大的旅游搜索引擎之一 Qunar.com 使用 Alluxio 后，性能提升了 15 到 300 倍。 [<sup>xxi</sup>](#Sec89)

### 多个框架和应用程序可以以内存速度共享数据

一个典型的大数据集群有多个会话运行不同的计算框架，如 Spark 和 MapReduce。对于 Spark，每个应用程序都有自己的执行器进程，执行器中的每个任务都运行在自己的 JVM 上，将 Spark 应用程序相互隔离。这意味着 Spark(和 MapReduce)应用程序无法共享数据，除了写入 HDFS 或 S3 等存储系统。如图 [2-6](#Fig6) 所示，Spark 作业和 MapReduce 作业使用存储在 HDFS 或 S3 的相同数据。在图 [2-7](#Fig7) 中，多个 Spark 作业使用相同的数据，每个作业在自己的堆空间中存储自己版本的数据。 [<sup>xxii</sup>](#Sec89) 不仅数据会重复，通过 HDFS 或 S3 共享数据也会很慢，尤其是当你共享大量数据时。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig7_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig7_HTML.png)

图 2-7

不同的工作通过 HDFS 或 S3 共享数据

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig6_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig6_HTML.png)

图 2-6

不同的框架通过 HDFS 或 S3 共享数据

通过使用 Alluxio 作为堆外存储(图 [2-8](#Fig8) )，多个框架和作业可以以内存速度共享数据，减少数据重复，提高吞吐量，减少延迟。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig8_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig8_HTML.png)

图 2-8

不同的作业和框架以内存速度共享数据

## 在应用程序终止或出现故障时提供高可用性和持久性

在 Spark 中，执行器进程和执行器内存驻留在同一个 JVM 中，所有缓存的数据都存储在 JVM 堆空间中(图 [2-9](#Fig9) )。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig9_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig9_HTML.png)

图 2-9

Spark 作业有自己的堆内存

当作业完成或由于某种原因 JVM 由于运行时异常而崩溃时，所有缓存在堆空间中的数据都将丢失，如图 [2-10](#Fig10) 和 [2-11](#Fig11) 所示。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig11_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig11_HTML.png)

图 2-11

Spark 作业崩溃或完成。堆空间丢失

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig10_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig10_HTML.png)

图 2-10

火花作业崩溃或完成

解决方案是使用 Alluxio 作为堆外存储(图 [2-12](#Fig12) )。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig12_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig12_HTML.png)

图 2-12

Spark 使用 Alluxio 作为堆外存储

在这种情况下，即使 Spark JVM 崩溃，数据在 Alluxio 中仍然可用(图 [2-13](#Fig13) 和 [2-14](#Fig14) )。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig14_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig14_HTML.png)

图 2-14

Spark 作业崩溃或完成。堆空间丢失。堆外内存仍然可用

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig13_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig13_HTML.png)

图 2-13

火花作业崩溃或完成

## 优化整体内存使用并最大限度地减少垃圾收集

通过使用 Alluxio，内存使用效率大大提高，因为数据在作业和框架之间共享，并且因为数据存储在堆外，所以垃圾收集也被最小化，从而进一步提高了作业和应用程序的性能(图 [2-15](#Fig15) )。

![img/488426_1_En_2_Chapter/488426_1_En_2_Fig15_HTML.png](img/488426_1_En_2_Chapter/488426_1_En_2_Fig15_HTML.png)

图 2-15

多个 Spark 和 MapReduce 作业可以访问存储在 Alluxio 中的相同数据

## 降低硬件要求

Alluxio 的大数据处理速度明显快于 HDFS 和 S3。IBM 的测试显示，在写入 io 方面，Alluxio 比 HDFS 快 110 倍。 [<sup>xxiii</sup>](#Sec89) 有了这样的性能，对额外硬件的需求就会减少，从而节省基础设施和许可成本。

## 阿帕奇 Spark 和 Alluxio

您在 Alluxio 中访问数据的方式类似于从 Spark 中访问存储在 HDFS 和 S3 的数据。

```py
val dataRDD = sc.textFile("alluxio://localhost:19998/test01.csv")

val parsedRDD = dataRDD.map{_.split(",")}

case class CustomerData(userid: Long, city: String, state: String, age: Short)

val dataDF = parsedRDD.map{ a =>CustomerData(a(0).toLong, a(1).toString, a(2).toString, a(3).toShort) }.toDF

dataDF.show()

+------+---------------+-----+---+
|userid|           city|state|age|
+------+---------------+-----+---+
|   300|       Torrance|   CA| 23|
|   302|Manhattan Beach|   CA| 21|
+------+---------------+-----+---+

```

## 摘要

本章向您简要介绍了 Spark 和 Spark MLlib，足以让您掌握执行常见数据处理和机器学习任务所需的技能。我的目标是让你尽快熟悉情况。为了更彻底的治疗，*比尔钱伯斯和马泰扎哈里亚(O'Reilly，2018)的《火花:权威指南*》提供了对火花的全面介绍。Irfan Elahi (Apress，2019)的 *Scala 编程用于大数据分析*，Jason Swartz (O'Reilly，2014)的*学习 Scala* ，以及 Martin Odersky、Lex Spoon 和 Bill Venners (Artima，2016)的*Scala 编程*都是对 Scala 的很好介绍。我还介绍了 Alluxio，这是一个内存分布式计算平台，可用于优化大规模机器学习和深度学习工作负载。

## 参考

1.  彼得·诺维格等人；《数据的不合理有效性》，googleuserconent.com，2009， [`https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf`](https://static.googleusercontent.com/media/research.google.com/en/pubs/archive/35179.pdf)

2.  火花；《星火总览》，spark.apache.org，2019， [`https://spark.apache.org/docs/2.2.0/`](https://spark.apache.org/docs/2.2.0/)

3.  阿帕奇软件基金会；“Apache 软件基金会宣布 Apache Spark 为顶级项目”，blogs.apache.org，2014， [`https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50`](https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50)

4.  火花；《星火新闻》，spark.apache.org，2019， [`https://spark.apache.org/news/`](https://spark.apache.org/news/)

5.  reddit“Matei Zaharia AMA，”reddit.com，2015 年，

6.  数据块；《阿帕奇星火》，Databricks.com，2019， [`https://databricks.com/spark/about`](https://databricks.com/spark/about)

7.  数据块；《如何在 Apache Spark 2.0 中使用 SparkSession》，Databricks.com，2016， [`https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html`](https://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html)

8.  Solr《利用索勒吉》，lucene.apache.org，2019， [`https://lucene.apache.org/solr/guide/6_6/using-solrj.html`](https://lucene.apache.org/solr/guide/6_6/using-solrj.html)

9.  Lucidworks《Lucidworks Spark/Solr 集成》，github.com，2019， [`https://github.com/lucidworks/spark-solr`](https://github.com/lucidworks/spark-solr)

10.  火花；《机器学习库(MLlib)指南》，spark.apache.org， [`http://spark.apache.org/docs/latest/ml-guide.html`](http://spark.apache.org/docs/latest/ml-guide.html)

11.  火花；《OneHotEncoderEstimator》，spark.apache.org，2019， [`https://spark.apache.org/docs/latest/ml-features#onehotencoderestimator`](https://spark.apache.org/docs/latest/ml-features%2523onehotencoderestimator)

12.  谷歌；“分类:ROC 曲线和 AUC”，developers.google.com，2019， [`https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc`](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)

13.  斯蒂芬妮·格伦；《均方误差:定义与实例》，statisticshowto.datasciencecentral.com，2013， [`www.statisticshowto.datasciencecentral.com/mean-squared-error/`](https://www.statisticshowto.datasciencecentral.com/mean-squared-error/)

14.  安朵斯·雅诺西，威廉·施泰因布鲁恩，马蒂亚斯·普菲斯特勒，罗伯特·德特拉诺；《心脏病数据集》，archive.ics.uci.edu，1988， [`http://archive.ics.uci.edu/ml/datasets/heart+Disease`](http://archive.ics.uci.edu/ml/datasets/heart%252BDisease)

15.  火花；“GraphX”，spark.apache.org，2019， [`https://spark.apache.org/graphx/`](https://spark.apache.org/graphx/)

16.  克里斯·马特曼；“孵化器的阿帕奇火花”，mail-archives.apache.org，2013 年，

    [T2`http://mail-archives.apache.org/mod_mbox/incubator-general/201306.mbox/%3CCDD80F64.D5F9D%25chris.a.mattmann@jpl.nasa.gov%3E`](http://mail-archives.apache.org/mod_mbox/incubator-general/201306.mbox/%253CCDD80F64.D5F9D%2525chris.a.mattmann%2540jpl.nasa.gov%253E)

17.  李皓原；“Alluxio，原名超光速粒子，随着 1.0 版本进入新时代，”alluxio.io，2016， [`www.alluxio.com/blog/alluxio-formerly-tachyon-is-entering-a-new-era-with-10-release`](https://www.alluxio.com/blog/alluxio-formerly-tachyon-is-entering-a-new-era-with-10-release)

18.  傅；《用 Alluxio 实现深度学习的灵活快速存储》，alluxio.io，2018， [`www.alluxio.io/blog/flexible-and-fast-storage-for-deep-learning-with-alluxio/`](https://www.alluxio.io/blog/flexible-and-fast-storage-for-deep-learning-with-alluxio/)

19.  Alluxio“Alluxio 虚拟化分布式存储，以内存速度进行 Pb 级计算，”globenewswire.com，2016， [`www.marketwired.com/press-release/alluxio-virtualizes-distributed-storage-petabyte-scale-computing-in-memory-speeds-2099053.html`](https://www.marketwired.com/press-release/alluxio-virtualizes-distributed-storage-petabyte-scale-computing-in-memory-speeds-2099053.htm)

20.  亨利·鲍威尔和吉安马里奥·斯帕卡尼亚；“用超光速粒子让不可能成为可能:将火花工作从数小时加速到数秒，”dzone.com，2016， [`https://dzone.com/articles/Accelerate-In-Memory-Processing-with-Spark-from-Hours-to-Seconds-With-Tachyon`](https://dzone.com/articles/Accelerate-In-Memory-Processing-with-Spark-from-Hours-to-Seconds-With-Tachyon)

21.  李皓原；“Alluxio 在 Strata+Hadoop World Beijing 2016 上的主题演讲”，slideshare.net，2016， [`www.slideshare.net/Alluxio/alluxio-keynote-at-stratahadoop-world-beijing-2016-65172341`](https://www.slideshare.net/Alluxio/alluxio-keynote-at-stratahadoop-world-beijing-2016-65172341)

22.  费明·s。《用用例入门超光速粒子》，intel.com，2016， [`https://software.intel.com/en-us/blogs/2016/02/04/getting-started-with-tachyon-by-use-cases`](https://software.intel.com/en-us/blogs/2016/02/04/getting-started-with-tachyon-by-use-cases)

23.  吉尔·韦尔尼克；“用于超快大数据处理的超光速粒子”，ibm.com，2015， [`www.ibm.com/blogs/research/2015/08/tachyon-for-ultra-fast-big-data-processing/`](https://www.ibm.com/blogs/research/2015/08/tachyon-for-ultra-fast-big-data-processing/)