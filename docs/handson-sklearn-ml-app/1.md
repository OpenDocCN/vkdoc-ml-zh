# 1.Scikit 简介-了解

Scikit-Learn 是一个 Python 库，为实现监督和非监督机器学习算法提供了简单高效的工具。该库对每个人都是可访问的，因为它是开源的，并且是商业可用的。它构建在 NumPY、SciPy 和 matplolib 库之上，这意味着它是可靠的、健壮的，并且是 Python 语言的核心。

Scikit-Learn 专注于数据建模，而不是数据加载、清理、管理或操作。它也非常容易使用，而且相对来说没有编程错误。

## 机器学习

机器学习是让计算机自己编程。我们使用算法来实现这一点。一个*算法*是一套用于计算机计算或解决问题的规则。

机器学习提倡创建、研究和应用算法来提高数据驱动任务的性能。他们通过训练机器如何学习，使用工具和技术来回答有关数据的问题。

目标是构建健壮的算法，该算法可以操纵输入数据来*预测*输出，同时随着新数据的出现不断更新输出。发送到计算机的任何信息或数据都被认为是*输入*。计算机产生的数据被认为是*输出*。

在机器学习社区中，输入数据被称为*特征集*，输出数据被称为*目标*。该特征集也被称为*特征空间*。样本数据通常被称为*训练数据*。一旦用样本数据训练了算法，它就可以对新数据进行预测。新数据通常被称为*测试数据*。

机器学习分为两个主要领域:监督学习和非监督学习。由于机器学习通常侧重于基于从训练数据中学习到的已知属性的预测，因此我们的重点是监督学习。

*监督学习*是指数据集同时包含输入(或特征集)和期望输出(或目标)。也就是说，我们知道数据的属性。目标是做出预测。这种监督算法训练的能力是机器学习变得如此流行的很大一部分原因。

为了对新数据进行分类或回归，我们必须对具有已知结果的数据进行训练。我们通过将数据组织成相关的类别来对其进行分类。我们通过找到特征集数据和目标数据之间的关系来回归数据。

有了*无监督学习*，数据集只包含输入，没有想要的输出(或目标)。目标是探索数据并找到一些组织数据的结构或方法。虽然这不是本书的重点，但我们将探索一些无监督学习场景。

## 蟒蛇

您可以使用任何 Python 安装，但是出于几个原因，我建议使用 Anaconda 安装 Python。首先，它拥有超过 1500 万用户。其次，Anaconda 允许轻松安装所需的 Python 版本。第三，它预装了许多有用的机器学习库，包括 Scikit-Learn。点击这个链接查看您的操作系统和 Python 版本的 Anaconda 包列表: [`https://docs.anaconda.com/anaconda/packages/pkg-docs/`](https://docs.anaconda.com/anaconda/packages/pkg-docs/) 。第四，它包括几个非常受欢迎的编辑器，包括 IDLE、Spyder 和 Jupyter 笔记本。第五，Anaconda 可靠且维护良好，消除了兼容性瓶颈。

你可以通过这个链接轻松下载安装 Anaconda:[`https://www.anaconda.com/download/`](https://www.anaconda.com/download/)。可以用这个链接更新: [`https://docs.anaconda.com/anaconda/install/update-version/`](https://docs.anaconda.com/anaconda/install/update-version/) 。只需打开 Anaconda 并按照说明操作。我建议更新到当前版本。

## Scikit-Learn

Python 的 Scikit-Learn 是最流行的机器学习库之一。它构建在 Python 库 NumPy、SciPy 和 Matplotlib 之上。该库是文档完善的、开源的、商业可用的，并且是开始机器学习的一个很好的工具。它也非常可靠且维护良好，其大量的算法集合可以很容易地集成到您的项目中。Scikit-Learn 专注于数据建模，而不是加载、操作、可视化和汇总数据。对于这样的活动，其他的库，比如 NumPy、pandas、Matplotlib 和 seaborn 也包括在内。Scikit-Learn 库作为 *sklearn* 导入到 Python 脚本中。

## 数据集

理解机器学习应用的一个很好的方法是通过 *Python* 数据驱动的代码例子。我们使用 Scikit-Learn、UCI 机器学习或 seaborn 数据集进行所有示例。Scikit-Learn *数据集*包嵌入了一些小数据集，用于入门，并帮助获取机器学习库中常用的较大数据集，以对来自世界各地的数据进行算法基准测试。UCI 机器学习知识库维护 468 个数据集，以服务于机器学习社区。Seaborn 在 Matplotlib 的基础上提供了一个 API，该 API 在处理绘图样式、颜色默认值和便于可视化的常见统计绘图类型的高级函数时提供了简单性。它还很好地集成了 Pandas DataFrame 功能。

我们选择数据集作为我们的示例，因为机器学习社区使用它们进行学习、探索、基准测试和验证，因此我们可以在学习如何应用机器学习算法的同时将我们的结果与其他人进行比较。

我们的数据集分为分类数据和回归数据。分类数据的复杂性从简单到相对复杂不等。简单分类数据集包括 load_iris、load_wine、bank.csv 和 load_digits。复杂分类数据集包括 fetch_20newsgroups、MNIST 和 fetch_1fw_people。回归数据集包括 tips、redwine.csv、whitewine.csv 和 load_boston。

## 表征数据

在使用算法之前，最好先了解数据特征。每个数据集都经过精心选择，以帮助您获得机器学习最常见方面的经验。我们首先描述每个数据集的特征，以便更好地理解其组成和用途。数据集由分类和回归数据组织。

分类数据按照复杂性进一步组织。也就是说，我们从不复杂的简单分类数据集开始，以便读者可以专注于机器学习内容而不是数据。然后我们转向更复杂的数据集。

### 简单分类数据

*分类*是一种机器学习技术，用于预测因变量所属的类别。一个*类*是一个离散响应。在机器学习中，因变量通常被称为*目标*。基于数据集的独立变量来预测类别。自变量通常被称为*特征集*或*特征空间*。特征空间是用于表征数据的特征的集合。

简单数据集是那些具有有限数量特征的数据集。这样的数据集被称为具有*低维特征空间*的数据集。

#### 虹膜数据

我们表征的第一个数据集是 load_iris，它由 Iris flower 数据组成。Iris 是一个多变量数据集，由来自三种鸢尾(*鸢尾*、*海滨鸢尾*和*杂色鸢尾*)的每一种的 50 个样本组成。每个样本包含四个特征，即以厘米为单位的萼片和花瓣的长度和宽度。Iris 是机器学习分类的典型测试用例。它也是数据科学文献中最著名的数据集之一，这意味着您可以根据许多其他可验证的示例来测试您的结果。

清单 [1-1](#PC1) 中显示的第一个代码示例加载 Iris 数据，显示它的键、特征集和目标的形状、特征和目标名称、来自 DESCR 键的片段以及特征重要性(从最重要到最不重要)。

```py
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier

if __name__ == "__main__":
    br = '\n'
    iris = datasets.load_iris()
    keys = iris.keys()
    print (keys, br)
    X = iris.data
    y = iris.target
    print ('features shape:', X.shape)
    print ('target shape:', y.shape, br)
    features = iris.feature_names
    targets = iris.target_names
    print ('feature set:')
    print (features, br)
    print ('targets:')
    print (targets, br)
    print (iris.DESCR[525:900], br)
    rnd_clf = RandomForestClassifier(random_state=0,
                                     n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    print ('most important features' + ' (' + rnd_name + '):')
    [print (row) for i, row in enumerate(importance)]

Listing 1-1Characterize the Iris data set

```

继续执行清单 [1-1](#PC1) 中的代码。请记住，您可以从本书的示例下载中找到示例。您不需要手动键入示例。更容易访问示例下载和复制/粘贴。

执行清单 [1-1](#PC1) 的输出应该如下所示:

```py
dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])

features shape: (150, 4)
target shape: (150,)

feature set:
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

targets:
['setosa' 'versicolor' 'virginica']

    ============== ==== ==== ======= ===== ====================
                    Min  Max   Mean    SD   Class Correlation
    ============== ==== ==== ======= ===== ====================
    sepal length:   4.3  7.9   5.84   0.83    0.7826
    sepal width:    2.0  4.4   3.05   0.43   -0.4194
    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
    petal width:

most important features (RandomForestClassifier):
(0.4604447396171521, 'petal length (cm)')
(0.4241162651271012, 'petal width (cm)')
(0.09090795402103086, 'sepal length (cm)')
(0.024531041234715754, 'sepal width (cm)')

```

代码从导入数据集和 RandomForestClassifier 包开始。RandomForestClassifier 是一种集成学习方法，它在训练时构建大量决策树，并输出作为类模式的类。

在这个例子中，我们*仅*使用它来返回特征重要性。主程序块首先加载数据并显示其特征。将特征集数据加载到变量 X 并将目标数据加载到变量 y 是机器学习社区的惯例。

代码通过在 pandas 数据上训练 RandomForestClassifier 得出结论，因此它可以返回特征重要性。当实际建模数据时，我们将 pandas 数据转换为 NumPy 以获得最佳性能。请记住，这些键是可用的，因为数据集嵌入在 Scikit-Learn 中。

注意，我们只从 DESCR 中提取了一小部分，其中包含了关于数据集的大量信息。我*总是*建议在开始任何机器学习实验之前，至少显示原始数据集的形状。

### 小费

RandomForestClassifier 是一种强大的机器学习算法，不仅可以对训练数据进行建模，还可以返回特征重要性。

#### 擦除数据

我们表征的下一个数据集是 load_wine。load_wine 数据集由 178 个数据元素组成。每个元素有十三个特征，描述了三个目标类。它被认为是机器学习社区中的经典，并提供了一个简单的多分类数据集。

清单 [1-2](#PC3) 中显示的下一个代码示例加载 wine 数据，并显示它的键、特性集和目标的形状、特性和目标名称、DESCR 键的一个片段以及特性重要性(从最重要到最不重要)。

```py
from sklearn.datasets import load_wine
from sklearn.ensemble import RandomForestClassifier

if __name__ == "__main__":
    br = '\n'
    data = load_wine()
    keys = data.keys()
    print (keys, br)
    X, y = data.data, data.target
    print ('features:', X.shape)
    print ('targets', y.shape, br)
    print (X[0], br)
    features = data.feature_names
    targets = data.target_names
    print ('feature set:')
    print (features, br)
    print ('targets:')
    print (targets, br)
    rnd_clf = RandomForestClassifier(random_state=0,
                                     n_estimators=100)
    rnd_clf.fit(X, y)
    rnd_name = rnd_clf.__class__.__name__
    feature_importances = rnd_clf.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    n = 6
    print (n, 'most important features' + ' (' + rnd_name + '):')
    [print (row) for i, row in enumerate(importance) if i < n]

Listing 1-2Characterize load_wine

```

在执行清单 [1-2](#PC3) 中的代码后，您的输出应该如下所示:

```py
dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])

features: (178, 13)
targets (178,)

[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00
 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]

feature set:
['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']

targets:
['class_0' 'class_1' 'class_2']

6 most important features (RandomForestClassifier):
(0.19399882779940295, 'proline')
(0.16095401215681593, 'flavanoids')
(0.1452667364559143, 'color_intensity')
(0.11070045042456281, 'alcohol')
(0.1097465262717493, 'od280/od315_of_diluted_wines')
(0.08968972021098301, 'hue')

```

### 小费

要创建(实例化)一个机器学习算法(模型)，只需将其赋给一个变量(例如 model = algorithm())。要基于模型进行训练，只需使其符合数据即可(例如，model.fit(X，y))。

代码从导入 load_wine 和 RandomForestClassifier 开始。主块显示关键点，将数据加载到 X 和 y 中，显示特征集 X 中的第一个向量，显示形状，并显示特征集和目标信息。代码以用 RandomForestClassifier 训练 X 结束，因此我们可以显示六个最重要的特性。请注意，我们显示了特性集 X 中的第一个向量，以验证所有特性都是数字的。

#### 银行数据

清单 [1-3](#PC5) 中显示的下一个代码示例处理银行数据。bank.csv 数据集由一家葡萄牙银行机构的直接营销活动组成。目标由客户是否会认购(是/否)定期存款(目标标签为 y)来描述。它由 41188 个数据元素组成，每个元素有 20 个特征。该网站还提供了 4119 个数据元素的 10%随机样本，用于计算成本更高的算法，如 svm 和 KNeighborsClassifier。

```py
import pandas as pd

if __name__ == "__main__":
    br = '\n'
    f = 'data/bank.csv'
    bank = pd.read_csv(f)
    features = list(bank)
    print (features, br)
    X = bank.drop(['y'], axis=1).values
    y = bank['y'].values
    print (X.shape, y.shape, br)
    print (bank[['job', 'education', 'age', 'housing',
                 'marital', 'duration']].head())

Listing 1-3Characterize bank data

```

在执行清单 [1-3](#PC5) 中的代码后，您的输出应该如下所示:

```py
['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'y']

(41188, 20) (41188,)

         job    education  age housing  marital  duration
0  housemaid     basic.4y   56      no  married       261
1   services  high.school   57      no  married       149
2   services  high.school   37     yes  married       226
3     admin.     basic.6y   40      no  married       151
4   services  high.school   56      no  married       307

```

代码示例从导入 pandas 包开始。主程序块将银行数据从 CSV 文件加载到 Pandas 数据框架中，并显示列名(或特征)。要从 pandas 中检索列名，我们需要做的就是将数据帧做成一个列表，并将结果赋给一个变量。接下来，创建特征集 X 和目标 y。最后，会显示 X 和 y 形状以及一些选择功能。

#### 数字数据

本小节的最后一个代码示例是 load_digits。load_digits 数据集由 1797 幅 8 × 8 的手写图像组成。每幅图像由 64 个像素(基于 8 × 8 矩阵)表示，构成了特征集。预测了十个目标，用数字 0 到 9 表示。

清单 [1-4](#PC7) 包含表征 load_digits 的代码。

```py
import numpy as np
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt

if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    print (digits.keys(), br)
    print ('2D shape of digits data:', digits.images.shape, br)
    X = digits.data
    y = digits.target
    print ('X shape (8x8 flattened to 64 pixels):', end=' ')
    print (X.shape)
    print ('y shape:', end=' ')
    print (y.shape, br)
    i = 500
    print ('vector (flattened matrix) of "feature" image:')
    print (X[i], br)
    print ('matrix (transformed vector) of a "feature" image:')
    X_i = np.array(X[i]).reshape(8, 8)
    print (X_i, br)
    print ('target:', y[i], br)
    print ('original "digits" image matrix:')
    print (digits.images[i])
    plt.figure(1, figsize=(3, 3))
    plt.title('reshaped flattened vector')
    plt.imshow(X_i, cmap="gray", interpolation="gaussian")
    plt.figure(2, figsize=(3, 3))
    plt.title('original images dataset')
    plt.imshow(digits.images[i], cmap="gray",
               interpolation='gaussian')
    plt.show()

Listing 1-4Characterize load_digits

```

在执行清单 [1-4](#PC7) 中的代码后，您的输出应该如下所示:

```py
dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])

2D shape of digits data: (1797, 8, 8)

X shape (8x8 flattened to 64 pixels): (1797, 64)
y shape: (1797,)

vector (flattened matrix) of "feature" image:
[ 0\.  0\.  3\. 10\. 14\.  3\.  0\.  0\.  0\.  8\. 16\. 11\. 10\. 13\.  0\.  0\.  0\.  7.
 14\.  0\.  1\. 15\.  2\.  0\.  0\.  2\. 16\.  9\. 16\. 16\.  1\.  0\.  0\.  0\. 12\. 16.
 15\. 15\.  2\.  0\.  0\.  0\. 12\. 10\.  0\.  8\.  8\.  0\.  0\.  0\.  9\. 12\.  4\.  7.
 12\.  0\.  0\.  0\.  2\. 11\. 16\. 16\.  9\.  0.]

matrix (transformed vector) of a "feature" image:
[[ 0\.  0\.  3\. 10\. 14\.  3\.  0\.  0.]
 [ 0\.  8\. 16\. 11\. 10\. 13\.  0\.  0.]
 [ 0\.  7\. 14\.  0\.  1\. 15\.  2\.  0.]
 [ 0\.  2\. 16\.  9\. 16\. 16\.  1\.  0.]
 [ 0\.  0\. 12\. 16\. 15\. 15\.  2\.  0.]
 [ 0\.  0\. 12\. 10\.  0\.  8\.  8\.  0.]
 [ 0\.  0\.  9\. 12\.  4\.  7\. 12\.  0.]
 [ 0\.  0\.  2\. 11\. 16\. 16\.  9\.  0.]]

target: 8

original "digits" image matrix:
[[ 0\.  0\.  3\. 10\. 14\.  3\.  0\.  0.]
 [ 0\.  8\. 16\. 11\. 10\. 13\.  0\.  0.]
 [ 0\.  7\. 14\.  0\.  1\. 15\.  2\.  0.]
 [ 0\.  2\. 16\.  9\. 16\. 16\.  1\.  0.]
 [ 0\.  0\. 12\. 16\. 15\. 15\.  2\.  0.]
 [ 0\.  0\. 12\. 10\.  0\.  8\.  8\.  0.]
 [ 0\.  0\.  9\. 12\.  4\.  7\. 12\.  0.]
 [ 0\.  0\.  2\. 11\. 16\. 16\.  9\.  0.]]

```

列表 [1-4](#PC7) 还显示了数字 [1-1](#Fig1) 和 [1-2](#Fig2) 。图 [1-1](#Fig1) 是数据集中第 500 张图像的整形展平矢量。特征集 *X* 中的每个数据元素都表示为一个 64 像素的展平向量，因为 Scikit-Learn *无法*识别 8 × 8 的图像矩阵，所以我们必须将第 500 个向量整形为 8 × 8 的图像矩阵才能可视化。图 [1-2](#Fig2) 是我们将数据加载到变量*数字*时，直接从可用的图像数据集中获取的第 500 张图像。

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig2_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig2_HTML.jpg)

图 1-2

第 500 个数据元素的原始图像矩阵

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig1_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig1_HTML.jpg)

图 1-1

第 500 个数据元素的整形扁平矢量

代码从导入 numpy、load_digits 和 matplotlib 包开始。主程序块将 load_digits 放入 *digits* 变量中，并显示其关键字:*数据*、*目标*、*目标名称*、*图像*和*描述*。它继续显示包含在*图像*中的图像的二维(2D)形状。*图像*中的数据由 1797 个 8 × 8 矩阵表示。接下来，特征数据(表示为向量)被放置在 X 中，目标数据被放置在 y 中。

*特征向量*是包含关于物体重要特征的信息的向量。*数据*中的数据由 1797 个 64 像素特征向量表示。图像的简单特征表示是每个像素的原始亮度值。因此，一个 8 × 8 的图像由 64 个像素表示。机器学习算法将特征数据作为向量进行处理，因此*数据*中的每个元素都必须是其 2D 图像矩阵的一维(1D)向量表示。

### 小费

特征数据必须由向量组成，以便与机器学习算法一起工作。

代码继续显示第 500 幅图像的特征向量。接下来，将第 500 个特征向量从其展平的 1D 向量形状转换成 2D 图像矩阵，并用 NumPy 整形函数显示。代码继续显示第 500 幅图像的目标值 y。接下来，通过参考*图像*来显示第 500 个图像矩阵。

我们将图像从其 1D 展平矢量状态转换为 2D 图像矩阵的原因是，大多数数据集不包括像 load_data 这样的 *images* 对象。因此，为了用机器学习算法可视化和处理数据，我们必须能够手动*展平*图像，并将展平的图像转换回其原始的 2D 矩阵形状。

代码以两种方式可视化第 500 张图片结束。首先，我们使用展平的矢量 X_i。其次，我们参考*图像*。虽然机器学习算法需要特征向量，但函数 imshow 需要 2D 图像矩阵来可视化。

### 复杂分类数据

现在让我们处理更复杂的数据集。复杂数据集是那些具有大量特征的数据集。这样的数据集被称为具有*高维特征空间*的数据集。

#### 新闻组数据

我们描述的第一个数据集是 fetch_20newsgroups，它由 20 个主题的大约 18000 篇帖子组成。数据被分成训练测试子集。这种拆分基于特定日期前后发布的消息。

清单 [1-5](#PC9) 包含描述 fetch_20newsgroups 的代码。

```py
from sklearn.datasets import fetch_20newsgroups

if __name__ == "__main__":
    br = '\n'
    train = fetch_20newsgroups(subset='train')
    test = fetch_20newsgroups(subset='test')
    print ('data:')
    print (train.target.shape, 'shape of train data')
    print (test.target.shape, 'shape of test data', br)
    targets = test.target_names
    print (targets, br)
    categories = ['rec.autos', 'rec.motorcycles', 'sci.space',
                  'sci.med']
    train = fetch_20newsgroups(subset='train',
                               categories=categories)
    test = fetch_20newsgroups(subset='test',
                              categories=categories)
    print ('data subset:')
    print (train.target.shape, 'shape of train data')
    print (test.target.shape, 'shape of test data', br)
    targets = train.target_names
    print (targets)

Listing 1-5Characterize fetch_20newsgroups

```

在执行清单 [1-5](#PC9) 中的代码后，您的输出应该如下所示:

```py
data:
(11314,) shape of train data
(7532,) shape of test data

['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']

data subset:
(2379,) shape of train data
(1584,) shape of test data

['rec.autos', 'rec.motorcycles', 'sci.med', 'sci.space']

```

代码从导入 fetch_20newsgroups 开始。主程序块首先加载训练和测试数据，并显示它们的形状。训练数据包含 11314 个帖子，而测试数据包含 7532 个帖子。代码继续显示目标名称和类别。接下来，从类别的子集创建训练和测试数据。代码以显示子集的形状和目标名称结束。

#### MNIST 数据

我们表征的下一个数据集是 MNIST。MNIST(改进的国家标准和技术研究所)是一个大型的手写数字数据库，通常用于机器学习社区和其他工业图像处理应用程序的训练和测试。MNIST 包含 70000 个手写数字图像的例子，从 0 到 9，大小为 28 × 28。每个目标(或标签)存储为一个数字值。该特征集是 70000 个 28 × 28 图像的矩阵，每个图像自动展平为 784 像素。因此，70000 个数据元素中的每一个都是长度为 784 的向量。目标集是一个 70000 位数值的向量。

清单 [1-6](#PC11) 包含了 MNIST 特有的代码。

```py
import numpy as np
from random import randint
import matplotlib.pyplot as plt

def find_image(data, labels, d):
    for i, row in enumerate(labels):
        if d == row:
            target = row
            X_pixels = np.array(data[i])
            return (target, X_pixels)

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_mnist.npy')
    y = np.load('data/y_mnist.npy')
    target = np.load('data/mnist_targets.npy')
    print ('labels (targets):')
    print (target, br)
    print ('feature set shape:')
    print (X.shape, br)
    print ('target set shape:')
    print (y.shape, br)
    indx = randint(0, y.shape[0]-1)
    target = y[indx]
    X_pixels = np.array(X[indx])
    print ('the feature image consists of', len(X_pixels),
           'pixels')
    X_image = X_pixels.reshape(28, 28)
    plt.figure(1, figsize=(3, 3))
    title = 'image @ indx ' + str(indx) + ' is digit ' \
            + str(int(target))
    plt.title(title)
    plt.imshow(X_image, cmap="gray")
    digit = 7
    target, X_pixels = find_image(X, y, digit)
    X_image = X_pixels.reshape(28, 28)
    plt.figure(2, figsize=(3, 3))
    title = 'find first ' + str(int(target)) + ' in dataset'
    plt.title(title)
    plt.imshow(X_image, cmap="gray")
    plt.show()

Listing 1-6Characterize MNIST

```

在执行清单 [1-6](#PC11) 中的代码后，您的输出应该如下所示:

```py
labels (targets):
[0\. 1\. 2\. 3\. 4\. 5\. 6\. 7\. 8\. 9.]

feature set shape:
(70000, 784)

target set shape:
(70000,)

the feature image consists of 784 pixels

```

列表 [1-6](#PC11) 还显示了数字 [1-3](#Fig3) 和 [1-4](#Fig4) 。图 [1-3](#Fig3) 是索引 6969 处数字 1 的整形图像。图 [1-4](#Fig4) 是数据集中数字 7 的第一幅图像。

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig4_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig4_HTML.jpg)

图 1-4

数据集中第一个数字 7 的图像

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig3_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig3_HTML.jpg)

图 1-3

索引 6969 处图像的整形展平矢量

代码从导入 randint 和其他必需的包开始。函数 find_image 定位图像的第一次出现。主块将 NumPy 文件中的数据加载到功能集 X、目标 y 和目标中。可变目标保存目标标签。它继续显示 X 和 y 的形状。特征集 X 由 70000 个 784 像素的向量组成，因此每个图像由 28 × 28 像素组成。

目标 y 由 70000 个标签组成。接下来，生成一个介于 0 和 69999 之间的随机整数，因此我们可以从数据集中显示一个随机图像。我们例子中的随机整数是 6969。索引 6969 处的图像是数字 1。显示图像的大小以验证其为 784 像素。然后，我们将 vector 6969 整形为一个 28 × 28 的矩阵，这样我们就可以用函数 imshow 来可视化。代码通过找到第一个数字 7 并显示它来结束。

#### 面孔数据

本小节中描述的最终数据集是 fetch_1fw_people。fetch_1fw_people 数据集用于分类标记的人脸。它包含 1288 张人脸图像和 7 个目标。每幅图像由一个 50 × 37 的像素矩阵表示，因此特征集有 1850 个特征(基于一个 50 × 37 的矩阵)。总之，该数据由 1288 个标记的人脸组成，每个人脸由 1850 个像素组成，每个像素预测七个目标。

清单 [1-7](#PC13) 包含描述 fetch_1fw_people 的代码。

```py
import numpy as np
import matplotlib.pyplot as plt

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_faces.npy')
    y = np.load('data/y_faces.npy')
    targets = np.load('data/faces_targets.npy')
    print ('shape of feature and target data:')
    print (X.shape)
    print (y.shape, br)
    print ('target faces:')
    print (targets)
    X_i = np.array(X[0]).reshape(50, 37)
    image_name = targets[y[0]]
    fig, ax = plt.subplots()
    image = ax.imshow(X_i, cmap="bone")
    plt.title(image_name)
    plt.show()

Listing 1-7Characterize fetch_1fw_people

```

在执行清单 [1-7](#PC13) 中的代码后，您的输出应该如下所示:

```py
shape of feature and target data:
(1288, 1850)
(1288,)

target faces:
['Ariel Sharon' 'Colin Powell' 'Donald Rumsfeld' 'George W Bush'
 'Gerhard Schroeder' 'Hugo Chavez' 'Tony Blair']

```

列表 [1-7](#PC13) 也显示图 [1-5](#Fig5) 。图 [1-5](#Fig5) 是数据集中第一个数据元素的整形图像。

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig5_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig5_HTML.jpg)

图 1-5

数据集中第一个数据元素的整形图像

代码从导入必需的包开始。主块将数据从 NumPy 文件加载到 X、y 和 targets 中。代码继续打印 X 和 y 的形状。X 包含 1288 个 1850 像素的向量，y 包含 1288 个目标值。然后显示目标标签。代码最后将第一个特征向量整形为 50 × 37 的图像，并用函数 imshow 显示出来。

### 回归数据

我们现在从分类转向回归。*回归*是一种基于数据集的自变量(或特征集)预测数值的机器学习技术。也就是说，我们正在测量特性集对一个*数字*输出的影响。我们表征回归的第一个数据集是 tips。

#### 小费数据

tips 数据集与 seaborn 库集成在一起。它由餐馆的服务员小费和相关因素组成，包括小费、饭菜价格和一天中的时间。具体来说，特征包括 total_bill(餐费)、小费(小费)、性别(男性或女性)、吸烟者(是或否)、日期(周四、周五、周六或周日)、时间(白天或晚上)以及聚会的规模。特征编码如下:总账单(美元)、小费(美元)、性别(0 =男性，1 =女性)、吸烟者(0 =否，1 =是)、日(3 =星期四，4=Fri，5=星期六，6 =星期日)。Tips 数据由 244 个元素表示，具有预测一个目标的六个特征。目标是从顾客那里得到的小费。

列表 [1-8](#PC15) 表征 tips 数据。

```py
import numpy as np, pandas as pd, seaborn as sns

if __name__ == "__main__":
    br = '\n'
    sns.set(color_codes=True)
    tips = sns.load_dataset('tips')
    print (tips.head(), br)
    X = tips.drop(['tip'], axis=1).values
    y = tips['tip'].values
    print (X.shape, y.shape)

Listing 1-8Characterize the tips data set

```

在执行清单 [1-8](#PC15) 中的代码后，您的输出应该如下所示:

```py
   total_bill   tip     sex smoker  day    time  size
0       16.99  1.01  Female     No  Sun  Dinner     2
1       10.34  1.66    Male     No  Sun  Dinner     3
2       21.01  3.50    Male     No  Sun  Dinner     3
3       23.68  3.31    Male     No  Sun  Dinner     2
4       24.59  3.61  Female     No  Sun  Dinner     4

(244, 6) (244,)

```

代码首先以 Pandas 数据帧的形式加载提示，显示前五条记录，将数据转换为 NumPy，并显示特性集和目标形状。Seaborn 数据被自动加载为 Pandas 数据帧。我们无法获取要素重要性，因为 RandomForestClassifier 需要数值数据。将数据集转换成这种形式需要大量的数据争论。我们将在后面的章节中把分类数据转换成数字。

#### 红酒和白酒

我们表征的下两个数据集是 redwine.csv 和 whitewine.csv。数据集 redwine.csv 和 whitewine.csv 分别与红葡萄酒和白葡萄酒的*质量、*相关。这两种葡萄酒都是由葡萄牙 Vinho Verde 葡萄酒的变种组成。

该特性集由 11 个属性组成。输入属性基于客观测试，如 pH(物质的酸度或碱度)和酒精(体积百分比)。输出质量是基于感官数据，报告为至少三个葡萄酒专家评估的中位数。每位专家将葡萄酒质量从 0(非常差)到 10(非常好)分等级。红酒数据集有 1599 个实例，而白酒数据集有 4898 个。

清单 [1-9](#PC17) 描述了 redwine.csv。

```py
import pandas as pd
from sklearn.ensemble import RandomForestRegressor

if __name__ == "__main__":
    br = '\n'
    f = 'data/redwine.csv'
    red_wine = pd.read_csv(f)
    X = red_wine.drop(['quality'], axis=1)
    y = red_wine['quality']
    print (X.shape)
    print (y.shape, br)
    features = list(X)
    rfr = RandomForestRegressor(random_state=0,
                                n_estimators=100)
    rfr_name = rfr.__class__.__name__
    rfr.fit(X, y)
    feature_importances = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    n = 3
    print (n, 'most important features' + ' (' + rfr_name + '):')
    [print (row) for i, row in enumerate(importance) if i < n]
    for row in importance:
        print (row)
    print ()
    print (red_wine[['alcohol', 'sulphates', 'volatile acidity',
                     'total sulfur dioxide', 'quality']]. head())

Listing 1-9Characterize redwine

```

在执行清单 [1-9](#PC17) 中的代码后，您的输出应该如下所示:

```py
(1599, 11)
(1599,)

3 most important features (RandomForestRegressor):
(0.27432500255956216, 'alcohol')
(0.13700073893077233, 'sulphates')
(0.13053941311188708, 'volatile acidity')
(0.27432500255956216, 'alcohol')
(0.13700073893077233, 'sulphates')
(0.13053941311188708, 'volatile acidity')
(0.08068199773601588, 'total sulfur dioxide')
(0.06294612644261727, 'chlorides')
(0.057730976351602854, 'pH')
(0.055499749756166, 'residual sugar')
(0.05198192402458334, 'density')
(0.05114079873500658, 'fixed acidity')
(0.049730883807319035, 'free sulfur dioxide')
(0.04842238854446754, 'citric acid')

   alcohol  sulphates  volatile acidity  total sulfur dioxide  quality
0      9.4       0.56              0.70                  34.0      5.0
1      9.8       0.68              0.88                  67.0      5.0
2      9.8       0.65              0.76                  54.0      5.0
3      9.8       0.58              0.28                  60.0      6.0
4      9.4       0.56              0.70                  34.0      5.0

```

代码示例从加载 pandas 和 RandomForestRegressor 包开始。主程序块将 redwine.csv 加载到 Pandas DataFrame 中。然后显示特征和目标形状。代码最后用 RandomForestRegressor 训练 pandas 数据，显示三个最重要的特性，并显示数据集中的前五条记录。RandomForestRegressor 也是一种集成算法，但它用于目标为数字或连续的情况。

### 小费

对于使用该参数来稳定结果的算法，总是硬编码 *random_state* (例如，random_state=0)。

白葡萄酒示例遵循*完全相同的*逻辑，但是输出在数据集大小和特性重要性方面有所不同。

列表 [1-10](#PC19) 描述了 whitewine.csv。

```py
import numpy as np, pandas as pd
from sklearn.ensemble import RandomForestRegressor

if __name__ == "__main__":
    br = '\n'
    f = 'data/whitewine.csv'
    white_wine = pd.read_csv(f)
    X = white_wine.drop(['quality'], axis=1)
    y = white_wine['quality']
    print (X.shape)
    print (y.shape, br)
    features = list(X)
    rfr = RandomForestRegressor(random_state=0,
                                n_estimators=100)
    rfr_name = rfr.__class__.__name__
    rfr.fit(X, y)
    feature_importances = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    n = 3
    print (n, 'most important features' + ' (' + rfr_name + '):')
    [print (row) for i, row in enumerate(importance) if i < n]
    print ()
    print (white_wine[['alcohol', 'sulphates',
                       'volatile acidity',
                       'total sulfur dioxide',
                       'quality']]. head())

Listing 1-10Characterize whitewine

```

在执行清单 [1-10](#PC19) 中的代码后，您的输出应该如下所示:

```py
(4898, 11)
(4898,)

3 most important features (RandomForestRegressor):
(0.24186185906056268, 'alcohol')
(0.1251626059551235, 'volatile acidity')
(0.11524332271725685, 'free sulfur dioxide')

   alcohol  sulphates  volatile acidity  total sulfur dioxide  quality
0      8.8       0.45              0.27                 170.0      6.0
1      9.5       0.49              0.30                 132.0      6.0
2     10.1       0.44              0.28                  97.0      6.0
3      9.9       0.40              0.23                 186.0      6.0
4      9.9       0.40              0.23                 186.0      6.0

```

#### 波士顿数据

我们表征的最终数据集是 load_boston。load_boston 数据集包含波士顿不同位置的房价。它由 506 条记录组成，有 13 个特征和一个目标。目标值代表业主自住房屋的中值。

清单 [1-11](#PC21) 表征 load_boston。

```py
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor

if __name__ == "__main__":
    br = '\n'
    boston = load_boston()
    print (boston.keys(), br)
    print (boston.feature_names, br)
    X = boston.data
    y = boston.target
    print ('feature shape', X.shape)
    print ('target shape', y.shape, br)
    keys = boston.keys()
    rfr = RandomForestRegressor(random_state=0,
                                n_estimators=100)
    rfr.fit(X, y)
    features = boston.feature_names
    feature_importances = rfr.feature_importances_
    importance = sorted(zip(feature_importances, features),
                        reverse=True)
    [print(row) for i, row in enumerate(importance) if i < 3]

Listing 1-11Characterize load_boston

```

在执行清单 [1-11](#PC21) 中的代码后，您的输出应该如下所示:

```py
dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])

['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'
 'B' 'LSTAT']

feature shape (506, 13)
target shape (506,)

(0.45730362625767496, 'RM')
(0.35008661885681375, 'LSTAT')
(0.06518862820215894, 'DIS')

```

代码从导入 load_boston 和 RandomForestRegressor 开始。主块显示键，将数据加载到 X 和 y 中，并显示要素和数据形状。代码继续创建 RandomForestRegressor 并用 X 和 y 对其进行训练，以便显示特征的重要性。

## 特征缩放

*特性缩放*正在标准化特性集数据。当要素集数据在量值、单位和范围方面变化很大时，要素缩放很重要。如果这样的数据没有被缩放，一些机器学习算法可能表现不佳，因为它们不能正确地考虑特征集数据差异。为了缓解这个问题，我们将方差标准化。标准化将要素重新调整为均值为零(μ = 0)和标准差为 1(σ= 1)的标准正态分布的属性。也就是说，我们通过移除平均值并缩放至单位方差来重新缩放要素。

Scikit-Learn 应用 StandardScaler，它通过移除平均值并缩放到单位方差来标准化特征。清单 [1-12](#PC23) 中显示的代码示例演示了 StandardScaler 如何与机器学习算法一起工作。

```py
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target
    X_train, X_test, y_train, y_test =\
             train_test_split(X, y, random_state=0)
    sgd = SGDClassifier(random_state=0, max_iter=1000,
                        tol=0.001)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    print ('<<' + sgd_name + '>>', br)
    y_pred = sgd.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print ('unscaled \'test\' accuracy:', accuracy)
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train),\
                              scaler.transform(X_test)
    sgd_std = SGDClassifier(random_state=0, max_iter=1000,
                            tol=0.001)
    sgd_std.fit(X_train_std, y_train)
    y_pred = sgd_std.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print ('scaled \'test\' accuracy:', np.round(accuracy, 4))

Listing 1-12Scaling load_digits

```

在执行清单 [1-12](#PC23) 中的代码后，您的输出应该如下所示:

```py
<<SGDClassifier>>

unscaled 'test' accuracy: 0.92
scaled 'test' accuracy: 0.9333

```

代码示例首先加载 train_test_split、SGDClassifier、StandardScaler、accuracy_score 和其他必需的包。train_test_split 将数据集中的向量(数据元素)分割成随机的训练测试子集。机器学习算法使用*训练*子集进行训练，而*测试*子集用于验证。

将数据分成训练测试子集是机器学习的基础，因为模型从训练数据中学习，而测试数据被认为是模型从未见过的新数据。由于测试数据从未被模型看到，我们可以确信我们的准确度分数是有效的。所以，*从来没有*用测试数据进行训练！

SGDClassifier 是一种分类算法，通过随机梯度下降(SGD)学习实现正则化线性模型。每次对每个样本估计损失(或误差)的梯度，并且该模型沿着递减的强度时间表(或学习速率)进行更新。分类是预测给定数据点的目标。目标也称为类、标签或类别。分类将在接下来的两章中深入讨论。

accuracy_score 用于计算准确度。主程序块首先将 load_digits 放入特征集 X 和目标集 y。代码接着将 X 和 y 分成训练测试子集。X_train 和 y_train 用于训练。X_test 和 y_test 用于验证。接下来，创建模型并将其分配给变量 sgd。然后用 X_train 和 y_train 对模型进行训练。然后根据 X_test 进行预测，并将其分配给 y_pred。通常，预测是根据测试数据进行的，但我们可以根据训练数据进行预测，以了解我们的模型表现如何。代码继续缩放训练数据，用缩放的数据训练模型，并显示精度。注意缩放提高了精确度。

## 降维

降维(或特征)是通过获得一组主变量(或分量)来减少所考虑的随机变量的数量。*主成分*是一组线性不相关变量的值。

前提是数据包含一些冗余或不相关的特征，因此可以删除而不会丢失太多信息。然而，请记住，维度缩减*总是*会导致一些信息丢失。

降维可以简化模型，减少训练时间，减少过拟合，避免维数灾难。*过拟合*是模型对数据训练得太好。也就是说，模型完全理解数据，但也会产生噪声(或误差)。因此，*不需要的*噪声成为模型理解数据的一部分。当在高维空间(可能包含成百上千个维度)处理数据时,*诅咒*尤为突出，因为它使得分析和组织数据变得非常困难。在低维空间如 2D 或人类经验中常见的三维(3D)空间中处理数据要容易得多。

维数约简对于无监督学习是有用的。无监督学习从特征数据中进行推断，而不知道它们各自的标记响应(或目标)。无监督学习对于探索数据中隐藏的模式或分组很有用。

三种常见的 Scikit-Learn 降维技术是主成分分析(PCA)、线性判别分析(LDA)和 Isomap。PCA 和 LDA 是*线性*降维方法。Isomap 是一种*非线性*降维方法。

清单 [1-13](#PC25) 中显示的第一个代码示例利用 Iris 上的 PCA 和 LDA 来识别聚类。

```py
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import\
     LinearDiscriminantAnalysis
import seaborn as sns, matplotlib.pyplot as plt

if __name__ == "__main__":
    br = '\n'
    iris = load_iris()
    X = iris.data
    y = iris.target
    pca = PCA(n_components=0.95)
    X_reduced = pca.fit_transform(X)
    components = pca.n_components_
    model = PCA(n_components=components)
    model.fit(X)
    X_2D = model.transform(X)
    iris_df = sns.load_dataset('iris')
    iris_df['PCA1'] = X_2D[:, 0]
    iris_df['PCA2'] = X_2D[:, 1]
    print (iris_df[['PCA1', 'PCA2']].head(3), br)
    sns.set(color_codes=True)
    sns.lmplot('PCA1', 'PCA2', hue="species",
               data=iris_df, fit_reg=False)
    plt.suptitle('PCA reduction')
    lda = LinearDiscriminantAnalysis(n_components=2)
    transform_lda = lda.fit_transform(X, y)
    iris_df['LDA1'] = transform_lda[:,0]
    iris_df['LDA2'] = transform_lda[:,1]
    print (iris_df[['LDA1', 'LDA2']].head(3))
    sns.lmplot('LDA1', 'LDA2', hue="species",
               data=iris_df, fit_reg=False)
    plt.suptitle('LDA reduction')
    plt.show()

Listing 1-13PCA and LDA Iris dimensionality reduction

```

在执行清单 [1-13](#PC25) 中的代码后，您的输出应该如下所示:

```py
       PCA1      PCA2
0 -2.684126  0.319397
1 -2.714142 -0.177001
2 -2.888991 -0.144949

       LDA1      LDA2
0  8.061800  0.300421
1  7.128688 -0.786660
2  7.489828 -0.265384

```

列表 [1-13](#PC25) 还显示了数字 [1-6](#Fig6) 和 [1-7](#Fig7) 。图 [1-6](#Fig6) 展示了 PCA 降维如何用于聚类的无监督学习可视化。图 [1-7](#Fig7) 展示了使用 LDA 进行降维如何有助于聚类的无监督学习可视化。

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig7_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig7_HTML.jpg)

图 1-7

LDA 降维

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig6_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig6_HTML.jpg)

图 1-6

主成分分析降维

代码示例从导入 PCA、LinearDiscriminantAnalysis 和其他必需的包开始。PCA 降低了由许多相关变量组成的数据的维数，同时保留了其大部分变化(或信息)。线性判别分析(LinearDiscriminantAnalysis)是判别函数分析(discriminant function analysis)，它是将事物分配到同一类型的组、类或类别中的行为。

主程序块将 Iris 加载到 X 和 y 中，然后创建一个信息损失为 5%的 PCA 模型，并将 X 转换到 2D 空间，这样我们就可以自动确定主成分的最佳数量。接下来，创建模型并对数据进行训练。代码继续将 Iris 加载到 Pandas 数据帧中，这样我们就可以将两个主要组件切掉以便可视化。代码继续创建 LDA 模型，并根据数据对其进行训练。

请注意，LDA 对 X 和 y 数据进行训练，而 PCA 只对 X 数据进行训练。这两种方法都很好地实现了三种鸢尾的聚类可视化。

清单 [1-14](#PC27) 中显示的最终代码示例使用 Isomap 来识别 load_digits 上的集群。

```py
from sklearn.datasets import load_digits
from sklearn.manifold import Isomap
import matplotlib.pyplot as plt

if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target
    print ('feature data shape:', X.shape)
    iso = Isomap(n_components=2)
    iso_name = iso.__class__.__name__
    iso.fit(digits.data)
    data_projected = iso.transform(X)
    print ('project data to 2D:', data_projected.shape)
    project_1, project_2 = data_projected[:, 0],\
                           data_projected[:, 1]
    plt.figure(iso_name)
    plt.scatter(project_1, project_2, c=y, edgecolor="none",
                alpha=0.5, cmap="jet")
    plt.colorbar(label='digit label', ticks=range(10))
    plt.clim(-0.5, 9.5)
    plt.show()

Listing 1-14Isomap visualization

```

在执行清单 [1-14](#PC27) 中的代码后，您的输出应该如下所示:

```py
feature data shape: (1797, 64)
project data to 2D: (1797, 2)

```

清单 [1-14](#PC27) 还显示了图 [1-8](#Fig8) ，展示了 load_digits 上的 Isomap 可视化。

![../images/481580_1_En_1_Chapter/481580_1_En_1_Fig8_HTML.jpg](../images/481580_1_En_1_Chapter/481580_1_En_1_Fig8_HTML.jpg)

图 1-8

load_digits 上聚类的 Isomap 可视化

代码示例从导入 Isomap 和其他必备包开始。主程序块将数字数据加载到 X 和 y 中，并显示特征集 X 的形状。代码继续创建 Isomap 模型，将数据投影到 2D 空间，将主要组件分割为变量 project_1 和 project_2，并进行可视化。Isomap 在识别数字群 0-9 方面表现出色。

Isomap 是一个优秀的*非线性*数据可视化工具。由于 load_digits 数据是非线性的，Isomap 工作得很好。