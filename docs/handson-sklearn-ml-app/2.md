# 2.简单训练集的分类

分类是预测离散类别标签的问题。类也称为目标、标签或类别。通过对训练数据训练分类器算法来预测新数据如何被分类，从而应用分类。

机器学习分类数据集由特征(X)和目标(y)组成，其中输入变量 *X* 描述已知的离散输出变量 *y* 。特征数据通常被称为特征集(或特征空间)。分类被认为是监督学习，因为我们知道对应于特征集的目标。

咻！那太多了。所以，让我们看一个简单的例子来帮助你理解分类是如何工作的。假设我们有一个由四类水果组成的数据集，即“苹果”、“橘子”、“柠檬”和“酸橙”。每个数据元素(或行)通过质量、宽度、高度和颜色(特征)描述一个水果(目标)。因此，苹果和桔子可以通过不同的质量、宽度、高度和颜色来区分。

在这个例子中，类标签是水果的类型。每种水果都是独立的。也就是说，苹果很容易与其他种类的水果区分开来。目标是根据水果的质量、宽度、高度和颜色来预测水果的类型。

为了训练数据集，我们将数据分成训练测试子集。列车数据特征称为 *X_train* ，目标称为 *y_train* 。测试数据特征称为 *X_test* ，目标称为 *y_test* 。然后，我们建立一个分类模型来训练 X_train 和 y_train 数据。一旦对模型进行了训练，我们就可以从 X_test 和 y_test 数据中进行验证和预测，因为模型还没有看到测试数据。通过将测试数据排除在训练过程之外，它有效地充当了*新*数据。

### 小费

千万不要在测试数据上训练，以保持数据的纯净。

典型的训练测试分割是 70%/30%，但是应该根据数据集的大小来选择该比率。如果数据集很小，30%的测试集可能不包含所有的类或足够的信息来正确验证。此外，不同类别在训练集和测试集中的分布应该等于实际数据集。确保这种分布的最好方法是随机分割训练测试子集。幸运的是，Scikit-Learn 的 train_test_split 包会自动随机化拆分，但其默认的 train-test 拆分是 75%/25%。

我推荐一些处理机器学习问题时的通用步骤。首先，总是为了训练和验证的目的分割数据。第二，尝试扩展数据以潜在地提高性能。第三，试验训练和测试规模。第四，始终从基线模型、简单算法或基于数据集先前经验的算法开始。从算法的默认超参数开始。第五，试验更复杂的模型，因为 Scikit-Learn 是高效的，并且允许容易的模型替换。当处理大数据集时，尝试抽取随机样本以减少计算开销。当处理高维数据集时，尝试使用 PCA 或 LDA 进行降维，以减少计算开销。第六，调整前面步骤中确定的最佳算法，以获得最佳性能。最后，多做一些实验。机器学习是非常耗费时间和严谨的，所以要有耐心，不要放弃。

### 小费

总是从算法的默认超参数开始训练。

## 简单数据集

我们专注于四个简单的数据集来介绍机器学习分类:葡萄酒、数字、银行和月亮。我们没有在第 [1](1.html) 章介绍人造月亮，因为它是人为的。也就是说，Scikit-Learn 为 make_moons 提供了基础，我们按照自己认为合适的方式构建它。

### 葡萄酒数据分类

清单 [2-1](#PC1) 中所示的代码示例对葡萄酒数据进行分类。

```py
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import\
     LinearDiscriminantAnalysis as LDA
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import train_test_split
from sklearn import metrics
from random import *

if __name__ == "__main__":
    br = '\n'
    data = load_wine()
    X = data.data
    y = data.target
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.30, random_state=0)
    lda = LDA().fit(X_train, y_train)
    print (lda, br)
    lda_name = lda.__class__.__name__
    y_pred = lda.predict(X_train)
    accuracy = metrics.accuracy_score(y_train, y_pred)
    accuracy = str(accuracy * 100) + '%'
    print (lda_name + ':')
    print ('train:', accuracy)
    y_pred_test = lda.predict(X_test)
    accuracy = metrics.accuracy_score(y_test, y_pred_test)
    accuracy = str(round(accuracy * 100, 2)) + '%'
    print ('test: ', accuracy, br)
    print('Confusion Matrix', lda_name)
    print(metrics.confusion_matrix(y_test, lda.predict(X_test)), br)
    std_scale = StandardScaler().fit(X_train)
    X_train = std_scale.transform(X_train)
    X_test = std_scale.transform(X_test)
    sgd = SGDClassifier(max_iter=5, random_state=0)
    print (sgd, br)
    sgd.fit(X_train, y_train)
    sgd_name = sgd.__class__.__name__
    y_pred = sgd.predict(X_train)
    y_pred_test = sgd.predict(X_test)
    print (sgd_name + ':')
    print('train: {:.2%}'.format(metrics.accuracy_score\(y_train, y_pred)))
    print('test:  {:.2%}\n'.format(metrics.accuracy_score\(y_test, y_pred_test)))
    print('Confusion Matrix', sgd_name)
    print(metrics.confusion_matrix(y_test, sgd.predict(X_test)), br)
    n, ls = 100, []

    for i, row in enumerate(range(n)):
        rs = randint(0, 100)
        sgd = SGDClassifier(max_iter=5, random_state=0)
        sgd.fit(X_train, y_train)
        y_pred = sgd.predict(X_test)
        accuracy = metrics.accuracy_score(y_test, y_pred)
        ls.append(accuracy)
    avg = sum(ls) / len(ls)
    print ('MCS (true test accuracy):', avg)

Listing 2-1Classify load_wine data

```

继续执行清单 [2-1](#PC1) 中的代码。请记住，您可以从本书的示例下载中找到示例。您不需要手动键入示例。更容易访问示例下载和复制/粘贴。

执行清单 [2-1](#PC1) 的输出应该如下所示:

```py
LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None, solver="svd", store_covariance=False, tol=0.0001)

LinearDiscriminantAnalysis:
train: 100.0%
test:  98.15%

Confusion Matrix LinearDiscriminantAnalysis
[[19  0  0]
 [ 1 21  0]
 [ 0  0 13]]

SGDClassifier(alpha=0.0001, average=False, class_weight=None,
       early_stopping=False, epsilon=0.1, eta0=0.0,
       fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss="hinge", max_iter=5,
       n_iter=None, n_iter_no_change=5, n_jobs=None,
       penalty='l2', power_t=0.5, random_state=0, shuffle=True,
       tol=None, validation_fraction=0.1, verbose=0,
       warm_start=False)

SGDClassifier:
train: 100.00%
test:  100.00%

Confusion Matrix SGDClassifier
[[19  0  0]
 [ 0 22  0]
 [ 0  0 13]]

MCS (true test accuracy): 1.0

```

代码从导入度量、随机和必需的包开始。主程序块首先加载数据，并将其分成训练测试子集。请注意，我们将测试规模调整为 30%。接下来，创建线性判别分析(LDA)模型，并在训练集上进行训练。你可以调整测试规模，看看你的准确性是否有所提高。但是，不要搞得太大。您的模型需要训练数据来更好地理解和学习。

### 小费

要查看模型的超参数，只需在创建后打印保存模型的变量(例如，print (lda))。

LDA 是在第 [1](1.html) 章中介绍的，作为一种无监督的降维学习模型。LDA 是一个非常有趣的模型，因为它执行非监督维度缩减*和*监督分类。

数据缩放并不能提高 LDA 的性能，所以模型是在未缩放的数据上训练的。然后计算训练和测试子集的准确度分数。性能准确性通常只在测试数据上报告。但是，获得训练和测试精度以查看模型与数据的拟合程度是很有用的。在这种情况下，模型非常符合数据，因为训练精度和测试精度非常相似。如果训练精度远高于测试精度，则模型会过度拟合数据。

代码继续显示混淆矩阵。一个*混淆矩阵*描述了一个分类模型(或分类器)对一组真实值已知的测试数据的性能。由 19、21 和 13 组成的对角线是模型正确分类的地方。该模型只对测试集中的一个数据元素进行了错误分类，这非常有意义，测试准确率超过 98%。接下来，我们缩放数据，因为众所周知 SGDClassifier 在处理缩放数据时性能更好。该模型被训练，并且训练和测试精度与混淆矩阵一起显示。有了这个模型，分类就完美了。

代码的最后一部分是可选的。它使用蒙特卡罗实验来验证 SGD 分类器在葡萄酒数据上的性能。*蒙特卡罗实验*利用随机性解决确定性(或监督性)问题。完美的测试准确率 100%，我们应该有点怀疑。因此，我们运行了 100 次蒙特卡罗实验来获得实际的测试性能。如你所见，我们得到 100%！

蒙特卡罗实验是一种很好的获得精确度的方法，但是计算量非常大。我们在这种情况下是安全的，因为数据集既小又简单。对于高维数据的大数据集，蒙特卡罗实验并不是很实用。

线性判别分析和 SGD 分类器不是随机选择的。通过严格的反复试验和研究，这些算法被战略性地确定为最佳表现。

### 小费

每个数据集都是不同的，所以通过反复试验和研究来战略性地选择算法。

### 分类数字

清单 [2-2](#PC3) 中显示的第一个代码示例加载数据，并将其分成训练测试子集。接下来，使用分类器 GaussianNB、SGDClassifier 和 svm 训练数据。算法 svm 表现最好。然后代码识别并可视化错误分类。代码以可视化第一个错误分类结束。

```py
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

def find_misses(test, pred):
    return [i for i, row in enumerate(test) if row != pred[i]]

if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target
    X_train, X_test, y_train, y_test = train_test_split\
                                       (X, y, random_state=0)
    gnb = GaussianNB().fit(X_train, y_train)
    gnb_name = gnb.__class__.__name__
    y_pred = gnb.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print (gnb_name + ' \'test\' accuracy:', accuracy)
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)
    sgd = SGDClassifier(random_state=0, max_iter=1000, tol=0.001)
    sgd_name = sgd.__class__.__name__
    sgd.fit(X_train_std, y_train)
    y_pred = sgd.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (sgd_name + ' \'test\' accuracy:', accuracy)
    svm = SVC(gamma='auto').fit(X_train_std, y_train)
    svm_name = svm.__class__.__name__
    y_pred = svm.predict(X_test_std)
    accuracy = accuracy_score(y_test, y_pred)
    print (svm_name + ' \'test\' accuracy:', accuracy, br)
    indx = find_misses(y_test, y_pred)
    print ('total misclassifications (' + str(svm_name) +\ '):', len(indx), br)
    print ('pred', 'actual')
    misses = [(y_pred[row], y_test[row], i)
              for i, row in enumerate(indx)]
    [print (row[0], '  ', row[1]) for row in misses]
    img_indx = misses[0][2]
    img_pred = misses[0][0]
    img_act = misses[0][1]
    text = str(img_pred)
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(1)
    ax = plt.axes()
    sns.heatmap(cm.T, annot=True, fmt="d",
                cmap='gist_ncar_r', ax=ax)
    title = svm_name + ' confusion matrix'
    ax.set_title(title)
    plt.xlabel('true value')
    plt.ylabel('predicted value')
    test_images = X_test.reshape(-1, 8, 8)
    plt.figure(2)
    plt.title('1st misclassifcation')
    plt.imshow(test_images[img_indx], cmap="gray", interpolation="gaussian")
    plt.text(0, 0.05, text, color="r", bbox=dict(facecolor='white'))
    plt.show()

Listing 2-2Classify load_digits data

```

在执行清单 [2-2](#PC3) 中的代码后，您的输出应该如下所示:

```py
GaussianNB 'test' accuracy: 0.8333333333333334
SGDClassifier 'test' accuracy: 0.9377777777777778
SVC 'test' accuracy: 0.9822222222222222

total misclassifications (SVC): 8

pred actual
7    2
1    8
7    9
9    5
4    7
4    3
2    8
4    1
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        37
           1       0.98      0.98      0.98        43
           2       0.98      0.98      0.98        44
           3       1.00      0.98      0.99        45
           4       0.93      1.00      0.96        38
           5       1.00      0.98      0.99        48
           6       1.00      1.00      1.00        52
           7       0.96      0.98      0.97        48
           8       1.00      0.96      0.98        48
           9       0.98      0.98      0.98        47

   micro avg       0.98      0.98      0.98       450
   macro avg       0.98      0.98      0.98       450
weighted avg       0.98      0.98      0.98       450

```

列表 [2-2](#PC3) 还显示了数字 [2-1](#Fig1) 和 [2-2](#Fig2) 。图 [2-1](#Fig1) 显示了最佳执行算法的混淆矩阵，即 svm。您会看到显示的 *SVC* ，因为我们正在实现 svm 算法的 SVC 实现。SVC 实现利用 C-支持向量分类，其被表示为 svm。Scikit 中的 SVC。图 [2-2](#Fig2) 显示了预测集中的第一个误分类，即数字 2 误分类为数字 7。如果我们查看混淆矩阵，我们可以在数字 7 的预测值行和数字 2 的真值列的交叉点上看到这种错误分类。因此，真实值(数字 2)被错误地预测(或错误分类)为数字 7。

![../images/481580_1_En_2_Chapter/481580_1_En_2_Fig2_HTML.jpg](../images/481580_1_En_2_Chapter/481580_1_En_2_Fig2_HTML.jpg)

图 2-2

预测集中的第一个错误分类

![../images/481580_1_En_2_Chapter/481580_1_En_2_Fig1_HTML.jpg](../images/481580_1_En_2_Chapter/481580_1_En_2_Fig1_HTML.jpg)

图 2-1

支持向量机的混淆矩阵。SVC 算法

代码示例首先导入 GaussianNB、confusion_matrix 和 classification_report 以及其他必需的包。高斯神经网络是一种优秀的基线算法，因为它速度快，在许多分类问题上表现良好，并且需要调整的超参数很少。

### 小费

如果您没有使用分类数据集的经验，GaussianNB 是一个很好的起点，因为它简单、快速、易于理解，并且需要调整的超参数很少。

函数 find _ misses 返回一个分类错误的数字列表。主块加载数据，将其分成训练测试子集，并使用 GaussianNB、SGDClassifier 和 svm 进行训练。

高斯神经网络是一种概率分类器，基于应用贝叶斯定理，具有特征之间的强独立性假设。SGDClassifier 是一个分类器，它实现了一个简单的随机梯度下降学习例程，支持不同的损失函数和分类惩罚。支持向量机(svm)建立一个模型，将新的样本分配到一个类别或另一个类别。

然后代码显示所有三个模型的测试精度。自从 svm。SVC 得分最高，我们用它来识别错误分类。然后显示错误分类总数。代码继续显示每个错误分类是如何呈现的。所以，第一个分类错误的数字是 2，它被误分类为 7。

接下来，代码创建一个分类报告(用于 svm。SVC 算法),它为每个数字提供了精确度、召回率和 f1_score 分数。准确性是报告分数的一个很好的方法，但是 f1_score 是一个值得考虑的方法，因为它是最保守的。

代码以显示一个 svm 结束。SVC 混淆矩阵和第一个误分类，其中 2 被误分类为 7。该图也很有趣，因为它显示了数字 2 的实际图像，以及它被分类为红色的数字 7 的方式。

一旦为一个数据集确定了一个很好的执行算法，我强烈推荐创建一个混淆矩阵可视化。不仅很容易理解算法对目标的分类有多好，它还允许更深入地检查算法在哪里没有按预期执行。

### 小费

创建混淆矩阵可视化是了解算法执行情况的一个很好的方式。

虽然我们在前面的例子中从 svm 获得了高准确度的分数，但是 Scikit-Learn 允许我们非常容易地替换分类器。因此，下一个代码示例有点疯狂，用六个额外的分类器训练葡萄酒数据。请记住，数据集是小而简单的。对于更大和更复杂的数据，替换分类器在计算上是昂贵的。

清单 [2-3](#PC5) 中显示的下一个代码示例使用几个 Scikit-Learn 算法对葡萄酒数据进行分类，以识别有希望提高性能的数据。

```py
import humanfriendly as hf
import time
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression,\
     LogisticRegressionCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,\
     ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score

def get_scores(model, Xtest, ytest, avg):
    y_pred = model.predict(Xtest)
    accuracy = accuracy_score(ytest, y_pred)
    f1 = f1_score(ytest, y_pred, average=avg)
    return (accuracy, f1)

def get_time(time):
    return hf.format_timespan(time, detailed=True)

if __name__ == "__main__":
    br = '\n'
    digits = load_digits()
    X = digits.data
    y = digits.target
    X_train, X_test, y_train, y_test = train_test_split\
                                       (X, y, random_state=0)
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)

    lr = LogisticRegression(random_state=0, solver="lbfgs",
                            multi_class='auto', max_iter=4000)
    lr.fit(X_train_std, y_train)
    lr_name = lr.__class__.__name__
    acc, f1 = get_scores(lr, X_test_std, y_test, 'micro')
    print (lr_name + ' scaled \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1, br)
    softmax = LogisticRegression(multi_class="multinomial",
                                 solver="lbfgs", max_iter=4000,
                                 C=10, random_state=0)
    softmax.fit(X_train_std, y_train)
    acc, f1 = get_scores(softmax, X_test_std, y_test, 'micro')
    print (lr_name + ' (softmax) scaled \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1, br)
    rf = RandomForestClassifier(random_state=0, n_estimators=100)
    rf.fit(X_train_std, y_train)
    rf_name = rf.__class__.__name__
    acc, f1 = get_scores(rf, X_test_std, y_test, 'micro')
    print (rf_name + ' \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1, br)
    et = ExtraTreesClassifier(random_state=0, n_estimators=100)
    et.fit(X_train, y_train)
    et_name = et.__class__.__name__

    acc, f1 = get_scores(et, X_test, y_test, 'micro')
    print (et_name + ' \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1, br)
    gboost_clf = GradientBoostingClassifier(random_state=0)
    gb_name = gboost_clf.__class__.__name__
    gboost_clf.fit(X_train, y_train)
    acc, f1 = get_scores(gboost_clf, X_test, y_test, 'micro')
    print (gb_name + ' \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1, br)
    knn_clf = KNeighborsClassifier().fit(X_train, y_train)
    knn_name = knn_clf.__class__.__name__
    acc, f1 = get_scores(knn_clf, X_test, y_test, 'micro')
    print (knn_name + ' \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1, br)
    start = time.perf_counter()
    lr_cv = LogisticRegressionCV(random_state=0, cv=5, multi_class="auto", max_iter=4000)
    lr_cv_name = lr_cv.__class__.__name__
    lr_cv.fit(X, y)
    end = time.perf_counter()
    elapsed_ls = end - start
    timer = get_time(elapsed_ls)
    print (lr_cv_name + ' timer:', timer)
    acc, f1 = get_scores(lr_cv, X_test, y_test, 'micro')
    print (lr_cv_name + ' \'test\':')
    print ('accuracy:', acc, ', f1_score:', f1)

Listing 2-3Classifying load_digits with various algorithms

```

在执行清单 [2-3](#PC5) 中的代码后，您的输出应该如下所示:

```py
LogisticRegression scaled 'test':
accuracy: 0.9733333333333334, f1_score: 0.9733333333333334

LogisticRegression (softmax) scaled 'test':
accuracy: 0.9644444444444444, f1_score: 0.9644444444444444

RandomForestClassifier 'test':
accuracy: 0.9755555555555555, f1_score: 0.9755555555555555

ExtraTreesClassifier 'test':
accuracy: 0.9822222222222222, f1_score: 0.9822222222222222

GradientBoostingClassifier 'test':
accuracy: 0.9622222222222222, f1_score: 0.9622222222222222

KNeighborsClassifier 'test':
accuracy: 0.98, f1_score: 0.98

LogisticRegressionCV timer: 49 seconds and 38.45 milliseconds
LogisticRegressionCV 'test':
accuracy: 0.9822222222222222, f1_score: 0.9822222222222222

```

代码从导入 humanfriendly、time、LogisticRegression、LogisticRegressionCV、KNeighborsClassifier、GradientBoostingClassifier 和 ExtraTreesClassifier 以及其他必需的包开始。函数 get_scores 返回精度和 f1_score。函数 get_time 返回经过的时间。它有助于发现一个算法训练一个数据集需要多长时间。

LogisticRegression 是一种传统上仅限于两类分类问题的分类算法。Softmax(多项逻辑回归)分类使用逻辑回归进行多类分类。RandomForestClassifier 是一种集成学习方法，它在训练时构建大量决策树，并输出作为类模式的类。ExtraTreesClassifier 实现了一个元估计器，该估计器在数据集的各种子样本上拟合多个随机决策树(或额外的树),并使用平均来提高预测精度和控制过度拟合。GradientBoostingClassifier 以弱预测模型(通常为决策树)的形式生成预测模型。KNeighborsClassifier 实现了 k-nearest neighbors 投票，其中输入由特征空间中的 *k* 个最近的训练示例组成。*特征空间*指的是你的特征存在的 n 维空间。LogisticRegression 使用逻辑函数对数据进行建模。LogisticRegressionCV 使用逻辑回归实现交叉验证估计。

交叉验证(CV)将数据分成 *n* 个子集，并迭代 *n* 次。通过每次迭代， *n 个*子集中的一个被拿出来作为测试集，而其余的用于训练。每次迭代使用不同的子集。因此，精确度和误差在所有的 *n 次*试验中被平均。结果精度非常好，但是 CV 的计算代价很高。

GradientBoostingClassifier 和 ExtraTreesClassifier 是与 RandomForestClassifier 相似的集成方法，它们根据数据和平均结果拟合(或训练)许多决策树，以提高预测准确性。

### 小费

您可能需要安装这个人性化的软件包，因为 Anaconda 不会自动安装它。打开一个新的 Anaconda 提示符，如清单 [2-4](#PC7) 所示进行安装。

```py
pip install humanfriendly

Listing 2-4Install a new package

```

主程序块从加载数据并将其分成训练测试子集开始。每种算法训练数据并显示分数。请注意，LogisticRegressionCV 消耗了超过 46 秒的时间。尽管所有算法的表现都令人钦佩，但我们仍然无法达到 98.22%的准确率。

### 银行数据分类

清单 [2-5](#PC8) 中显示的第一个代码示例从一个 CSV 文件中加载银行数据。接下来，*教育*功能被设计得更像样。

*特征工程*正在创建使机器学习算法工作的特征(基于数据的领域知识)。虽然特征工程是机器学习应用的基础，但它既困难又昂贵。

然后，代码将分类特征转换为数字特征，以便进行算法训练。这种转换通常被称为编码。

### 小费

机器学习算法只对数字数据进行运算。

最后，将显示五个最重要的要素以及数据要素和类计数。功能集和目标保存在 NumPy 文件中。

```py
import numpy as np, pandas as pd
from sklearn.ensemble import RandomForestClassifier

if __name__ == "__main__":

    br = '\n'
    f = 'data/bank.csv'
    data = pd.read_csv(f)
    print ('original "education" categories:')
    print (data.education.unique(), br)
    data['education'] = np.where(data['education'] == 'basic.9y',
                                 'basic', data['education'])
    data['education'] = np.where(data['education'] == 'basic.6y',
                                 'basic', data['education'])
    data['education'] = np.where(data['education'] == 'basic.4y',
                                 'basic', data['education'])
    data['education'] = np.where(data['education'] == 'high.school', 'high_school', data.education)
    data['education'] = np.where(data['education'] == 'professional.course', 'professional', data['education'])
    data['education'] = np.where(data['education'] == 'university.degree', 'university', data['education'])
    print ('engineered "education" categories:')
    print (data.education.unique(), br)
    print ('target value counts:')
    print (data.y.value_counts(), br)
    data_X = data.loc[:, data.columns != 'y']
    cat_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']
    data_new = pd.get_dummies(data_X, columns=cat_vars)
    X = data_new.values
    y = data.y.values
    attributes = list(data_X)
    rf = RandomForestClassifier(random_state=0, n_estimators=100)
    rf.fit(X, y)
    rf_name = rf.__class__.__name__
    feature_importances = rf.feature_importances_
    importance = sorted(zip(feature_importances, attributes), reverse=True)
    n = 5
    print (n, 'most important features' + ' (' + rf_name + '):')
    [print (row) for i, row in enumerate(importance) if i < n]
    print ()
    features_file = 'data/features'
    np.save(features_file, attributes)
    features = np.load('data/features.npy')
    print ('features:')
    print (features, br)
    y_file = 'data/y'
    X_file = 'data/X'
    np.save(y_file, y)
    np.save(X_file, X)
    d = {}
    dvc = data.y.value_counts()
    d['no'], d['yes'] = dvc['no'], dvc['yes']
    dvc_file = 'data/value_counts'
    np.save(dvc_file, d)
    d = np.load('data/value_counts.npy')
    print ('class counts:', d)

Listing 2-5Engineering and wrangling bank data

```

在执行清单 [2-5](#PC8) 中的代码后，您的输出应该如下所示:

```py
original "education" categories:
['basic.4y' 'high.school' 'basic.6y' 'basic.9y' 'professional.course'
 'unknown' 'university.degree' 'illiterate']

engineered "education" categories:
['basic' 'high_school' 'professional' 'unknown' 'university' 'illiterate']

target value counts:
no     36548
yes     4640
Name: y, dtype: int64

5 most important features (RandomForestClassifier):
(0.28697175347986037, 'job')
(0.08761238456151103, 'month')
(0.0797624194551633, 'age')
(0.05492109153356108, 'day_of_week')
(0.04027613029914145, 'marital')

features:
['age' 'job' 'marital' 'education' 'default' 'housing' 'loan' 'contact' 'month' 'day_of_week' 'duration' 'campaign' 'pdays' 'previous' 'poutcome' 'emp.var.rate' 'cons.price.idx' 'cons.conf.idx' 'euribor3m' 'nr.employed']

class counts: {'no': 36548, 'yes': 4640}

```

该代码示例从导入必备包开始。主程序块读取数据并显示来自*教育*功能的原始值。代码继续对特征进行特征工程，并显示新值。请注意，只对单个特征进行特征工程是多么困难。接下来，分类特征由熊猫 *get_dummies* 函数编码为一个热编码(OHE)向量。Scikit-Learn 希望特征数据是数字，这就是为什么我们需要对它们进行编码。

*OHE 向量*也被称为虚拟变量。OHE 是一个很好的选择，因为它是机器学习中处理分类数据最常用的方法之一。OHE 获取每个类别值，并将其转换为大小为 I 的二进制向量(其中 I 是类别 I 中值的数量)，并使除类别列之外的所有列都等于零。例如，在我们的数据集中，婚姻状况是“已婚”、“单身”或“离婚”。如果有人结婚了，OHE 编码一个[1 0 0]向量。如果是 single，OHE 编码一个[0 1 0]向量。最后，如果离婚，OHE 编码一个[0 0 1]向量。简单地说，1 位是热的，表示适合数据元素的类别。

然后，代码根据转换后的数据集创建特征集 X 和目标集 y。在 RandomForestClassifier 的帮助下显示特征重要性。接下来，X 和 y 被保存在 NumPy 文件中。最后，创建、保存和显示类计数。查看类计数有助于了解目标之间的平衡。

请注意，我们有更多的*否*值，而不是*是*值。所以，数据集有点不平衡。这种情况通常被称为不平衡类分布，即属于一个类的观测值数量明显低于属于其他类的观测值数量。在我们的例子中，是和不是的比例大约是 12.6%。所以，我们没有大问题。低于 5%的比率(或事件率)是一个问题，因为当这种情况发生时，机器学习算法可能产生不令人满意的分类。

现在银行数据已经准备好了，我们可以运行实验来识别高性能的分类算法，如下面的代码示例所示，如清单 [2-6](#PC10) 所示。记住，许多小时的实验导致了这个例子中算法的选择。

```py
import numpy as np, pandas as pd, random
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier,\
     ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

def get_scores(model, xtrain, ytrain, xtest, ytest, scoring):
    ypred = model.predict(xtest)
    train = model.score(xtrain, ytrain)
    test = model.score(xtest, y_test)
    f1 = f1_score(ytest, ypred, average=scoring)
    return (train, test, f1)

def prep_data(data, target):
    d = [data[i] for i, _ in enumerate(data)]
    t = [target[i] for i, _ in enumerate(target)]
    return list(zip(d, t))

def create_sample(d, n, replace="yes"):
    if replace == 'yes': s = random.sample(d, n)
    else: s = [random.choice(d) for i, _ in enumerate(d) if i < n]
    Xs = [row[0] for i, row in enumerate(s)]
    ys = [row[1] for i, row in enumerate(s)]
    return np.array(Xs), np.array(ys)

if __name__ == "__main__":

    br = '\n'
    X = np.load('data/X.npy')
    y = np.load('data/y.npy')
    print ('full data set shape for X and y:')
    print (X.shape, y.shape, br)
    X_train, X_test, y_train, y_test = train_test_split\
                                       (X, y, random_state=0)
    et = ExtraTreesClassifier(random_state=0, n_estimators=100)
    et.fit(X_train, y_train)
    et_scores = get_scores(et, X_train, y_train, X_test, y_test, 'micro')
    print (et.__class__.__name__ + '(train, test, f1_score):')
    print (et_scores, br)
    rf = RandomForestClassifier(random_state=0, n_estimators=100)
    rf.fit(X_train, y_train)
    rf_scores = get_scores(rf, X_train, y_train, X_test, y_test, 'micro')
    print (rf.__class__.__name__ + '(train, test, f1_score):')
    print (rf_scores, br)
    sample_size = 4000
    data = prep_data(X, y)
    Xs, ys = create_sample(data, sample_size, replace="no")
    print ('sample data set shape for X and y:')
    print (Xs.shape, ys.shape, br)
    X_train, X_test, y_train, y_test = train_test_split\
                                       (Xs, ys, random_state=0)
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train),\
                              scaler.transform(X_test)
    knn = KNeighborsClassifier().fit(X_train, y_train)
    knn_scores = get_scores(knn, X_train, y_train, X_test, y_test, 'micro')
    print (knn.__class__.__name__ + '(train, test, f1_score):')
    print (knn_scores, br)
    svm = SVC(random_state=0, gamma="scale")
    svm.fit(X_train_std, y_train)
    svm_scores = get_scores(svm, X_train_std, y_train, X_test_std, y_test, 'micro')
    print (svm.__class__.__name__ + '(train, test, f1_score):')
    print (svm_scores, br)
    knn_name, svm_name = knn.__class__.__name__,\
                         svm.__class__.__name__

    y_pred_knn = knn.predict(X_test)
    cm_knn = confusion_matrix(y_test, y_pred_knn)
    cm_knn_T = cm_knn.T
    y_pred_svm = svm.predict(X_test_std)
    cm_svm = confusion_matrix(y_test, y_pred_svm)
    cm_svm_T = cm_svm.T
    plt.figure(knn.__class__.__name__)
    ax = plt.axes()
    sns.heatmap(cm_knn_T, annot=True, fmt="d", cmap="gist_ncar_r", cbar=False)
    ax.set_title(str(knn_name) + ' confusion matrix')
    plt.xlabel('true label')
    plt.ylabel('predicted label')
    plt.figure(str(svm_name) + ' confusion matrix' )
    ax = plt.axes()
    sns.heatmap(cm_svm_T, annot=True, fmt="d", cmap="gist_ncar_r", cbar=False)
    ax.set_title(svm_name)
    plt.xlabel('true label')
    plt.ylabel('predicted label')
    cnt_no, cnt_yes = 0, 0
    for i, row in enumerate(y_test):
        if row == 'no': cnt_no += 1
        elif row == 'yes': cnt_yes += 1
    cnt_no, cnt_yes = str(cnt_no), str(cnt_yes)
    print ('true =>', 'no: ' + cnt_no + ', yes: ' + cnt_yes, br)
    p_no, p_nox = cm_knn_T[0][0], cm_knn_T[0][1]
    p_yes, p_yesx = cm_knn_T[1][1], cm_knn_T[1][0]
    print ('knn classification report:')
    print ('predict \'no\':', p_no, '(' +\str(p_nox) + ' misclassifed)')
    print ('predict \'yes\':', p_yes, '(' +\str(p_yesx) + ' misclassifed)', br)
    p_no, p_nox = cm_svm_T[0][0], cm_svm_T[0][1]
    p_yes, p_yesx = cm_svm_T[1][1], cm_svm_T[1][0]
    print ('svm classification report:')
    print ('predict \'no\':', p_no, '(' +\str(p_nox) + ' misclassifed)')
    print ('predict \'yes\':', p_yes, '(' +\str(p_yesx) + ' misclassifed)')

    plt.show()

Listing 2-6Classifying bank data

```

在执行清单 [2-6](#PC10) 中的代码后，您的输出应该如下所示:

```py
full data set shape for X and y:
(41188, 61) (41188,)

ExtraTreesClassifier(train, test, f1_score):
(1.0, 0.9009420219481402, 0.9009420219481401)

RandomForestClassifier(train, test, f1_score):
(0.9999676281117478, 0.9121103233951636, 0.9121103233951636)

sample data set shape for X and y:
(4000, 61) (4000,)

KNeighborsClassifier(train, test, f1_score):
(0.9323333333333333, 0.916, 0.916)

SVC(train, test, f1_score):
(0.9376666666666666, 0.92, 0.92)

true => no: 902, yes: 98

knn classification report:
predict 'no': 869 (51 misclassifed)
predict 'yes': 47 (33 misclassifed)

svm classification report:
predict 'no': 883 (61 misclassifed)
predict 'yes': 37 (19 misclassifed)

```

列表 [2-6](#PC10) 还显示了数字 [2-3](#Fig3) 和 [2-4](#Fig4) 。图 [2-3](#Fig3) 显示 KNeighborsClassifier 的混淆矩阵，图 [2-4](#Fig4) 显示 svm.SVC 的混淆矩阵。

![../images/481580_1_En_2_Chapter/481580_1_En_2_Fig4_HTML.jpg](../images/481580_1_En_2_Chapter/481580_1_En_2_Fig4_HTML.jpg)

图 2-4

svm。SVC 混淆矩阵

![../images/481580_1_En_2_Chapter/481580_1_En_2_Fig3_HTML.jpg](../images/481580_1_En_2_Chapter/481580_1_En_2_Fig3_HTML.jpg)

图 2-3

近邻分类器混淆矩阵

代码从导入必需的包开始。函数 get_scores 返回训练和测试准确度分数。函数 prep_data 将 NumPy 矩阵转换为向量列表，以便更容易地操作数据元素进行采样。函数 create_sample 构建一个随机样本，并将其作为 X 和 y NumPy 矩阵返回。

Scikit-Learn 算法只能训练表示为 NumPy 的数据。主块从上一个示例中创建的 NumPy 文件中加载 X 和 y。x 和 y 被分成训练测试子集。然后，代码使用 ExtraTreesClassifier 和 RandomForestClassifier 对数据进行定型。抽取 4000 个样本，以便我们可以使用 KNeighborsClassifier 和 svm.SVC 高效地进行训练。这两种算法是优秀的分类器，但是对于大数据集来说计算开销很大。

近邻分类器和支持向量机的混淆矩阵。然后显示 SVC，因为它们更适合数据。也就是说，这些模型的精确度更高，过度拟合更少。代码通过计算 KNeighborsClassifier 和 svm 的数据和错误分类的目标值的平衡得出结论。

值得注意的是 KNeighborsClassifier 和 svm。基于少于原始数据的 10% 的样本*，SVC 比其他算法执行得更好。这其实是很可观的！*

UCI 机器学习库包括从银行数据中随机选择的样本，其中有 10%的例子。为了完整起见，清单 [2-7](#PC12) 中显示的下一个例子测试了这个样本的准确性。

```py
import pandas as pd, numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier,\
     ExtraTreesClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score

def get_scores(model, xtrain, ytrain, xtest, ytest, scoring):
    ypred = model.predict(xtest)
    train = model.score(xtrain, ytrain)
    test = model.score(xtest, y_test)
    f1 = f1_score(ytest, ypred, average=scoring)
    return (train, test, f1)

if __name__ == "__main__":
    br = '\n'
    f = 'data/bank_sample.csv'
    data = pd.read_csv(f)
    print ('data shape:', data.shape, br)
    data['education'] =\
                      np.where(data['education'] == 'basic.9y',
                               'basic', data['education'])
    data['education'] = np.where(data['education'] == 'basic.6y',
                                 'basic', data['education'])
    data['education'] = np.where(data['education'] == 'basic.4y',
                                 'basic', data['education'])
    data['education'] = np.where(data['education'] == 'high.school', 'high_school', data.education)
    data['education'] = np.where(data['education'] == 'professional.course', 'professional', data['education'])
    data['education'] = np.where(data['education'] == 'university.degree', 'university', data['education'])
    data_X = data.loc[:, data.columns != 'y']
    cat_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']
    data_new = pd.get_dummies(data_X, columns=cat_vars)
    attributes = list(data_X)
    y = data.y.values

    X = data_new.values
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    rf = RandomForestClassifier(random_state=0, n_estimators=100)
    rf.fit(X_train, y_train)
    rf_name = rf.__class__.__name__
    rf_scores = get_scores(rf, X_train, y_train, X_test, y_test, 'micro')
    print (rf.__class__.__name__ + '(train, test, f1_score):')
    print (rf_scores, br)
    et = ExtraTreesClassifier(random_state=0, n_estimators=100)
    et.fit(X_train, y_train)
    et_name = et.__class__.__name__
    et_scores = get_scores(et, X_train, y_train, X_test, y_test, 'micro')
    print (et.__class__.__name__ + '(train, test, f1_score):')
    print (et_scores, br)
    scaler = StandardScaler().fit(X_train)
    X_train_std, X_test_std = scaler.transform(X_train),\
                              scaler.transform(X_test)
    knn = KNeighborsClassifier().fit(X_train, y_train)
    knn_scores = get_scores(knn, X_train, y_train, X_test, y_test, 'micro')
    print (knn.__class__.__name__ + '(train, test, f1_score):')
    print (knn_scores, br)
    svm = SVC(random_state=0, gamma="scale")
    svm.fit(X_train_std, y_train)
    svm_scores = get_scores(svm, X_train_std, y_train, X_test_std, y_test, 'micro')
    print (svm.__class__.__name__ + '(train, test, f1_score):')
    print (svm_scores)

Listing 2-7Classifying UCI Irvine sample bank data

```

在执行清单 [2-7](#PC12) 中的代码后，您的输出应该如下所示:

```py
data shape: (4119, 21)

RandomForestClassifier(train, test, f1_score):
(1.0, 0.9058252427184466, 0.9058252427184466)

ExtraTreesClassifier(train, test, f1_score):
(1.0, 0.8990291262135922, 0.8990291262135922)

KNeighborsClassifier(train, test, f1_score):
(0.9323405632890903, 0.8883495145631068, 0.8883495145631068)

SVC(train, test, f1_score):
(0.9494982194885077, 0.9, 0.9)

```

代码从导入必需的包开始。函数 get_scores 返回准确度分数。主程序块加载样本，设计*教育*特征，并将分类特征编码成 OHE 形式。我们不得不为这个例子添加工程师*教育*的特征，因为我们没有从我们已经设计的特征的完整数据集中抽取样本。

代码继续将 NumPy 数据加载到 X 和 y 中，将其分成训练测试子集，并用 RandomForestClassifier 进行训练。然后显示精确度。其余代码使用 ExtraTreesClassifier、KNeighborsClassifier 和 svm 进行训练。SVC 并显示准确度分数。

我们创建的样本的性能至少与来自 UCI 资源库的样本一样好。我们的样本甚至更小，这意味着我们的取样技术已经足够了。

### 给月亮分类

Scikit-Learn make_moons 数据主要用于可视化聚类和分类算法。然而，它也是一个很好的数据集，可以帮助您了解分类算法如何尝试分离二进制目标标签(或二进制分类)。make_moons 的部署描述了 2D 空间中的两个交错圆以及相关联的数据点。

通过可视化，我们可以很容易地看到两个(或二元)标签之间的分离。如果人眼可以在 2D 空间中轻松区分这种分离，分类算法应该也能做到。我们可以用一个例子来验证这一点。

清单 [2-8](#PC14) 中显示的第一个代码示例创建了一个包含 1000 个元素的数据集，将特征数据及其关联的目标放入 Pandas DataFrame 中，并绘制出结果。每个特征元素表示用于在 2D 空间中绘图的 x 和 y 坐标。每个目标代表要素的标签，它是 0 或 1 的二进制值。

```py
import matplotlib.pyplot as plt, pandas as pd
from sklearn import datasets

if __name__ == "__main__":
    br = '\n'
    X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=0)
    df = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))
    colors = {0:'magenta', 1:'cyan'}
    fig, ax = plt.subplots()
    data = df.groupby('label')
    for key, label in data:
        label.plot(ax=ax, kind="scatter", x="x", y="y", label=key, color=colors[key])
    plt.show()

Listing 2-8Plot make_moons

```

在执行清单 [2-8](#PC14) 中的代码后，您的输出应该类似于图 [2-5](#Fig5) 中所示的可视化结果:

![../images/481580_1_En_2_Chapter/481580_1_En_2_Fig5_HTML.jpg](../images/481580_1_En_2_Chapter/481580_1_En_2_Fig5_HTML.jpg)

图 2-5

随机生成的卫星数据的可视化

清单 [2-9](#PC15) 中显示的下一个代码示例创建了一个包含 1000 个元素的 make_moons 数据集，将其分割成训练测试子集，并使用 svm 进行训练。SVC 和 KNeighborsClassifier。我特意选择了这两种算法，因为我知道它们会在二进制分类方面做得很好，因为它们会查看每个数据点。

```py
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def get_scores(model, Xtrain, Xtest, ytrain, ytest):
    y_ptrain = model.predict(Xtrain)
    y_ptest = model.predict(Xtest)
    acc_train = accuracy_score(ytrain, y_ptrain)
    acc_test = accuracy_score(ytest, y_ptest)
    name = model.__class__.__name__
    return (name, acc_train, acc_test)

if __name__ == "__main__":
    br = '\n'
    X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
    knn = KNeighborsClassifier().fit(X_train, y_train)
    accuracy = get_scores(knn, X_train, X_test, y_train, y_test)
    print ('<<' + str(accuracy[0]) + '>>')
    print ('train:', accuracy[1], 'test:', accuracy[2], br)
    svm = svm.SVC(gamma='scale', random_state=0)
    svm.fit(X_train, y_train)
    accuracy = get_scores(svm, X_train, X_test, y_train, y_test)
    print ('<<' + str(accuracy[0]) + '>>')
    print ('train:', accuracy[1], 'test:', accuracy[2])

Listing 2-9Classify make_moons

```

在执行清单 [2-9](#PC15) 中的代码后，您的输出应该如下所示:

```py
<<KNeighborsClassifier>>
train: 0.9666666666666667 test: 0.964

<<SVC>>
train: 0.9653333333333334 test: 0.96

```

该代码示例从导入必备包开始。函数 get_scores 返回模型名称以及训练和测试准确度分数。主程序块首先加载样本数据，并将其分成训练测试子集。它继续用 KNeighborsClassifier 和 svm 训练数据。SVC 和报告准确性分数。正如所料，两种算法都非常准确地识别了标签，基本上没有过度拟合。

清单 [2-10](#PC17) 中显示的最终代码示例通过将数据分成训练、测试和验证子集来扩展我们的知识。KNeighborsClassifier 用于训练和启用报告。

```py
from sklearn.datasets import make_moons
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def get_scores(model, Xtrain, ytrain, Xtest, ytest, Xvalid, yvalid):
    y_ptrain = model.predict(Xtrain)
    y_ptest = model.predict(Xtest)
    y_pvalid = model.predict(Xvalid)
    acc_train = accuracy_score(ytrain, y_ptrain)
    acc_test = accuracy_score(ytest, y_ptest)
    acc_valid = accuracy_score(yvalid, y_pvalid)
    name = model.__class__.__name__
    return (name, acc_train, acc_test, acc_valid)

if __name__ == "__main__":
    br = '\n'
    X_train, y_train = make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=0)
    X_test, y_test = make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=0)
    X_valid, y_valid = make_moons(n_samples=10000, shuffle=True, noise=0.2, random_state=0)
    knn = KNeighborsClassifier().fit(X_train, y_train)
    accuracy = get_scores(knn, X_train, y_train, X_test, y_test, X_valid, y_valid)
    print ('train test valid split (technique 1):')
    print ('<<' + str(accuracy[0]) + '>>')
    print ('train:', accuracy[1], 'test:', accuracy[2], 'valid:', accuracy[3])
    print ('sample split:', X_train.shape, X_test.shape, X_valid.shape)
    print ()

    X, y = make_moons(n_samples=1000, shuffle=True, noise=0.2, random_state=0)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)
    knn = KNeighborsClassifier().fit(X_train, y_train)
    accuracy = get_scores(knn, X_train, y_train, X_test, y_test, X_valid, y_valid)
    print ('train test valid split (technique 2):')
    print ('<<' + str(accuracy[0]) + '>>')
    print ('train:', accuracy[1], 'test:', accuracy[2], 'valid:', accuracy[3])
    print ('sample split:', X_train.shape, X_test.shape, X_val.shape)

Listing 2-10Classify make_moons on train, validate, and test subsets

```

在执行清单 [2-10](#PC17) 中的代码后，您的输出应该如下所示:

```py
train test valid split (technique 1):
<<KNeighborsClassifier>>
train: 0.969 test: 0.969 valid: 0.9688
sample split: (1000, 2) (1000, 2) (10000, 2)

train test valid split (technique 2):
<<KNeighborsClassifier>>
train: 0.9616666666666667 test: 0.975 valid: 0.9694
sample split: (600, 2) (200, 2) (200, 2)

```

代码开始导入必需的包。函数 get_scores 被扩展以说明验证分数。主模块首先创建三个独立的测试、训练和验证子集。使用这种技术，我们创建了三个相同大小的数据集。虽然这种技术产生了很好的结果，但是随着数据集变得越来越大，它的计算开销也越来越大。实际上，这种技术要贵三倍，因为要创建和训练三个数据集。KNeighborsClassifier 用于训练、验证和测试。第二种技术非常常见，因为它将一个数据集分为训练、验证和测试。再次使用 KNeighborsClassifier。这两种技术的结果不相上下，并且和预期的一样优秀。

### 小费

只有当模型在训练和验证阶段完成训练后，才能使用测试数据，这样它才能对最终模型与训练数据的拟合程度进行无偏见的评估。

在行业中，机器学习工程师通过在训练之前将数据分成训练、测试和验证子集来试验数据问题。训练数据用于拟合(或训练)模型。模型从训练数据中观察和学习。

验证数据用于评估模型。机器学习工程师使用验证数据来微调模型的超参数。测试数据根据从拟合训练数据和用验证数据调整超参数中学到的知识，提供最终模型拟合的无偏评估。