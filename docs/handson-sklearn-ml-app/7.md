# 7.sci kit-学习回归调优

回归预测建模(或简称为*回归*)就是学习自变量(或特征)和*连续*因变量(或结果)之间关联强度的问题。优化回归算法类似于优化分类算法。也就是说，我们调整模型的超参数，直到我们达到最优解。

不同之处在于，回归调优的目标是降低均方根误差(RMSE)，而分类调优的目标是最大化精度。 *RMSE* 的一个好处是误差分数的单位与预测值相同。虽然回归预测可以使用 RMSE 进行评估，但分类预测却不能。

### 小费

回归调优的目标是最小化 RMSE。

为我们的调整示例选择的机器学习算法不是巧合。我选择它们是基于许多小时的实验、阅读和洞察力。对于给定的数据集，表现最好的算法被包括在内，表现差的算法则不包括在内。

对于本章中的回归实验，我们利用 GridSearchCV 进行调优。

### 小费

在给定足够的计算资源的情况下，使用 GridSearchCV 进行调优适合于彻底搜索性能最佳的超参数。使用 RandomizedSearchCV 进行调优适用于良好的搜索，或者如果调优高维数据。

学习调整回归算法可以通过使用各种数据集和回归变量的例子来加速。但是，我也建议遵循结构化流程:

1.  总是从使用基线算法的默认超参数开始。

2.  尝试训练和测试规模。

3.  处理高维数据时使用降维。

4.  处理大型数据集时，随机抽取样本。

5.  缩放数据(在适当的情况下)以潜在地提高性能。

6.  使用 GridSearchCV 或 RandomizedSearchCV 进行调整。

7.  一旦使用基线算法进行了调整，就可以使用复杂的算法进行实验。

### 小费

从基线算法(及其默认的超参数)开始调优，以建立基线性能。

## 调整数据集

我们专注于四个数据集:小费、波士顿和葡萄酒(红和白)。小费数据由餐馆的服务员小费和相关因素组成，包括小费、餐费和时间。波士顿数据由波士顿不同地点的房价组成。葡萄酒数据由两个数据集(红色和白色)组成，这两个数据集由葡萄牙 Vinho Verde 葡萄酒的变体组成。

## 调谐提示

清单 [7-1](#PC1) 中所示的代码示例基于未缩放和缩放的数据计算各种回归算法的 RMSE。因为 tips 是如此小的数据集，所以运行这种类型的实验在计算上是廉价的。

```
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor as rfr,\
     AdaBoostRegressor as ada, GradientBoostingRegressor as gbr
from sklearn.linear_model import LinearRegression as lr,\
     BayesianRidge as bay, Ridge as rr, Lasso as l,\
     LassoLars as ll, ElasticNet as en,\
     ARDRegression as ard, RidgeCV as rcv
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor as dtr
from sklearn.neighbors import KNeighborsRegressor as knn
from sklearn.preprocessing import StandardScaler

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_tips.npy')
    y = np.load('data/y_tips.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    regressors = [lr(), bay(), rr(alpha=.5, random_state=0),
                  l(alpha=0.1, random_state=0), ll(), knn(),
                  ard(), rfr(random_state=0, n_estimators=100),
                  SVR(gamma='scale', kernel="rbf"),
                  rcv(fit_intercept=False), en(random_state=0),
                  dtr(random_state=0), ada(random_state=0),
                  gbr(random_state=0)]
    print ('unscaled:', br)
    for reg in regressors:
        reg.fit(X_train, y_train)
        rmse, name = get_error(reg, X_test, y_test)
        name = reg.__class__.__name__
        print (name + '(rmse):', end=' ')
        print (rmse)
    print ()
    print ('scaled:', br)
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)
    for reg in regressors

:
        reg.fit(X_train_std, y_train)
        rmse, name = get_error(reg, X_test_std, y_test)
        name = reg.__class__.__name__
        print (name + '(rmse):', end=' ')
        print (rmse)

Listing 7-1Calculating RMSE for tips data with regression algorithms

```

继续执行清单 [7-1](#PC1) 中的代码。请记住，您可以从本书的示例下载中找到示例。您不需要手动键入示例。更容易访问示例下载和复制/粘贴。

执行清单 [7-1](#PC1) 的输出应该如下所示:

```
unscaled:

LinearRegression(rmse): 0.9474705746817206
BayesianRidge(rmse): 0.9245282337469829
Ridge(rmse): 0.9471900902779103
Lasso(rmse): 0.9158574785712037
LassoLars(rmse): 1.333812899498391
KNeighborsRegressor(rmse): 1.086204460049883
ARDRegression(rmse): 0.9264801346401996
RandomForestRegressor(rmse): 0.8850975551298138
SVR(rmse): 0.9441992099702836
RidgeCV(rmse): 0.9426372075893412
ElasticNet(rmse): 0.9307377813721578
DecisionTreeRegressor(rmse): 1.2994272932036561
AdaBoostRegressor(rmse): 0.932681302158466
GradientBoostingRegressor(rmse): 0.9112440690311495

scaled:

LinearRegression(rmse): 0.9007751177881488
BayesianRidge(rmse): 0.9096801291989541

Ridge(rmse): 0.9010890080377257
Lasso(rmse): 0.8785977911833892
LassoLars(rmse): 1.333812899498391
KNeighborsRegressor(rmse): 0.9613578099280607
ARDRegression(rmse): 0.8745960871430548
RandomForestRegressor(rmse): 0.893772251516372
SVR(rmse): 0.9749204385201592
RidgeCV(rmse): 3.1960055364135638
ElasticNet(rmse): 1.1310151423347359
DecisionTreeRegressor(rmse): 1.1835900827021861
AdaBoostRegressor(rmse): 0.986987944835978
GradientBoostingRegressor(rmse): 0.8908489427010696

```

代码从导入必要的包和各种回归算法开始。函数 get_error 返回型号名称和 RMSE。主程序块从从 NumPy 文件加载预处理的 tips 数据开始。请记住，我们在第 [4](4.html) 章中对 tips 数据进行了编码并保存，以备将来处理。

### 小费

Scikit-Learn 允许您试验各种算法来测试性能，而不需要它们的上下文知识。

代码继续将数据分割成训练测试子集。接下来，我们创建一个回归算法列表。代码继续在未缩放的数据上训练每个算法并显示结果。然后，代码缩放数据，根据缩放后的数据训练每个算法，并显示结果。

缩放数据是这个实验的一个非常重要的部分，因为许多算法报告的 RMSE 结果比它们未缩放的兄弟要低。对缩放数据执行最好的算法是 Lasso 和 ARDRegression。

### 小费

在调优过程中，缩放可能是一项非常重要的技术。

所以，实验成功了！它引导我们找到了两种算法，我们可以集中精力进行调优。

清单 [7-2](#PC3) 中显示的下一个代码示例用套索调整提示。

```
import numpy as np, humanfriendly as hf
import time
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV,\
     cross_val_score
from sklearn.metrics import mean_squared_error

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

def see_time(note):
    end = time.perf_counter()
    elapsed = end - start
    print (note, hf.format_timespan(elapsed, detailed=True))

def get_cross(model, data, target, groups=10):
    return cross_val_score(model, data, target, cv=groups,
                           scoring='neg_mean_squared_error')

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_tips.npy')
    y = np.load('data/y_tips.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)
    lasso = Lasso(random_state=0, alpha=0.1)
    print (lasso, br)
    lasso.fit(X_train_std, y_train)
    rmse, name = get_error(lasso, X_test_std, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    alpha_lasso = [1e-1]
    params = {'alpha': alpha_lasso, 'positive': [True, False],
              'max_iter': [10, 50, 100]}
    grid = GridSearchCV(lasso, params, cv=5, n_jobs=-1, verbose=1)
    start = time.perf_counter()
    grid.fit(X_train, y_train)
    see_time('training time:')
    bp = grid.best_params_

    print (bp, br)
    lasso = Lasso(**bp, random_state=0).fit(X_train_std, y_train)
    rmse, name = get_error(lasso, X_test_std, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    start = time.perf_counter()
    scores = get_cross(lasso, X, y)
    see_time('cross-validation rmse:')
    rmse = np.sqrt(np.mean(scores) * -1)
    print (rmse)

Listing 7-2Tuning tips with Lasso

```

执行清单 [7-2](#PC3) 的输出应该如下所示:

```
Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,
   normalize=False, positive=False, precompute=False,
   random_state=0, selection="cyclic", tol=0.0001,
   warm_start=False)

Lasso(rmse): 0.8785977911833892

Fitting 5 folds for each of 6 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    2.1s finished
training time: 2 seconds and 246.86 milliseconds
{'alpha': 0.1, 'max_iter': 10, 'positive': True}

Lasso(rmse): 0.8781319871042923

cross-validation rmse: 8.58 milliseconds
1.0379804468729155

```

代码从导入必需的包开始。函数 get_error 返回 RMSE。函数 see_time 返回经过的时间。函数 get_cross 返回交叉验证 RMSE。

主程序块从加载预处理的 tips 数据开始。代码继续将数据分割成训练测试子集。接下来，我们扩展数据。然后，我们用 Lasso 训练数据，并显示结果，以便与调整后的 RMSE 进行基线比较。

Lasso 是一种使用 L1 罚函数进行正则化的算法。我们根据之前的实验调整*α*、*正*和*最大值*超参数。

超参数 *alpha* 是乘以 L1 罚项的常数。也是用套索调音最重要的超参数。超参数*正*强制系数为正。超参数 *max_iter* 代表最大迭代次数。

使用 GridSearchCV 和网格*参数*开始调优。通过调整，我们能够将 RMSE 降低一个非常小的量。交叉验证表明我们做得很好。

### 小费

请记住，函数 get_error 会返回负的均方误差，因此我们必须将结果乘以-1 并取结果的平方根以获得 RMSE，从而使结果为正。

清单 [7-3](#PC5) 中显示的下一个代码示例使用 ARDRegression 调整提示。

```
import numpy as np, humanfriendly as hf
import time
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import ARDRegression
from sklearn.model_selection import GridSearchCV,\
     cross_val_score
from sklearn.metrics import mean_squared_error

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

def see_time(note):
    end = time.perf_counter()
    elapsed = end - start
    print (note, hf.format_timespan(elapsed, detailed=True))

def get_cross(model, data, target, groups=10):
    return cross_val_score(model, data, target, cv=groups,
                           scoring='neg_mean_squared_error')

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_tips.npy')
    y = np.load('data/y_tips.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)
    ard = ARDRegression().fit(X_train_std, y_train)
    print (ard, br)
    rmse, name = get_error(ard, X_test_std, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    iters = [50]
    a1 = [1e5, 1e4]
    a2 = [1e5, 1e4]
    params = {'n_iter': iters, 'alpha_1': a1, 'alpha_2': a2}
    grid = GridSearchCV(ard, params, cv=5, n_jobs=-1, verbose=1)
    start = time.perf_counter()
    grid.fit(X_train, y_train)
    see_time('training time:')
    bp = grid.best_params_

    print (bp, br)
    ard = ARDRegression(**bp).fit(X_train_std, y_train)
    rmse, name = get_error(ard, X_test_std, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    start = time.perf_counter()
    scores = get_cross(ard, X, y)
    see_time('cross-validation rmse:')
    rmse = np.sqrt(np.mean(scores) * -1)
    print (rmse)

Listing 7-3Tuning tips with ARDRegression

```

执行清单 [7-3](#PC5) 的输出应该如下所示:

```
ARDRegression(alpha_1=1e-06, alpha_2=1e-06, compute_score=False,
       copy_X=True, fit_intercept=True, lambda_1=1e-06,
       lambda_2=1e-06, n_iter=300, normalize=False,
       threshold_lambda=10000.0, tol=0.001, verbose=False)

ARDRegression(rmse): 0.8745960871430548

Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    3.5s finished
training time: 4 seconds and 286.03 milliseconds
{'alpha_1': 10000.0, 'alpha_2': 100000.0, 'n_iter': 50}

ARDRegression(rmse): 0.8645625277607758

cross-validation rmse: 4 seconds and 10.17 milliseconds
1.0376527153700184

```

代码从导入必需的包开始。函数 get_error 返回 RMSE。函数 see_time 返回经过的时间。函数 get_cross 返回交叉验证 RMSE。

主程序块从加载预处理的 tips 数据开始。代码继续将数据分割成训练测试子集。接下来，我们扩展数据。然后，我们使用 ARDRegression 训练数据，并显示结果，以便与调整后的 RMSE 进行基线比较。

*ARDRegression* (自动相关性确定回归)用贝叶斯岭回归拟合回归模型。模型的估计是通过迭代最大化观测值的边际对数似然来完成的。

我们用 *n_iter* 、 *alpha_1* 和 *alpha_2* 调谐。超参数 *n_iter* 是最大迭代次数。超参数 *alpha_1* 是优先于 alpha 参数的伽马分布的形状参数。超参数 *alpha_2* 是优先于 alpha 参数的伽马分布的逆比例参数(或速率参数)。

我们能够通过调谐来降低 RMSE。此外，交叉验证显示我们做得非常好。

## 调谐波士顿

清单 [7-4](#PC7) 中所示的代码示例基于未缩放和缩放的数据计算各种回归算法的 RMSE。由于波士顿是一个相对较小的数据集，所以运行这种类型的实验在计算上并不昂贵。

```
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor as rfr,\
     AdaBoostRegressor as ada, GradientBoostingRegressor as gbr
from sklearn.linear_model import LinearRegression as lr,\
     BayesianRidge as bay, Ridge as rr, Lasso as l,\
     LassoLars as ll, ElasticNet as en,\
     ARDRegression as ard, RidgeCV as rcv
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor as dtr
from sklearn.neighbors import KNeighborsRegressor as knn
from sklearn.preprocessing import StandardScaler

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_boston.npy')
    y = np.load('data/y_boston.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    regressors = [lr(), bay(), rr(alpha=.5, random_state=0),
                  l(alpha=0.1, random_state=0), ll(), knn(),
                  ard(), rfr(random_state=0, n_estimators=100),
                  SVR(gamma='scale', kernel="rbf"),
                  rcv(fit_intercept=False), en(random_state=0),
                  dtr(random_state=0), ada(random_state=0),
                  gbr(random_state=0)]
    print ('unscaled:', br)
    for reg in regressors:
        reg.fit(X_train, y_train)
        rmse, name = get_error(reg, X_test, y_test)
        name = reg.__class__.__name__
        print (name + '(rmse):', end=' ')
        print (rmse)
    print ()
    print ('scaled:', br)
    scaler = StandardScaler()
    X_train_std = scaler.fit_transform(X_train)
    X_test_std = scaler.fit_transform(X_test)
    for reg in regressors:
        reg.fit(X_train_std, y_train)
        rmse, name = get_error(reg, X_test_std, y_test)
        name = reg.__class__.__name__
        print (name + '(rmse):', end=' ')
        print (rmse)

Listing 7-4Calculating RMSE for boston data with regression algorithms

```

执行清单 [7-4](#PC7) 的输出应该如下所示:

```
unscaled:

LinearRegression(rmse): 4.236710574387242
BayesianRidge(rmse): 4.317939916221959
Ridge(rmse): 4.243658717030716
Lasso(rmse): 4.300740333025026
LassoLars(rmse): 8.754893348840868
KNeighborsRegressor(rmse): 5.9934937623789
ARDRegression(rmse): 4.28415048500826
RandomForestRegressor(rmse): 3.37169151536684
SVR(rmse): 7.100029068343849
RidgeCV(rmse): 4.392246392993031
ElasticNet(rmse): 4.88844846745213
DecisionTreeRegressor(rmse): 4.346328232622458
AdaBoostRegressor(rmse): 3.652816906059683
GradientBoostingRegressor(rmse): 3.1941117128039194

scaled:

LinearRegression(rmse): 4.398269524691269
BayesianRidge(rmse): 4.419543929268475
Ridge(rmse): 4.400075160458176
Lasso(rmse): 4.489952156682322
LassoLars(rmse): 8.754893348840868
KNeighborsRegressor(rmse): 4.757936288305807
ARDRegression(rmse): 4.383622227159
RandomForestRegressor(rmse): 4.053037237125816
SVR(rmse): 5.083294658978756
RidgeCV(rmse): 22.34757636411328
ElasticNet(rmse): 5.277752330669967
DecisionTreeRegressor(rmse): 5.2796587719252726
AdaBoostRegressor(rmse): 4.100148076529094
GradientBoostingRegressor(rmse): 3.7490071027496015

```

代码从导入必要的包和各种回归算法开始。函数 get_error 返回型号名称和 RMSE。主程序块首先从 NumPy 文件加载清理后的波士顿数据。请记住，我们清理了波士顿数据，并在第 [4](4.html) 章中将其保存以备将来处理。

代码继续将数据分割成训练测试子集。接下来，我们创建一个回归算法列表。代码继续在未缩放的数据上训练每个算法并显示结果。然后，代码缩放数据，根据缩放后的数据训练每个算法，并显示结果。

在这个实验中表现最好的算法是 GradientBoostingRegressor 和 RandomForestRegressor(两者都具有未缩放的数据)。所以，缩放数据并没有增加这个数据集的价值。

清单 [7-5](#PC9) 中显示的下一个代码示例使用 GradientBoostingRegressor 调优波士顿数据集。

```
import numpy as np, humanfriendly as hf, warnings, sys
import time
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV,\
     cross_val_score
from sklearn.metrics import mean_squared_error

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

def see_time(note):
    end = time.perf_counter()
    elapsed = end - start

    print (note, hf.format_timespan(elapsed, detailed=True))

def get_cross(model, data, target, groups=10):
    return cross_val_score(model, data, target, cv=groups,
                           scoring='neg_mean_squared_error')

if __name__ == "__main__":
    br = '\n'
    if not sys.warnoptions:
        warnings.simplefilter('ignore')
    X = np.load('data/X_boston.npy')
    y = np.load('data/y_boston.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    gbr = GradientBoostingRegressor(random_state=0)
    print (gbr, br)
    gbr.fit(X_train, y_train)
    rmse, name = get_error(gbr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    loss = ['ls', 'lad', 'huber']
    lr = [1e-2, 1e-1, 1e-0]
    n_est = [150, 200, 300, 500]
    alpha = [0.9]
    params = {'loss': loss, 'learning_rate': lr,
              'n_estimators': n_est, 'alpha': alpha}
    grid = GridSearchCV(gbr, params, cv=5, n_jobs=-1,
                        verbose=1, refit=False)
    start = time.perf_counter()
    grid.fit(X_train, y_train)
    see_time('training time:')
    bp = grid.best_params_

    print (bp, br)
    gbr = GradientBoostingRegressor(**bp, random_state=0)
    gbr.fit(X_train, y_train)
    rmse, name = get_error(gbr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    start = time.perf_counter()
    scores = get_cross(gbr, X, y)
    see_time('cross-validation rmse:')
    rmse = np.sqrt(np.mean(scores) * -1)
    print (rmse)

Listing 7-5Tuning boston data with GradientBoostingRegressor

```

执行清单 [7-5](#PC9) 的输出应该如下所示:

```
GradientBoostingRegressor(alpha=0.9, criterion="friedman_mse",
             init=None, learning_rate=0.1, loss="ls",
             max_depth=3, max_features=None,
             max_leaf_nodes=None, min_impurity_decrease=0.0,
             min_impurity_split=None,
             min_samples_leaf='deprecated', min_samples_split=2,
             min_weight_fraction_leaf='deprecated',
             n_estimators=100, n_iter_no_change=None,
             presort='auto', random_state=0, subsample=1.0,
             tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)

GradientBoostingRegressor(rmse): 3.1941117128039194

Fitting 5 folds for each of 36 candidates, totalling 180 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.1s
[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    9.1s finished
training time: 9 seconds and 170.11 milliseconds
{'alpha': 0.9, 'learning_rate': 0.1, 'loss': 'huber', 'n_estimators': 300}

GradientBoostingRegressor(rmse): 3.0839764165411934

cross-validation rmse: 3 seconds and 258.29 milliseconds
3.7929403445012064

```

代码从导入 GradientBoostingRegressor 以及其他必需的包开始。*GradientBoostingRegressor*通过建立一个前向模式的附加模型来执行回归的梯度推进。

函数 get_error 返回给定算法的 RMSE 和模型名称。函数 see_time 返回经过的时间。函数 get_cross 返回负的均方误差。

主块加载波士顿数据，将其分成训练测试子集，并使用 GradientBoostingRegressor 训练数据。代码继续显示带有默认参数的 RMSE，以提供与调优的 RMSE 进行比较的基线分数。接下来，用超参数*损失*、*学习率*、 *n 估计量*和*α*来调整模型。

超参数*损失*是要优化的损失函数。超参数 *learning_rate* 控制我们相对于损失梯度调整模型学习的程度。超参数 *n_estimators* 是要执行的升压阶段的数量。Hypeparameter *alpha* 是 huber 损失函数的 alpha 分位数。

通过调整可以降低 RMSE。我们通过运行交叉验证来结束。因为我们调整的 RMSE 低于交叉验证的 RMSE，所以我们的情况很好。

### 小费

您可能需要偶尔重启计算机，因为调整需要大量的计算资源。

本节的最后一个代码示例(如清单 [7-6](#PC11) 所示)使用 RandomForestRegressor 调优波士顿数据集。

```
import numpy as np, humanfriendly as hf, warnings, sys
import time
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV,\
     cross_val_score
from sklearn.metrics import mean_squared_error

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

def see_time(note):
    end = time.perf_counter()
    elapsed = end - start
    print (note, hf.format_timespan(elapsed, detailed=True))

def get_cross(model, data, target, groups=10):
    return cross_val_score(model, data, target, cv=groups,
                           scoring='neg_mean_squared_error')

if __name__ == "__main__":
    br = '\n'
    if not sys.warnoptions:
        warnings.simplefilter('ignore')
    X = np.load('data/X_boston.npy')
    y = np.load('data/y_boston.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    rfr = RandomForestRegressor(random_state=0)
    print (rfr, br)
    rfr.fit(X_train, y_train)
    rmse, name = get_error(rfr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    n_est = [100, 500, 1000]
    boot = [True, False]
    params = {'n_estimators': n_est, 'bootstrap': boot}
    grid = GridSearchCV(rfr, params, cv=5, n_jobs=-1,
                        verbose=1, refit=False)
    start = time.perf_counter()
    grid.fit(X_train, y_train)
    see_time('training time:')
    bp = grid.best_params_
    print (bp, br)
    rfr = RandomForestRegressor(**bp, random_state=0)
    rfr.fit(X_train, y_train)
    rmse, name = get_error(rfr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    start = time.perf_counter()
    scores = get_cross(rfr, X, y)
    see_time('cross-validation rmse:')
    rmse = np.sqrt(np.mean(scores) * -1)
    print (rmse)

Listing 7-6Tuning boston data with RandomForestRegressor

```

执行清单 [7-6](#PC11) 的输出应该如下所示:

```
RandomForestRegressor(bootstrap=True, criterion="mse",
           max_depth=None, max_features="auto",
           max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None,
           min_samples_leaf='deprecated', min_samples_split=2,
           min_weight_fraction_leaf='deprecated',
           n_estimators='warn', n_jobs=None, oob_score=False,
           random_state=0, verbose=0, warm_start=False)

RandomForestRegressor(rmse): 3.5587794792757004

Fitting 5 folds for each of 6 candidates, totalling 30 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    8.3s finished
training time: 8 seconds and 453.84 milliseconds
{'bootstrap': True, 'n_estimators': 100}

RandomForestRegressor(rmse): 3.37169151536684

cross-validation rmse: 1 second and 845.76 milliseconds
3.6815463792891623

```

代码从导入 RandomForestRegressor 以及其他必需的包开始。 *RandomForestRegressor* 在数据集的各种子样本上拟合多个分类决策树，并使用平均来提高预测精度和控制过度拟合。

函数 get_error 返回给定算法的 RMSE 和模型名称。函数 see_time 返回经过的时间。函数 get_cross 返回负的均方误差。

主块加载波士顿数据，将其分成训练测试子集，并用 RandomForestRegressor 训练数据。代码继续显示带有默认参数的 RMSE，以提供与调优的 RMSE 进行比较的基线分数。接下来，用超参数 *n 估计器*和*自助*来调整模型。

超参数 *n_estimators* 是森林中树木的数量。超参数 *bootstrap* 决定构建树时是否使用 bootstrap 样本。

通过调整可以降低 RMSE。我们通过运行交叉验证来结束。因为我们调整的 RMSE 低于交叉验证的 RMSE，所以我们的情况很好。

## 调酒

通过运行类似于清单 [7-1](#PC1) 和 [7-4](#PC7) 中所示的实验，我们发现 RandomForestRegressor(具有未缩放的数据)为红葡萄酒和白葡萄酒数据提供了最低的 RMSE。如果你愿意，继续做你自己的实验来验证我们的结果。

清单 [7-7](#PC13) 中的代码示例用 RandomForestRegressor 调优红酒数据集。

```
import numpy as np, humanfriendly as hf
import time
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV,\
     cross_val_score
from sklearn.metrics import mean_squared_error

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

def see_time(note):
    end = time.perf_counter()
    elapsed = end - start
    print (note, hf.format_timespan(elapsed, detailed=True))

def get_cross(model, data, target, groups=10):
    return cross_val_score(model, data, target, cv=groups,
                           scoring='neg_mean_squared_error')

if __name__ == "__main__":
    br = '\n'
    X = np.load('data/X_red.npy')
    y = np.load('data/y_red.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    rfr = RandomForestRegressor(random_state=0, n_estimators=10)
    print (rfr, br)
    rfr.fit(X_train, y_train)
    rmse, name = get_error(rfr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    n_est = [100, 500]
    boot = [True, False]
    params = {'n_estimators': n_est, 'bootstrap': boot}
    grid = GridSearchCV(rfr, params, cv=5, n_jobs=-1, verbose=1)
    start = time.perf_counter()
    grid.fit(X_train, y_train)
    see_time('training time:')
    bp = grid.best_params_
    print (bp, br)
    rfr = RandomForestRegressor(**bp, random_state=0)
    rfr.fit(X_train, y_train)
    rmse, name = get_error(rfr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    start = time.perf_counter()
    scores = get_cross(rfr, X, y)
    see_time('cross-validation rmse:')
    rmse = np.sqrt(np.mean(scores) * -1)
    print (rmse)

Listing 7-7Tuning red wine data with RandomForestRegressor

```

执行清单 [7-7](#PC13) 的输出应该如下所示:

```
RandomForestRegressor(bootstrap=True, criterion="mse",
                      max_depth=None, max_features="auto",
                      max_leaf_nodes=None,
                      min_impurity_decrease=0.0,
                      min_impurity_split=None,
                      min_samples_leaf='deprecated',
                      min_samples_split=2,
                      min_weight_fraction_leaf='deprecated',
                      n_estimators=10, n_jobs=None,
                      oob_score=False, random_state=0, verbose=0,
                      warm_start=False)

RandomForestRegressor(rmse): 0.626079068488957

Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    7.1s finished
training time: 7 seconds and 629.56 milliseconds
{'bootstrap': True, 'n_estimators': 100}

RandomForestRegressor(rmse): 0.5847897057917487

cross-validation rmse: 4 seconds and 804.96 milliseconds
0.6498982966515346

```

代码从导入必需的包开始。函数 get_error 返回给定算法的 RMSE 和模型名称。函数 see_time 返回经过的时间。函数 get_cross 返回负的均方误差。

主块加载红酒数据，将其分成训练测试子集，并用 RandomForestRegressor 训练数据。代码继续显示带有默认参数的 RMSE，以提供与调优的 RMSE 进行比较的基线分数。接下来，用超参数 *n 估计器*和*自助*来调整模型。

通过调整可以降低 RMSE。我们通过运行交叉验证来结束。因为我们调整的 RMSE 低于交叉验证的 RMSE，所以我们的情况很好。

清单 [7-8](#PC15) 中显示的最后一个代码示例用 RandomForestRegressor 调优了白葡萄酒数据集。

```
import numpy as np, humanfriendly as hf
import time
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV,\
     cross_val_score
from sklearn.metrics import mean_squared_error

def get_error(model, Xtest, ytest):
    y_pred = model.predict(Xtest)
    return np.sqrt(mean_squared_error(ytest, y_pred)),\
           model.__class__.__name__

def see_time(note):
    end = time.perf_counter()
    elapsed = end - start
    print (note, hf.format_timespan(elapsed, detailed=True))

def get_cross(model, data, target, groups=10):
    return cross_val_score(model, data, target, cv=groups,
                           scoring='neg_mean_squared_error')

if __name__ == "__main__":

    br = '\n'
    X = np.load('data/X_white.npy')
    y = np.load('data/y_white.npy')
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, random_state=0)
    rfr = RandomForestRegressor(random_state=0, n_estimators=10)
    print (rfr, br)
    rfr.fit(X_train, y_train)
    rmse, name = get_error(rfr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    n_est = [100, 500]
    boot = [True, False]
    params = {'n_estimators': n_est, 'bootstrap': boot}
    grid = GridSearchCV(rfr, params, cv=5, n_jobs=-1, verbose=1)
    start = time.perf_counter()
    grid.fit(X_train, y_train)
    see_time('training time:')
    bp = grid.best_params_
    print (bp, br)
    rfr = RandomForestRegressor(**bp, random_state=0)
    rfr.fit(X_train, y_train)
    rmse, name = get_error(rfr, X_test, y_test)
    print (name + '(rmse):', end=' ')
    print (rmse, br)
    start = time.perf_counter()
    scores = get_cross(rfr, X, y)
    see_time('cross-validation rmse:')
    rmse = np.sqrt(np.mean(scores) * -1)
    print (rmse)

Listing 7-8Tuning white wine data with RandomForestRegressor

```

执行清单 [7-8](#PC15) 的输出应该如下所示:

```
RandomForestRegressor(bootstrap=True, criterion="mse",
           max_depth=None, max_features="auto",
           max_leaf_nodes=None, min_impurity_decrease=0.0,
           min_impurity_split=None,
           min_samples_leaf='deprecated', min_samples_split=2,
           min_weight_fraction_leaf='deprecated',n_estimators=10,
           n_jobs=None, oob_score=False, random_state=0,
           verbose=0, warm_start=False)

RandomForestRegressor(rmse): 0.6966098665124181

Fitting 5 folds for each of 4 candidates, totalling 20 fits
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.
[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:   18.7s finished
training time: 25 seconds and 709.64 milliseconds
{'bootstrap': True, 'n_estimators': 500}

RandomForestRegressor(rmse): 0.6728175517621279

cross-validation rmse: 1 minute, 24 seconds and 70.99 milliseconds 0.7183073387927801

```

代码从导入必需的包开始。函数 get_error 返回给定算法的 RMSE 和模型名称。函数 see_time 返回经过的时间。函数 get_cross 返回负的均方误差。

主块加载白葡萄酒数据，将其分成训练测试子集，并用 RandomForestRegressor 训练数据。代码继续显示带有默认参数的 RMSE，以提供与调优的 RMSE 进行比较的基线分数。接下来，用超参数 *n 估计器*和*自助*来调整模型。

通过调整可以降低 RMSE。我们通过运行交叉验证来结束。因为我们调整的 RMSE 低于交叉验证的 RMSE，所以我们的情况很好。