

# 四、半监督和主动学习

在[第 2 章](ch02.html "Chapter 2. Practical Approach to Real-World Supervised Learning")、*现实世界监督学习的实用方法*和[第 3 章](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques")、*无监督机器学习技术*中，我们讨论了两组主要的机器学习技术，它们适用于标记数据可用性的相反情况——一组是所有目标值都已知，另一组是一无所知。相比之下，本章中的技术解决了这样一种情况，即我们必须从混合了一小部分有标签的数据和大量无标签实例的数据中进行分析和学习。

在语音和图像识别中，大量数据是可用的，并且是各种形式的。然而，标记或分类所有这些数据的成本是昂贵的，因此，在实践中，被分类的语音或图像与未被分类的语音或图像的比例非常小。类似地，在 web 文本或文档分类中，万维网上有大量的文档，但是基于主题或上下文对它们进行分类需要领域专家，这使得该过程复杂且昂贵。在这一章中，我们将讨论两个广泛的主题，涵盖“从未标记数据中学习”的领域，即**半监督学习** ( **SSL** )和主动学习。我们将介绍每个主题，并讨论与每个主题相关的分类法和算法，就像我们在前几章所做的那样。由于这本书强调实践方法，我们将讨论每种学习类型可用的工具和库。然后，我们将考虑真实世界的案例研究，并展示在实际情况中应用这些工具时有用的技术。

以下是本章涵盖的主题列表:

*   半监督学习:

    *   表示、符号和假设
    *   半监督学习技术:

        *   自训练 SSL
        *   协同训练 SSL
        *   聚类和标签 SSL
        *   直推式图标签传播
        *   直推式 SVM

    *   半监督学习案例研究

*   主动学习:

    *   表示和批注
    *   主动学习场景
    *   主动学习方法:

        *   不确定性采样

            *   最小置信采样
            *   最小间隔采样
            *   标签熵采样

        *   版本空间采样:

            *   查询被异议
            *   查询被委员会

        *   密度加权法

*   案例学习中的主动学习

# 半监督学习

半监督学习背后的想法是从有标签和无标签的数据中学习，以提高模型的预测能力。这个概念用一个简单的例子来解释，*图 1* ，它显示了当大量未标记的数据可用时，例如，网络上的 HTML 文档，专家可以将其中一些分类到已知的类别中，如体育、新闻、娱乐等。然后，半监督学习技术可以使用这一小组已标记数据和大的未标记数据集来学习模型。因此，使用已标记和未标记数据的知识，该模型可以在未来对看不见的文档进行分类。相比之下，监督学习仅使用标记数据:

![Semi-supervised learning](graphics/B05137_04_003.jpg)![Semi-supervised learning](graphics/B05137_04_005.jpg)

图一。以 web 文档分类为例，半监督学习过程(下图)与监督学习(下图)进行了对比。主要区别是可用于学习的标记数据的数量，在半监督的情况下由限定词“小”突出显示。

## 表示、符号和假设

像以前一样，我们将介绍我们在本章中使用的符号。数据集 *D* 由表示为 **x** 的单个数据实例组成，也表示为一个集合{**x**1，**x**2，…**x**n}，即没有标签的数据实例集合。与这些数据实例相关联的标签是{ *y* [1] ， *y* [2] ，… *y* [n] }。整个被标记的数据集可以表示为集合中的成对元素，如下所示:*d*= {(**x**t35】1，*y*t39】1)，( **x** 2，*y*t45】2)，…(**x**t49】n， *y* 在半监督学习中，我们将数据集 *D* 进一步分为两组 *U* 和 *L* 分别用于未标记和标记数据。

标记数据![Representation, notation, and assumptions](graphics/B05137_04_015.jpg)由所有已知结果的标记数据组成{y [1] ，y [2] ，..y [ l ] }。未标记的数据![Representation, notation, and assumptions](graphics/B05137_04_017.jpg)是结果未知的数据集。| * U * | > | * L * |。

归纳半监督学习由一组技术组成，给定带有标记数据![Representation, notation, and assumptions](graphics/B05137_04_015.jpg)和未标记数据![Representation, notation, and assumptions](graphics/B05137_04_017.jpg)的训练集 *D* ，学习表示为![Representation, notation, and assumptions](graphics/B05137_04_021.jpg)的模型，使得模型 *f* 能够很好地预测训练未标记数据 *U* 之外的未知数据。它“归纳”了一个模型，可以像监督学习算法一样用于预测看不见的实例。

直推式半监督学习由一套技术组成，在给定训练集 *D* 的情况下，这些技术学习一个模型![Representation, notation, and assumptions](graphics/B05137_04_024.jpg)，该模型只对未标记的数据进行预测。它不需要在看不见的未来实例上执行，因此是比基于归纳的学习更简单的 SSL 形式。

下面列出了半监督学习算法中所做的一些假设，这些假设应该适用于这些类型的成功学习。要使 SSL 工作，这些假设中的一个或多个必须为真:

*   **半监督平滑度**:简单来说，如果两个点在密度或距离上“接近”，那么它们的标签一致。相反，如果两个点是分开的并且在不同的密度区域，那么它们的标签不需要一致。
*   **聚类一致性**:如果类的数据实例倾向于形成一个聚类，那么未标记的数据可以帮助聚类算法找到更好的聚类。
*   **流形一致性**:在许多现实世界的数据集中，高维数据位于低维流形中，使得学习算法能够克服维数灾难。如果在给定的数据集中这是真的，则未标记的数据也映射到流形，并且可以改进学习。

## 半监督学习技术

在本节中，我们将描述不同的 SSL 技术，以及一些伴随的算法。我们将使用与前几章相同的结构，并在三个小节中描述每种方法:*输入和输出*，*它是如何工作的？*和*优点和局限性*。

### 自我训练 SSL

自我训练是 SSL 的最简单形式，我们通过应用从标记集学习的模型，执行一个简单的迭代过程，从未标记集输入数据(*参考文献* [1]):

![Self-training SSL](graphics/B05137_04_025.jpg)

图二。二进制分类中的自训练 SSL，一些带标签的数据用蓝色矩形和黄色圆圈显示。在各种迭代之后，未标记的数据被映射到各自的类。

#### 输入和输出

输入是带有少量已标记数据和大量未标记数据的训练数据。基本分类器(线性或非线性，如朴素贝叶斯、KNN、决策树或其他)与每个算法所需的超参数一起提供。对数据类型的约束类似于基础学习者。诸如达到*最大迭代次数*或*未标记数据用尽*等停止条件也是必须做出的选择。通常，我们使用基础学习者来给出输出的概率或等级。作为输出，该技术生成的模型可用于对除了所提供的未标记数据之外的未知数据集执行预测。

#### 它是如何工作的？

整个算法可以总结如下:

1.  当未达到停止标准时:

    1.  用标记数据训练分类器模型![How does it work?](graphics/B05137_04_021.jpg)*L*
    2.  将分类器模型 *f* 应用于未标记数据 *U*
    3.  选择 *k* 来自 *U* 的最有把握的预测作为集合 *L* [u]
    4.  用 k 个数据点扩充标记数据【T24

2.  重复 2 下的所有步骤。

抽象地说，自我训练可以被看作是一个应用于半监督设置的期望最大化过程。训练分类器模型的过程是使用 MLE 或 MAP 来寻找参数θ。使用学习模型计算标签类似于*期望*步骤，其中![How does it work?](graphics/B05137_04_031.jpg)在给定参数θ的情况下从 *U* 估计标签。学习具有扩充标签的模型的迭代下一步类似于*最大化*步骤，其中新参数被调整到*θ’*。

#### 优点和局限性

的优点和局限性如下:

*   简单，适用于大多数监督学习技术。
*   异常值和噪声会导致预测中的错误得到加强，技术退化。

### 协同训练 SSL 或多视图 SSL

基于 SSL 的联合训练涉及从同一数据的两个不同“视图”中学习。它是多视图 SS 的一个特例(参考文献[2])。每个视图可以被认为是捕获一些领域知识的点的特征集，并且与另一个视图正交。例如，web 文档数据集可以被认为具有两个视图:一个视图是表示文本的特征，另一个视图是表示到其他文档的超链接的特征。假设每个视图都有足够的数据，从每个视图中学习可以改进整个标注过程。在无法对要素进行此类划分的数据集中，将要素随机分割成不相交的集合会形成视图。

#### 输入和输出

输入是带有少量标记的训练数据和大量未标记的数据。除了提供数据点之外，还有对应于每个视图的特征集，并且假设这些特征集不重叠并且解决不同的分类问题。选择线性或非线性的基本分类器，例如朴素贝叶斯、KNN、决策树或任何其他分类器，以及每个算法所需的超参数。作为输出，该方法生成的模型可用于对除所提供的未标记数据之外的未知数据集执行预测。

#### 它是如何工作的？

我们将使用两种数据视图来演示该算法:

1.  将数据初始化为![How does it work?](graphics/B05137_04_015.jpg)标记和![How does it work?](graphics/B05137_04_017.jpg)未标记。每个数据点都有两个视图**x =【x****1****，x****²****】**和*L =【L**¹**，L**2**。*
2.  *未达到停止标准时:

    1.  分别用标记数据 *L* 1 和 *L* 2 训练分类器模型![How does it work?](graphics/B05137_04_035.jpg)和![How does it work?](graphics/B05137_04_036.jpg)。
    2.  将分类器模型 *f* ¹ 和 *f* ² 应用到未标记数据上 *U* 使用它们自己的特征。
    3.  从 *U* 中选择 *k* 最有把握的预测，分别套用*f*1 和*f*2 为 set*L*[U]¹和*L*[U]2。
    4.  用 *k* 数据点*L*¹=*L*¹∪*L*[¹和*L*²=*L*对标记的数据进行扩充]* 
3.  *重复 2 下的所有步骤。*

#### *优点和局限性*

*优点和局限性是:*

*   *当特性具有不同的方面或不同领域的组合时，联合训练比简单的自我训练更有益*
*   *具有正交视图和从中学习的能力的必要和充分条件对该技术的通用性提出了挑战*

### *聚类并标记 SSL*

*这种技术，像自我训练一样，非常通用，适用于域和数据集，其中假设部分提到的聚类假设成立(参考文献【3】)。*

#### *输入和输出*

*输入是用一些有标签的和大量无标签的实例训练数据。聚类算法及其参数以及分类算法及其参数构成了附加输入。该技术生成一个分类模型，可以帮助预测看不见的数据的类别。*

#### *它是如何工作的？*

*抽象算法可以给定为:*

1.  *将数据初始化为![How does it work?](graphics/B05137_04_015.jpg)标记和![How does it work?](graphics/B05137_04_017.jpg)未标记。*
2.  *使用聚类算法对整个数据进行聚类，包括已标记和未标记的数据。*
3.  *对于每个聚类，设 *S* 是从集合 *L* 中抽取的标记实例的集合。

    1.  从 *S* ，*f*[S]=*L*[S]学习一个监督模型。
    2.  应用模型 *f* [s] 并使用前面的模型对每个聚类的未标记实例进行分类。* 
4.  *由于所有未标记的实例![How does it work?](graphics/B05137_04_017.jpg)都被前面的过程分配了一个标签，因此在整个集合上运行监督分类模型。![How does it work?](graphics/B05137_04_048.jpg)

    图 3。聚类和标签 SSL–先聚类后分类* 

#### *优点和局限性*

*的优势和的局限性是:*

*   *当聚类假设成立并且聚类算法和参数的选择正确时工作得非常好*
*   *大量的参数和选择使得这在许多现实世界的问题中成为一种笨拙的技术*

### *直推图标签传播*

*基于图的方法背后的关键思想是将数据集中标记和未标记的每个实例表示为一个节点，并将边计算为它们之间某种形式的“相似性”。使用假设部分中讨论的标签平滑度的基本概念，已知标签用于传播未标签数据中的标签，也就是说，相似的数据点将在图形上彼此“接近”(*参考文献*【4】)。*

*图 4 示出了当手写数字模式改变时，从第一个数据点到最后一个数据点的箭头粗细所指示的相似性是如何变化的。已知第一个标签，由于特征的相似性，标签传播可以有效地标记接下来的三个数字，而最后一个数字虽然标记相同，但与前三个数字相比具有较低的相似性。*

*![Transductive graph label propagation](graphics/B05137_04_050.jpg)

图 4。直推图形标签传播–手写数字的分类。最左边和最右边的图像被标记，其他的没有标记。箭头粗细是与左边带标签的数字“2”相似性的视觉度量。* 

#### *输入和输出*

*输入是带有少数标记和大量未标记数据的训练数据。选择图加权或相似度计算方法，如 k-最近加权、高斯衰减距离或ϵ-radius 方法。输出是整个数据的标记集；它通常不像之前看到的算法那样建立归纳模型。*

#### *它是如何工作的？*

*通用标签传播方法如下:*

1.  *构建一个图 *g = (V，E)* 其中:

    *   顶点 *V = {1，2…n}* 对应于属于标记集合 *L* 和未标记集合 *U* 的数据。
    *   边 *E* 是权重矩阵 **W** ，使得 **W** [i，j] 表示两个数据点**x**I，**x**j 之间某种形式的相似性。* 
2.  *通过![How does it work?](graphics/B05137_04_059.jpg)计算对角度矩阵 **D** 。*
3.  *假设标记集是二进制的，有![How does it work?](graphics/B05137_04_060.jpg)。将所有未标记数据的标签初始化为 0。![How does it work?](graphics/B05137_04_061.jpg)*
4.  *迭代到*t*= 0:

    1.  ![How does it work?](graphics/B05137_04_063.jpg)
    2.  ![How does it work?](graphics/B05137_04_064.jpg)(将标注实例的标签重置回原来)
    3.  回到步骤 4，直到收敛![How does it work?](graphics/B05137_04_065.jpg)* 
5.  *使用收敛标签![How does it work?](graphics/B05137_04_065.jpg)标记未标记的点![How does it work?](graphics/B05137_04_017.jpg)。*

*基于相似性、迭代中选择的优化等等，有许多变化。*

#### *优点和局限性*

*优点和局限性是:*

*   *基于图的半监督学习方法在计算方面是昂贵的——通常为 O(n ³ )，其中 *n* 是实例的数量。虽然加速和缓存技术有所帮助，但大数据的计算成本使得它在许多现实世界的数据情况下不可行。*
*   *这种直推性质使其难以用于实际目的，因为在实际目的中，模型需要对看不见的数据进行归纳。还有一些扩展，如谐波混合等等，可以解决这些问题。*

### *直推式 SVM (TSVM)*

*直推式 SVM 是最古老和最流行的直推式半监督学习方法之一，由 Vapnik 提出(*参考文献* [5])。关键原则是，未标记的数据和标记的数据可以帮助使用大幅度的概念找到决策边界。基本原则是决策边界通常不在高密度区域！*

#### *输入和输出*

*输入是具有很少标记和大量未标记数据的训练数据。对于 TSVM 计算，输入必须是数字要素。核的选择、核参数和成本因子都是基于 SVM 的参数，也是输入变量。输出是未标注数据集的标注。*

#### *它是如何工作的？*

*一般来说，SVM 在根据权重向量 **w** 和服从![How does it work?](graphics/B05137_04_072.jpg)的偏差*b*公式化的标记硬边界 SVM 中作为优化问题*

1.  *将数据初始化为![How does it work?](graphics/B05137_04_015.jpg)标记和![How does it work?](graphics/B05137_04_017.jpg)未标记。*
2.  *在 TSVM，方程式修改如下:![How does it work?](graphics/B05137_04_073.jpg)*

*这取决于以下条件:*

*![How does it work?](graphics/B05137_04_074.jpg)**![How does it work?](graphics/B05137_04_075.jpg)**![How does it work?](graphics/B05137_04_076.jpg)*

*这很像归纳 SVM，但只使用标记数据。当我们约束未标记的数据以符合标记数据的超平面的边以便最大化边缘时，它导致未标记的数据以最大的边缘分离被标记！通过将惩罚因子添加到约束条件中，或者像在归纳 SVM 中那样用核来替换输入空间中的点积，可以从未标记的数据中标记复杂的非线性噪声数据集。*

**图 5* 展示了 TSVM 的概念与仅在标记数据上运行的归纳 SVM 的比较，以及为什么 TSVM 可以使用未标记数据集找到更好的决策边界。超平面两侧的未标记数据集更接近它们各自的类，因此有助于找到更好的边界分隔符。*

*![How does it work?](graphics/B05137_04_077.jpg)

图 5。转导 SVM* 

#### *优点和局限性*

*优点和局限性:*

*   *给定无噪声的标记数据，TSVMs 可以在线性或非线性数据集中很好地工作。*
*   *TSVMs 在寻找超参数并调整它们以获得与归纳 SVM 相同的最佳结果方面存在同样的问题。*

## *半监督学习案例研究*

*在这个案例研究中，我们使用了另一个来自 UCI 知识库的经过充分研究的数据集，即威斯康星州乳腺癌数据集。在实验的第一部分，我们使用名为`JKernelMachines`的开源库演示了如何应用半监督学习的直推式 SVM 技术。我们为这项技术选择了 SVMLight 算法和高斯核。*

*在第二部分中，我们使用了基于 GUI 的框架 KEEL，并使用 UCI 乳腺癌数据集比较了几种基于进化学习的算法的结果。以下小节描述了工具、方法和评估措施。*

### *工具和软件*

*在半监督学习案例研究中使用的两个开源 Java 工具是`JKernelMachines`，一个直推式 SVM，和一个基于 GUI 的工具 KEEL，它使用进化算法进行学习。*

### *注意*

***JKernelMachines(直推型 SVM)***

*`JKernelMachines`是一个纯 Java 库，为使用和快速开发专用内核提供了一个高效的框架。核是用于支持向量机的相似性函数。`JKernelMachines`除了线性和高斯等矢量数据上的标准内核之外，还提供了在结构化数据上定义的内核实现。特别是，它提供了内核、在列表上定义的内核以及具有各种缓存策略的内核的组合。该库还包含 SVM 优化算法实现，包括 LaSVM 和使用 SMO 的一类 SVM。该库的创建者报告说，JKernelMachines 在一些常见的 UCI 知识库数据集上的结果与 Weka 库相当或更好。*

*使用`JKernelMachines`加载数据和运行直推式 SVM 的示例如下:*

```
*try {
//load the labeled training data
List<TrainingSample<double[]>> labeledTraining = ArffImporter.importFromFile("resources/breast-labeled.arff");
//load the unlabeled data
List<TrainingSample<double[]>> unlabeledData =ArffImporter.importFromFile("resources/breast-unlabeled.arff");
//create a kernel with Gaussian and gamma set to 1.0
DoubleGaussL2 k = new DoubleGaussL2(1.0);
//create transductive SVM with SVM light
S3VMLight<double[]> svm = new S3VMLight<double[]>(k);
//send the training labeled and unlabeled data
svm.train(labeledTraining, unlabeledData);
} catch (IOException e) {
	e.printStackTrace();
}*
```

*在第二种方法中，我们使用带有相同数据集的 KEEL。*

### *注意*

***KEEL***

***KEEL** ( **基于进化学习的知识提取**)是一个带有 GUI 的非商业(GPLv3) Java 工具，它使用户能够为各种数据挖掘问题分析进化学习的行为，包括回归、分类和无监督学习。它减轻了用户编写复杂进化算法的负担，并允许他们专注于使用工具包创建的新学习模型。KEEL 旨在满足研究人员和学生的需求。*

*KEEL 包含用于数据预处理和后处理的算法以及统计库，以及一个知识提取算法库，该算法库将多个进化学习算法与经典学习技术相结合。*

*工具中的 GUI 向导为管道的每个阶段提供了不同的功能组件，包括:*

*   *数据管理:数据的导入、导出、数据转换、可视化等等*
*   *实验设计:选择分类器、估计器、无监督技术、验证方法等*
*   *SSL 实验:直推式和归纳式分类(见本节 SSL 实验设计的离线方法图片)*
*   *统计分析:这为成对和多重比较、参数和非参数程序提供测试。*

*更多信息请访问[http://sci2s.ugr.es/keel/](http://sci2s.ugr.es/keel/)和[http://sci2s . ugr . es/keel/pdf/keel/articulo/Alcalaetal-soft computing-keel 1.0 . pdf](http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf)。*

*![Tools and software](graphics/B05137_04_078.jpg)

图 6:KEEL——基于向导的图形界面* 

### *商业问题*

*乳腺癌是世界范围内女性的头号癌症，尤其是在发展中国家，大多数病例在晚期才被诊断出来。使用非手术方法检查肿瘤块是一种廉价的疾病早期检测预防措施。*

*在本案例研究中，使用了来自这种程序的标记数据集，目标是使用多种 SSL 技术将乳腺癌数据分类为恶性和良性。*

### *机器学习映射*

*为了说明到目前为止本章所学的技术，我们将使用 SSL 进行分类。尽管数据集包含所有示例的标签，但是为了将此视为我们可以应用 SSL 的问题，我们将考虑一小部分数据是未标记的。事实上，我们使用未标记数据的不同部分运行了多个实验进行比较。使用的不同基础学习器是我们在前面章节中熟悉的分类算法。*

### *数据收集*

*这个数据集是由威斯康星大学麦迪逊医院收集的。数据集以 Weka AARF 格式提供。数据没有划分为训练、验证和测试。*

#### *数据质量分析*

*数据中的示例不包含唯一标识符。有 16 个示例的裸细胞核属性缺少值。目标类是唯一的分类属性，有两个值。所有其他属性都是连续的，并且在范围[1，10]内。*

### *数据采样和转换*

*在实验中，我们给出了 10 重交叉验证的结果。为了进行比较，进行了四次运行，每次使用不同比例的标记数据—10%、20%、30%和 40%。*

*每个示例都添加了一个数字样本代码编号作为唯一标识符。类别属性的分类值恶性和良性分别由数值 4 和 2 替换。*

### *数据集和分析*

*乳腺癌症数据集 Wisconsin(原始)[可从 UCI 机器学习储存库获得:](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original))[https://archive . ics . UCI . edu/ml/datasets/Breast+Cancer+Wisconsin+(原始)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original))。*

*这个数据库最初是从威斯康星大学麦迪逊医院的 William H. Wolberg 博士那里获得的。该数据集由 Wolberg 博士创建，用于乳腺肿瘤的诊断和预后。数据完全基于涉及**细针抽吸** ( **FNA** )测试的测量。在这项测试中，使用小规格针从乳房肿块中抽取液体，然后在显微镜下进行目视检查。*

*共有 699 个实例组成了数据集，其中包含九个数字属性和一个二元类(恶性/良性)。缺失值的百分比为 0.2%。在数据集中有 65.5%的恶性和 34.5%的良性病例。下表列出了功能名称和有效值范围:*

| 

民数记

 | 

功能名称

 | 

领域

 |
| --- | --- | --- |
| one | 样本代码编号 | 识别号 |
| Two | 团块厚度 | 1 - 10 |
| three | 细胞大小的均匀性 | 1 - 10 |
| four | 细胞形状的均匀性 | 1 - 10 |
| five | 边缘粘连 | 1 - 10 |
| six | 单一上皮细胞大小 | 1 - 10 |
| seven | 裸核 | 1 - 10 |
| eight | 平淡的染色质 | 1 - 10 |
| nine | 正常核仁 | 1 - 10 |
| Ten | 有丝分裂 | 1 - 10 |
| Eleven | 班级 | 良性 2 例，恶性 4 例 |

#### *特征分析结果*

*表 1 显示了按功能分类的汇总统计数据。*

|   | 

团块厚度

 | 

细胞大小均匀性

 | 

细胞形状均匀性

 | 

边缘粘连

 | 

单个 Epi 单元大小

 | 

裸核

 | 

平淡的染色质

 | 

正常核仁

 | 

有丝分裂

 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 意思是 | Four point four one eight | Three point one three four | Three point two zero seven | Two point eight zero seven | Three point two one six | Three point five four five | Three point four three eight | Two point eight six seven | One point five eight nine |
| 标准 | Two point eight one six | Three point zero five one | Two point nine seven two | Two point eight five five | Two point two one four | Three point six four four | Two point four three eight | Three point zero five four | One point seven one five |
| 部 | one | one | one | one | one | one | one | one | one |
| 25% | Two | one | one | one | Two |   | Two | one | one |
| 50% | four | one | one | one | Two |   | three | one | one |
| 75% | six | five | five | four | four |   | five | four | one |
| 最大 | Ten | Ten | Ten | Ten | Ten | Ten | Ten | Ten | Ten |

> **表 1。功能总结**

### *实验和结果*

*两种 SSL 算法被选择用于实验——自我训练和共同训练。此外，选择了四种分类方法作为基础学习器——朴素贝叶斯、C4.5、K-NN 和 SMO。此外，每个实验使用标记和未标记数据的四个不同部分(10%、20%、30%和 40%标记)来运行。*

*表 2 中给出了算法和基本分类器的超参数。对于这两种 SSL 算法，您可以看到对应于标记和未标记数据的四个分区的不同运行的准确性。*

*最后，我们给出了 40%标记情况下每个实验的性能结果。所提供的性能度量是准确度和带有标准偏差的 Kappa 统计量。*

| 

方法

 | 

因素

 |
| --- | --- |
| 自我训练 | 最大 ITER = 40 |
| 合作培训 | 最大 ITER = 40，初始未标记池=75 |
| KNN | K = 3，欧几里德距离 |
| C4.5 | 修剪过的树，置信度= 0.25，每片叶子 2 个样本 |
| 铌 | 未指定参数 |
| 高年资军医(senior medical officer) | C = 1.0，容差参数= 0.001，ε= 1.0E-12，核类型=多项式，多项式次数= 1，拟合逻辑模型=真 |

> **表 2。用于自训练和协同训练的基本分类器超参数**

| SSL 算法 | 10% | 20% | thirty percent | 40% |
| 自学 C 4.5 | Zero point nine | Zero point nine three | Zero point nine four | Zero point nine four seven |
| 共同培训 SMO | Zero point nine five nine | Zero point nine four nine | Zero point nine six two | Zero point nine five nine |

> **表 3。标签样本比例不同的样本的模型精度**

| 

算法

 | 

准确性(无未标注)

 |
| --- | --- |
| C4.5 10 倍 CV | Zero point nine four seven |
| SMO 10 fold CV | Zero point nine six seven |

|   |   | 10 倍 CV 威斯康星 40%标记数据 |   |
| 自我培训(kNN) | 准确(性) | 0.9623 (1) | 卡帕 | 0.9170 (2) |
|   | 标准发展 | 0.0329 | 标准发展 | 0.0714 |
| 自我培训(C45) | 准确(性) | 0.9606 (3) | 卡帕 | 0.9144 |
|   | 标准发展 | 0.0241 | 标准发展 | 0.0511 |
| 自我培训(NB) | 准确(性) | 0.9547 | 卡帕 | 0.9036 |
|   | 标准发展 | 0.0252 | 标准发展 | 0.0533 |
| 自我培训(SMO) | 准确(性) | 0.9547 | 卡帕 | 0.9035 |
|   | 标准发展 | 0.0208 | 标准发展 | 0.0435 |
| 联合培训 | 准确(性) | 0.9492 | 卡帕 | 0.8869 |
|   | 标准发展 | 0.0403 | 标准发展 | 0.0893 |
| 联合培训(C45) | 准确(性) | 0.9417 | 卡帕 | 0.8733 |
|   | 标准发展 | 0.0230 | 标准发展 | 0.0480 |
| 合作培训(NB) | 准确(性) | 0.9622 (2) | 卡帕 | 0.9193 (1) |
|   | 标准发展 | 0.0290 | 标准发展 | 0.0614 |
| 共同培训(SMO) | 准确(性) | 0.9592 | 卡帕 | 0.9128 (3) |
|   | 标准发展 | 0.0274 | 标准发展 | 0.0580 |

> **表 4。使用 40%标记示例的模型性能比较。每个类别中排名最高的表现者显示在括号中。**

#### *半监督学习分析*

*使用 40%的标记数据，使用 C4.5 的半监督自训练达到了与仅使用 C4.5 的 100%标记数据相同的结果。这显示了当数据稀疏标记时半监督学习的强度。*

*具有多项式核的 SMO，具有 30-40%的数据接近 100%的数据，但不如 C4.5 好*

*在 40%标记的训练数据上用四个分类器进行自训练和共同训练显示*

*   *作为基础分类器和自训练的 KNN 具有最高的准确度(0.9623)，其指示数据的非线性边界。与朴素贝叶斯的联合训练非常接近。*
*   *使用线性朴素贝叶斯、非线性 C4.5 和高度非线性 KNN 等分类器的自我训练显示出准确度的稳步提高:0.9547、0.9606、0.9623，这再次表明使用自我训练但为问题选择正确的底层分类器非常重要。*
*   *与朴素贝叶斯的联合训练具有最高的 Kappa 统计量(0.9193)和与自我训练的 KNN 几乎相似的准确性。特征之间的独立关系——因此将特征集分解成正交的特征集并将其用于分类器——改善了学习。*

*

# 主动学习

虽然主动学习与半监督学习有许多相似之处，但它有自己独特的方法来对包含标记和未标记数据的数据集进行建模。它植根于人类的基本心理，即问更多的问题往往有助于解决问题。

主动学习背后的主要思想是，如果学习者能够挑选实例进行学习，而不是得到带标签的数据，那么学习者可以用更少的数据更有效地学习(*参考* [6】)。使用非常少量的标记数据，它可以仔细地从未标记数据中挑选实例，以获得标记信息，并使用它来迭代地改进学习。这种查询未标记数据以从所谓的 oracle(该领域的专家)获得标签的基本方法将主动学习与半监督或被动学习区分开来。下图说明了其中的差异和迭代过程:

![Active learning](graphics/B05137_04_079.jpg)

图 7。主动机器学习过程与监督和半监督学习过程的对比。

## 表示和符号

由 *D* = {( **x** [1] ， *y* [2] )，( **x** [2] ， *y* [2] )，… ( **x** [n] 给出了数据集 *D* ，其中表示所有的数据实例及其标签 *y* [n] )}其中![Representation and notation](graphics/B05137_04_012.jpg)是数据的个个体实例，{ * y * [1] ， *y* [2] ，… * y * [n] 是关联标签的集合。 *D* 由两组 *U* 标记数据和 *L* 未标记数据组成。 **x** 是没有标签的数据实例的集合{ ** x ** [1] ， **x** [2] ，… ** x ** [ n ] }

数据集![Representation and notation](graphics/B05137_04_015.jpg)包括所有已知结果的标记数据{*y*1，*y*2，…*y*l}，而![Representation and notation](graphics/B05137_04_017.jpg)是结果未知的数据集。和之前一样，|*U*|>|>|*L*|。

## 主动学习场景

主动学习场景可以大致分为:

*   **基于流的主动学习**:在这种方法中，仅从未标记的数据集中挑选实例或例子，并决定是忽略数据还是将其传递给 oracle 以获得其标签(*裁判*【10，11】)。
*   **基于池的主动学习**:在这种方法中，从未标记的数据集中查询实例，然后根据信息量进行排序，并将这些实例中的一组实例发送到 Oracle 以获得标签(*裁判*【12】)。
*   **查询合成**:在这种方法中，学习者只有关于输入空间(特征)的信息，并从成员的未标记集合中合成查询。这个并不用于实际应用，因为它通常不考虑数据生成分布，因此查询通常是任意的或无意义的。

## 主动学习方法

不管所涉及的场景如何，每一种主动学习方法都包括选择一种查询策略或采样方法，这种策略或方法建立了在每次迭代中挑选查询的机制。每种方法都揭示了一种独特的方式来寻找具有最佳信息内容的未标记的例子，以改进学习过程。在下面的小节中，我们将描述主要的查询策略框架，它们是如何工作的，它们的优点和局限性，以及每个框架中的不同策略。

### 不确定性采样

这种形式的采样背后的关键思想是从当前模型最不确定的未标记池中选择个实例。然后，学习者可以避免模型在分类中更确定或更有信心的实例(*参考*【8】)。

基于概率的模型(朴素贝叶斯、逻辑回归等)是这种方法最自然的选择，因为它们给出了给定模型的置信测度，比如说数据 **x** 的 *θ* ，类*y*It8】Iϵ类，以及概率![Uncertainty sampling](graphics/B05137_04_086.jpg)作为后验概率。

#### 它是如何工作的？

所有基于不确定性的算法的一般过程概述如下:

1.  将数据初始化为带标签的![How does it work?](graphics/B05137_04_015.jpg)和不带标签的![How does it work?](graphics/B05137_04_017.jpg)。
2.  当仍有未标记数据时:

    1.  用标记数据 *L* 训练分类器模型![How does it work?](graphics/B05137_04_021.jpg)。
    2.  将分类器模型 *f* 应用于未标记的数据 *U* 以评估信息性 *J* 使用一种采样机制(参见下一节)
    3.  从 *U* 中选择 *k* 最具信息性的数据作为集合 *L* [u] 以从 oracle 中获取标签。
    4.  用上一步得到的 *k* 个新的已标记数据点对已标记数据进行扩充:*L*=*L*∩*L*[u]。

    
3.  重复 2 下的所有步骤。

下面给出了从数据中抽取信息实例的一些最常见的查询合成算法。

##### 最小置信抽样

在这种技术中，数据实例基于它们的置信度反向排序，最有可能被查询或选择的实例是模型最不信任的实例。这背后的想法是，最不自信的是那些靠近边缘或分离超平面的，获得它们的标签将是有效学习边界的最佳方式。

这可以公式化为![Least confident sampling](graphics/B05137_04_088.jpg)。

这种方法的缺点是它有效地考虑了最佳信息；没有使用关于其余后验分布的信息。

##### 最小间隔抽样

这个是基于边缘的采样，其中具有较小边缘的实例比具有较大边缘的实例具有更多模糊性。

这可以表述为![Smallest margin sampling](graphics/B05137_04_089.jpg)，其中![Smallest margin sampling](graphics/B05137_04_090.jpg)和![Smallest margin sampling](graphics/B05137_04_091.jpg)是实例 **x** 的两个标签。

##### 标签熵采样

熵,是数据中平均信息量的度量，也是杂质度量，可以用来对实例进行采样。这可以表述为:

![Label entropy sampling](graphics/B05137_04_092.jpg)

#### 优点和局限性

优点和局限性是:

*   熵抽样是最简单的方法，可以和任何概率分类器一起工作——这是最大的优势
*   离群值或错误反馈的存在可能会被忽视，并且模型可能会降级

## 版本空间采样

假设 *H* 是概括或解释训练数据的所有特定模型的集合；例如，分隔两个线性可分类的所有可能的权重集。版本空间 *V* 是假设 *H* 的子集，其与 Tom Mitchell ( *引用*【15】)定义的训练数据一致，使得![Version space sampling](graphics/B05137_04_095.jpg)。

这种采样背后的思想是从未标记的数据集中查询实例，以减少版本空间的大小或最小化| *V* |。

### 通过异议查询(QBD)

QBD是最早的算法之一，它致力于维护版本空间*V*——当两个假设在新输入数据的标签上不一致时，该实例被选择用于从 oracle 或专家那里获取标签。

#### 它是如何工作的？

整个算法可以总结如下:

1.  初始化![How does it work?](graphics/B05137_04_095.jpg)为所有法律假设的集合。
2.  将数据初始化为![How does it work?](graphics/B05137_04_015.jpg)标记和![How does it work?](graphics/B05137_04_017.jpg)未标记。
3.  而数据**x***[']'*则在 *U* :

    1.  如果![How does it work?](graphics/B05137_04_099.jpg)为任意*h*[2]∈*V*:

        1.  查询 **x** *[']* 的标签，得到*y[']*
        2.  *V*= { h:h(**x**['])=*y[']*为所有点。

    2.  否则:

        1.  忽略 **x** *[']* 。

##### 委员会查询(QBC)

委员会查询通过创建分类器委员会并使用它们的投票作为捕获不一致的机制，克服了与维护所有可能的版本空间相关的不一致查询的限制(*参考文献*【7】)。

#### 它是如何工作的？

对于该算法:

1.  将数据初始化为![How does it work?](graphics/B05137_04_015.jpg)标记和![How does it work?](graphics/B05137_04_017.jpg)未标记。
2.  培养模特委员会*C*= {*θ*¹*θ*²，... *θ* ^c }上标注的数据 *w* (见下文)。
3.  对于所有数据**x***'T5*U*:

    1.  对 **x** *'* 的预测投票为{ ![How does it work?](graphics/B05137_04_107.jpg)。
    2.  根据最大差异对实例进行排序(见下文)。
    3.  从 *U* 中选择 *k* 最翔实的数据作为集合 *L* [u] 从 oracle 中获取标签。
    4.  用 *k* 新标记的数据点*L*=*L*∩*L*[u]扩充标记的数据。
    5.  重新培训模特{ *θ* [1] ， *θ* [2] ，...*θ*c}带新 *L* 。* 

对于训练学习者委员会和选择分歧方法这两项任务，每个人都有不同的选择。

可以使用来自 *L* 的不同样本来训练不同的模型，或者可以使用诸如 boosting 和 bagging 之类的集合方法来训练它们。

投票熵是选择作为不一致度量来排序的方法之一。它的数学表达方式是:

![How does it work?](graphics/B05137_04_109.jpg)

这里， *V(y [i] )* 是从所有可能的标签中给予标签*y*I 的票数，而| *C* |是委员会的规模。

**kull back-lei bler**(**KL**)散度是两个概率分布之间散度的信息论度量。分歧被量化为每个委员会的预测与委员会一致意见的平均差异 *C* :

![How does it work?](graphics/B05137_04_114.jpg)

## 优点和局限性

优点和局限性如下:

*   简单性和它可以与任何监督算法一起工作的事实给了它很大的优势。
*   在某些情况下，理论上可以保证误差最小化和推广。
*   由不一致引起的查询受到维护大量有效假设的困扰。
*   这些方法仍然存在错误反馈被忽视和模型可能降级的问题。

## 数据分布抽样

以前的方法要么基于模型上的样本造成的不确定性，要么通过减少假设空间大小，从未标记的集合中选择最佳实例。这两种方法都不能解决模型本身的最佳问题。数据分布抽样背后的想法是，添加有助于减少模型误差的样本有助于使用期望值改进对未知实例的预测(*参考文献*【13 和 14】)。

### 它是如何工作的？

有不同的方法可以找到给定模型的最佳样本，这里我们将详细描述每一种方法。

#### 预期的车型变化

这背后的想法是从未标记的集合中选择将在模型中带来最大变化的示例:

![Expected model change](graphics/B05137_04_115.jpg)

这里，P[θ](*y*|**x**)=对 **x** ，![Expected model change](graphics/B05137_04_117.jpg)的标签的期望是用 **x** 再训练后包含 **x** *'* 的熵对未标记实例的和。

#### 预期误差减少

在这里，方法是从未标记的集合中选择最能减少模型广义误差的例子。使用具有预期标签的未标记集合来测量广义误差:

![Expected error reduction](graphics/B05137_04_119.jpg)

这里，Pθ ( *y* | **x** ) =对 **x** ，![Expected error reduction](graphics/B05137_04_120.jpg)的标签的期望，是用 **x** 再训练后包含 x *^( ' )* 的熵的未标记实例的和。

##### 方差减少

根据噪声偏差方差估计样本外误差的一般公式如下:

![Variance reduction](graphics/B05137_04_121.jpg)

这里， **G** ( **x** )是给定标签 *y* 的模型预测。在方差减少中，我们从未标记的集合中选择最能减少模型中方差的示例:

![Variance reduction](graphics/B05137_04_124.jpg)

这里， *θ* +表示模型经过新的点 **x** *^'* 及其标签 *y ^'* 重新训练后的模型。

##### 密度加权法

在这种方法中，我们从未标记的集合中选择与标记的集合具有平均相似性的例子。

这可以表示如下:

![Density weighted methods](graphics/B05137_04_127.jpg)

这里， *sim* ( **x** ，**x***^’*)是密度项或相似性项，其中 H[θ](*y*|**x**)是基本效用度量。

## 优点和局限性

的优点和局限性如下:

*   最大的优势是它们直接将模型作为优化目标，而不是之前描述的隐式或间接方法
*   这些方法可以在基于池或基于流的场景中工作
*   这些方法在界限和推广上有一些理论保证
*   这些方法的最大缺点是计算成本和实施困难



# 主动学习案例研究

这个案例研究使用另一个众所周知的公开可用数据集来演示使用开源 Java 库的主动学习技术。像以前一样，我们从定义业务问题开始，使用什么工具和框架，如何在解决方案中实现机器学习的原则，以及数据分析步骤揭示了什么。接下来，我们描述进行的实验，评估各种模型的性能，并提供结果分析。

## 工具和软件

在主动学习的实验中，JCLAL 是使用的工具。JCLAL 是一个面向主动学习的 Java 框架，支持单标签和多标签学习。

### 注意

JCLAL 是开源的，在 GNU 通用公共许可证下发布:[https://sourceforge.net/p/jclal/git/ci/master/tree/](https://sourceforge.net/p/jclal/git/ci/master/tree/)。

## 商业问题

这些实验中使用的鲍鱼数据集包含鲍鱼(通常称为海螺)的各种物理和解剖特征的数据。目标是预测贝壳年轮的数量，这表明了标本的年龄。

## 机器学习映射

正如我们已经看到的，主动学习的特点是从一小组标记数据开始，伴随着查询未标记数据的技术，这样我们就可以逐渐地向标记集添加实例。这是通过多次迭代执行的，一次一批。迭代次数和批量大小是这些技术的超参数。用于在不断增长的标记实例数量上进行训练的查询策略和监督学习方法的选择是额外的输入。

## 数据收集

和以前一样，我们将使用来自 https://archive.ics.uci.edu/ml/datasets/Abalone 的 UCI 知识库的现有数据集。数据库的原始所有者是澳大利亚塔斯马尼亚岛的初级工业和渔业部。

数据类型和属性描述伴随数据，并在*表 5* 中再现。类属性 Rings 有 29 个不同的类:

| 

名字

 | 

数据类型

 | 

度量单位

 | 

描述

 |
| --- | --- | --- | --- |
| 性 | 名义上的 | 男，女，和我(婴儿) | 标本性别 |
| 长度 | 连续的 | 毫米 | 最长外壳尺寸 |
| 直径 | 连续的 | 毫米 | 垂直于长度 |
| 高度 | 连续的 | 毫米 | 带壳的肉 |
| 整体重量 | 连续的 | 克 | 全鲍鱼 |
| 震惊重量 | 连续的 | 克 | 肉的重量 |
| 内脏重量 | 连续的 | 克 | 内脏重量(出血后) |
| 外壳重量 | 连续的 | 克 | 干燥后 |
| 戒指 | 整数 | 数数 | +1.5 给出以年为单位的年龄 |

> *表 5。鲍鱼数据集特征*

## 数据采样和转换

对于这个实验，我们将随机选择的【4,155 条记录视为未标记，而将剩余的 17 条记录保留为标记。没有数据的转换。

## 特征分析和降维

在只有八个特征的情况下，不需要进行降维。数据集附带了一些关于特征的统计数据，如*表 6* 所示:

|   | **长度** | **直径** | **高度** | **整体** | **剥壳** | **内脏** | **外壳** | **铃声** |
| 福建话 | Zero point zero seven five | Zero point zero five five | Zero | Zero point zero zero two | Zero point zero zero one | Zero point zero zero one | Zero point zero zero two | one |
| 最大 | Zero point eight one five | Zero point six five | One point one three | Two point eight two six | One point four eight eight | Zero point seven six | One point zero zero five | Twenty-nine |
| 平均 | Zero point five two four | Zero point four zero eight | Zero point one four | Zero point eight two nine | Zero point three five nine | Zero point one eight one | Zero point two three nine | Nine point nine three four |
| 南达科他州 | Zero point one two | Zero point zero nine nine | Zero point zero four two | Zero point four nine | Zero point two two two | Zero point one one | Zero point one three nine | Three point two two four |
| 科雷尔 | Zero point five five seven | Zero point five seven five | Zero point five five seven | Zero point five four | Zero point four two one | Zero point five zero four | Zero point six two eight | one |

> *表 6。按功能汇总统计数据*

## 模型、结果和评估

我们进行了两组实验。第一个使用基于池的场景，第二个使用基于流的场景。在每个集合中，我们使用熵抽样、最小置信抽样、边际抽样和投票熵抽样。使用的分类器是朴素贝叶斯、逻辑回归和 J48(c 4.5 的实现)。对于每个实验，运行 100 次迭代，批量大小为 1 和 10。在*表 7* 中，我们展示了这些结果的一个子集，特别是使用朴素贝叶斯、简单逻辑和 C4.5 分类器(批量为 10)的每种采样方法的基于池和基于流的场景。

### 注意

全套结果可以在[https://github . com/mjmlbook/mastering-Java-machine-learning/tree/master/chapter 4](https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter4)上看到。

JCLAL 库需要一个 XML 配置文件来指定使用哪个场景、选择的查询策略、批处理大小、最大迭代次数和基本分类器。以下是一个配置示例:

```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<experiment>
    <process evaluation-method-type="net.sf.jclal.evaluation.method.RealScenario">
        <file-labeled>datasets/abalone-labeled.arff</file-labeled>
        <file-unlabeled>datasets/abalone-unlabeled.arff</file-unlabeled>    
        <algorithm type="net.sf.jclal.activelearning.algorithm.ClassicalALAlgorithm">
      <stop-criterion type="net.sf.jclal.activelearning.stopcriteria.MaxIteration">
              <max-iteration>10</max-iteration>	
      </stop-criterion>
      <stop-criterion type="net.sf.jclal.activelearning.stopcriteria.UnlabeledSetEmpty"/>
            <listener type="net.sf.jclal.listener.RealScenarioListener">
                <informative-instances>reports/real-scenario-informative-data.txt</informative-instances>
            </listener>
            <scenario type="net.sf.jclal.activelearning.scenario.PoolBasedSamplingScenario">
                <batch-mode type="net.sf.jclal.activelearning.batchmode.QBestBatchMode">
                    <batch-size>1</batch-size>
                </batch-mode>
                <oracle type="net.sf.jclal.activelearning.oracle.ConsoleHumanOracle"/>
               <query-strategy type="net.sf.jclal.activelearning.singlelabel.querystrategy.EntropySamplingQueryStrategy">
                    <wrapper-classifier type="net.sf.jclal.classifier.WekaClassifier">
                        <classifier type="weka.classifiers.bayes.NaiveBayes"/>
                    </wrapper-classifier>
                </query-strategy>
            </scenario>
        </algorithm>
    </process>
</experiment>
```

工具本身通过以下方式调用:

```
java -jar jclal-<version>.jar -cfg <config-file>

```

### 基于池的场景

在下面的三个表格中，我们比较了使用朴素贝叶斯、简单逻辑和 C4.5 分类器时基于池的场景的结果。

**朴素贝叶斯:**

| 

实验

 | 

ROC 下的区域

 | 

f 测度

 | 

假阳性率

 | 

精确

 | 

回忆

 |
| --- | --- | --- | --- | --- | --- |
| 基于池的-EntropySampling-naive Bayes-b10 | 0.6021 | 0.1032 | 0.0556(1) | 0.1805 | 0.1304 |
| 基于池的-KLDivergence-NaiveBayes-b10 | 0.6639(1) | 0.1441(1) | 0.0563 | 0.1765 | 0.1504 |
| 基于池的-最小确认采样-朴素贝叶斯-b10 | 0.6406 | 0.1300 | 0.0827 | 0.1835(1) | 0.1810(1) |
| 基于池的-VoteEntropy-NaiveBayes-b10 | 0.6639(1) | 0.1441(1) | 0.0563 | 0.1765 | 0.1504 |

> *表 7。使用朴素贝叶斯分类器的基于池的场景的性能*

**逻辑回归**:

| 

实验

 | 

ROC 下的区域

 | 

f 测度

 | 

假阳性率

 | 

精确

 | 

回忆

 |
| --- | --- | --- | --- | --- | --- |
| 基于池的-EntropySampling-简单逻辑-b10 | 0.6831 | 0.1571 | 0.1157 | 0.1651 | 0.2185(1) |
| 基于池的-KL divergence-简单逻辑-b10 | 0.7175(1) | 0.1616 | 0.1049 | 0.2117(1) | 0.2065 |
| 基于池-最小确认采样-简单逻辑-b10 | 0.6629 | 0.1392 | 0.1181(1) | 0.1751 | 0.1961 |
| 基于池的-VoteEntropy-simple logistic-b10 | 0.6959 | 0.1634(1) | 0.0895 | 0.2307 | 0.1880 |

> *表 8。使用逻辑回归分类器的基于池的场景的性能*

**C4.5** :

| 

实验

 | 

ROC 下的区域

 | 

f 测度

 | 

假阳性率

 | 

精确

 | 

回忆

 |
| --- | --- | --- | --- | --- | --- |
| 基于池的-EntropySampling-J48-b10 | 0.6730(1) | 0.3286(1) | 0.0737 | 0.3432(1) | 0.32780(1) |
| 基于池-KLDivergence-J48-b10 | 0.6686 | 0.2979 | 0.0705(1) | 0.3153 | 0.2955 |
| 基于池的-LeastConfidentSampling-J48-b10 | 0.6591 | 0.3094 | 0.0843 | 0.3124 | 0.3227 |
| 基于池的-VoteEntropy-J48-b10 | 0.6686 | 0.2979 | 0.0706 | 0.3153 | 0.2955 |

> *表 9。使用 C4.5 分类器的基于池的场景的性能*

### 基于流的场景

在下面的三个表中，我们使用朴素贝叶斯、逻辑回归和 C4.5 分类器，通过四种不同的采样方法，获得了基于流的场景的实验结果。

朴素贝叶斯:

| 

实验

 | 

ROC 下的区域

 | 

f 测度

 | 

假阳性率

 | 

精确

 | 

回忆

 |
| --- | --- | --- | --- | --- | --- |
| stream based-EntropySampling-naive Bayes-b10 | 0.6673(1) | 0.1432(1) | 0.0563 | 0.1842(1) | 0.1480 |
| 基于流-最小确认采样-朴素贝叶斯-b10 | 0.5585 | 0.0923 | 0.1415 | 0.1610 | 0.1807(1) |
| 基于流-边缘采样-朴素贝叶斯-b10 | 0.6736(1) | 0.1282 | 0.0548(1) | 0.1806 | 0.1475 |
| 基于流的 VoteEntropyQuery-naive Bayes-b10 | 0.5585 | 0.0923 | 0.1415 | 0.1610 | 0.1807(1) |

> *表 10。使用朴素贝叶斯分类器的基于流的场景的性能*

**逻辑回归**:

| 

实验

 | 

ROC 下的区域

 | 

f 测度

 | 

假阳性率

 | 

精确

 | 

回忆

 |
| --- | --- | --- | --- | --- | --- |
| stream based-EntropySampling-simple logistic-b10 | 0.7343(1) | 0.1994(1) | 0.0871 | 0.2154 | 0.2185(1) |
| stream based-least confident sampling-simple logistic-b10 | 0.7068 | 0.1750 | 0.0906 | 0.2324(1) | 0.2019 |
| 基于流-边缘采样-简单逻辑-b10 | 0.7311 | 0.1994(1) | 0.0861 | 0.2177 | Zero point two one four |
| stream based-VoteEntropy-simple logistic-b10 | 0.5506 | 0.0963 | 0.0667(1) | 0.1093 | 0.1117 |

> *表 11。使用逻辑回归分类器的基于流的场景的性能*

**C4.5** :

| 

实验

 | 

ROC 下的区域

 | 

f 测度

 | 

假阳性率

 | 

精确

 | 

回忆

 |
| --- | --- | --- | --- | --- | --- |
| stream based-EntropySampling-J48-b10 | 0.6648 | 0.3053 | 0.0756 | 0.3189(1) | 0.3032 |
| stream based-least confident sampling-J48-b10 | 0.6748(1) | 0.3064(1) | 0.0832 | 0.3128 | 0.3189(1) |
| 基于流-余量取样-J48-b10 | 0.6660 | 0.2998 | 0.0728(1) | 0.3163 | 0.2967 |
| stream based-VoteEntropy-J48-b10 | 0.4966 | 0.0627 | 0.0742 | 0.1096 | 0.0758 |

> *表 12。使用 C4.5 分类器的基于流的场景的性能*

## 主动学习结果分析

有趣的是，基于池的委员会查询(一种集成方法)使用 KL-Divergence 采样在大多数分类器中表现良好。如本节所述，这些方法已被证明具有通过保持较大假设空间来减少误差的理论保证，并且该实验结果从经验上支持了这一点。

使用 C4.5 作为分类器的基于池、基于熵的采样具有最高的精度、召回率、FPR 和 F-Measure。同样，对于基于流的熵采样和 C4.5，指标也很高。使用不同的采样技术和基于池的 C4.5，如 KL-Divergence、LeastConfident 或 vote entropy，指标明显更高。因此，这可以更强烈地归因于底层分类器 C4.5 寻找非线性模式。

当考虑 AUC 时，逻辑回归算法在基于流和基于池两者中表现非常好。这可能完全是因为 LR 在置信度作图中具有良好的概率方法，这是给出良好 AUC 分数的重要因素。



# 总结

在前几章中浏览了监督和非监督机器学习技术及其在真实世界数据集上的应用之后，本章介绍了**半监督学习** ( **SSL** )和**主动学习** ( **AL** )的概念、技术和工具。

在 SSL 中，我们有一些带标签的示例和许多未标记的示例，目标是简单地在带标签的示例上进行训练，以便对未标记的示例进行分类(直推式 SSL)，或者使用未标记和带标签的示例来训练模型，以便对新的、看不见的数据进行正确分类(归纳式 SSL)。SSL 中的所有技术都基于一个或多个与半监督平滑度、集群一致性和流形一致性相关的假设。

不同的 SSL 技术适用于不同的情况。简单的自训练 SSL 是直接的，并且与大多数监督学习算法一起工作；当数据来自不止一个域时，协同训练 SSL 是一种合适的方法。当集群一致性假设成立时，可以使用集群和标签 SSL 技术；直推图标签传播利用了“接近度”测量，这在计算上可能是昂贵的。直推式 SVM 对线性或非线性数据表现良好，我们看到一个使用`JKernelMachines`库在 UCI 乳腺癌数据集上训练具有高斯核的 TSVM 的例子。在本章 SSL 部分的结论部分，我们使用图形化 Java 工具 KEEL 展示了比较 SSL 模型的实验。

我们在本章的后半部分介绍了主动学习。在这种类型的学习中，使用各种策略来查询数据集的未标记部分，以便向专家呈现将证明在从整个数据集学习中最有效的示例。随着专家或甲骨文向选定的实例提供标签，学习者稳步提高其概括能力。人工智能技术的特征在于分类器的选择，或者分类器委员会，更重要的是，在于所选择的查询策略。这些策略包括不确定性抽样，其中置信度最低的实例是查询；版本抽样，其中选择解释训练数据的假设子集；以及数据分布抽样，其中涉及通过减少泛化错误的选择来改进模型。我们提出了一个使用 UCI 鲍鱼数据集的案例研究，以展示实践中的主动学习。这里使用的工具是用于主动学习的 JCLAL Java 框架。



# 参考文献

1.  亚罗斯基博士(1995 年)。无监督的词义消歧对抗监督的方法。计算语言学协会第 33 届年会会议录(第 189-196 页)
2.  布卢姆和米切尔(1998 年)。*通过联合训练将标记和未标记的数据结合起来*。计算学习理论研讨会论文集。
3.  Demiriz，a .，Bennett，k .和 Embrechts，M .(1999 年)。*使用遗传算法的半监督聚类*。工程中的人工神经网络。
4.  Yoshua Bengio，Olivier Delalleau，Nicolas Le Roux (2006 年)。*标签传播和二次准则*。《半监督学习》第 193-216 页
5.  T.约阿希姆(1998 年)。*使用支持向量机进行文本分类的直推推理*，ICML。
6.  B.已解决(2008 年)。*好奇的机器:结构化实例的主动学习*。威斯康星大学麦迪逊分校博士论文。
7.  D.安格鲁因(1988)。*查询和概念学习*。机器学习，2:319–342。
8.  D.刘易斯和 w .盖尔(1994 年)。*训练文本分类器的序贯算法*。美国计算机学会 SIGIR 信息检索研究与发展会议论文集，第 3-12 页。ACM/Springer。
9.  韩升洙、奥珀和宋波林斯基(1992)。*委员会查询*。《美国计算机学会计算学习理论研讨会论文集》, 287-294 页。
10.  D.科恩、l .阿特拉斯、r .拉德纳、M. El-Sharkawi、R. Marks II、M. Aggoune 和 D. Park (1992 年)。*用查询和选择性采样训练连接主义者网络*。神经信息处理系统进展。摩根·考夫曼。
11.  D.科恩，l .阿特拉斯和 r .拉德纳(1994 年)。*主动学习提高概括能力*。机器学习，15(2):201–221。
12.  D.刘易斯和 j .卡特莱特(1994 年)。*用于监督学习的异质不确定性采样*。《机器学习国际会议论文集》(ICML)，第 148–156 页。摩根·考夫曼。
13.  南 Dasgupta、A. Kalai 和 C. Monteleoni (2005 年)。*基于感知器的主动学习分析*。《学习理论会议录》，第 249-263 页。斯普林格。
14.  南 Dasgupta、D. Hsu 和 C. Monteleoni (2008 年)。*一种通用的不可知主动学习算法*。《神经信息处理系统(NIPS)进展》,第 20 卷，第 353–360 页。麻省理工出版社。
15.  T.米切尔(1982)。*概括为搜索*。人工智能，18:203–226。*