

# 检测词性

以前，我们识别文本的各个部分，如人物、地点和事物。在这一章中，我们将考察寻找**词性** ( **词性**)的过程。这些是我们在英语中认为是语法元素的部分，如名词和动词。我们会发现，单词的上下文是决定它是什么类型单词的一个重要方面。

我们将研究标记过程，它本质上是将一个 POS 分配给一个标记。这个过程是检测 POS 的核心。我们将简要讨论为什么标记是重要的，然后检查使检测词性变得困难的各种因素。各种**自然语言处理**(**NLP**)API 随后被用来说明标记过程。我们还将演示如何训练一个模型来处理专门的文本。

我们将在本章中讨论以下主题:

*   标记过程
*   使用 NLP APIs



# 标记过程

标记是将描述分配给令牌或部分文本的过程。这种描述称为标签。位置标记是将位置标记分配给令牌的过程。这些标签通常是语法标签，如名词、动词和形容词。例如，考虑下面的句子:

"奶牛跳过了月亮。"

对于这些初始示例中的许多示例，我们将说明 POS tagger 使用 OpenNLP tagger 的结果，这将在本章后面的*使用 OpenNLP POS tagger*一节中讨论。如果我们在前面的例子中使用这个标记符，我们将得到下面的结果。注意，单词后面是一个正斜杠，然后是它们的 POS 标签。这些标签将很快得到解释:

```
 The/DT cow/NN jumped/VBD over/IN the/DT moon./NN
```

根据上下文，单词可能有多个相关联的标签。例如，单词 *saw* 可以是名词，也可以是动词。当一个单词可以被分类成不同的类别时，诸如其位置、其附近的单词或类似信息的信息被用于概率性地确定适当的类别。例如，如果一个单词前面是限定词，后面是名词，那么将这个单词标记为形容词。

一般的标记过程包括标记文本、确定可能的标记和解决不明确的标记。算法用于执行位置识别(标记)。有两种通用方法:

*   **基于规则的**:基于规则的标签使用一组规则，以及一个单词和可能标签的字典。当一个单词有多个标签时，使用这些规则。规则通常使用前面和/或后面的单词来选择标签。
*   随机标签:随机标签要么基于马尔可夫模型，要么基于线索，要么使用决策树，要么使用最大熵。马尔可夫模型是有限状态机，其中每个状态有两种概率分布。它的目标是为一个句子找到最佳的标签序列。**隐马尔可夫模型** ( **HMM** )也有使用。在这些模型中，状态转换是不可见的。

最大熵标记器使用统计学来确定单词的词性，并且经常使用语料库来训练模型。语料库是用词性标签标注的单词的集合。许多语言都有语料库。这些都需要很大的努力去开发。常用的语料库有宾夕法尼亚树库[(https://www.seas.upenn.edu/~pdtb//](https://www.seas.upenn.edu/~pdtb/))或布朗语料库([http://www . Essex . AC . uk/linguistics/external/clmt/W3C/Corpus _ ling/content/corpora/list/private/Brown/Brown . html](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html))。

来自 Penn Treebank 语料库的示例说明了 POS 标记，如下所示:

```
    Well/UH what/WP do/VBP you/PRP think/VB about/IN
    the/DT idea/NN of/IN ,/, uh/UH ,/, kids/NNS having/VBG
    to/TO do/VB public/JJ service/NN work/NN for/IN a/DT
    year/NN ?/.
```

英语传统上有九种词类:名词、动词、冠词、形容词、介词、代词、副词、连词和感叹词。然而，更完整的分析通常需要额外的类别和子类别。已经发现了多达 150 种不同的词类。在某些情况下，可能需要创建新的标签。下表列出了一个简短的列表。这些是我们将在本章中经常使用的标签:

| **部分** | **意为** |
| 神经网络 | 名词，单数，还是复数 |
| 暗行扫描(Dark Trace) | 限定词 |
| 动词 | 动词，基本形式 |
| VBD | 动词，过去式 |
| VBZ | 动词，第三人称单数现在时 |
| 在…里 | 介词或从属连词 |
| NNP | 专有名词，单数 |
| 到 | 到 |
| 姐姐(网络用语)ˌ法官ˌ裁判员(judges) | 形容词 |

下表显示了更全面的列表。本榜单改编自[https://www . ling . upenn . edu/courses/Fall _ 2003/ling 001/Penn _ tree bank _ pos . html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)。宾夕法尼亚大学(Penn)树库标签集的完整列表可以在 http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html[找到。一组标签被称为*标签集*:](http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html)

| **标签** | **描述** | **标签** | **描述** |
| --- | --- | --- | --- |
| 抄送 | 并列连词 | PRP 元 | 所有格代名词 |
| 激光唱片 | 基数 | 铷 | 副词 |
| 暗行扫描(Dark Trace) | 限定词 | RBR | 副词，比较 |
| 前妻；前夫 | 存在主义 | 随机阻塞系统（Random Barrage System 的缩写） | 副词，最高级 |
| 转发 | 外来词 | 菲律宾共和国 | 颗粒 |
| 在…里 | 介词或从属连词 | 符号 | 标志 |
| 姐姐(网络用语)ˌ法官ˌ裁判员(judges) | 形容词 | 到 | 到 |
| JJR | 形容词，比较级 | 哦 | 感叹词 |
| JJS | 形容词，最高级 | 动词 | 动词，基本形式 |
| 莱索托 | 列表项目标记 | VBD | 动词，过去式 |
| 医学博士 | 情态的 | VBG | 动词、动名词或现在分词 |
| 神经网络 | 名词，单数，还是复数 | VBN | 动词，过去分词 |
| NNS | Noun, plural | VBP | 动词，非第三人称单数现在时 |
| NNP | 专有名词，单数 | VBZ | 动词，第三人称单数现在时 |
| NNPS | 专有名词，复数 | 禁水试验 | 疑问限定词 |
| 太平洋夏季时间 | 前限定词 | 文字处理 | 疑问代词 |
| 刷卡机 | 所有格结尾 | WP$ | 所有格 wh 代词 |
| 富含血小板血浆 | 人称代词 | 战时难民事务委员会（War Refugee Board） | 疑问副词 |

人工语料库的开发是劳动密集型的。然而，已经开发了一些统计技术来创建语料库。许多语料库是可用的。第一个是布朗文集([http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM](http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM))。较新的包括超过 1 亿字的英国国家语料库(【http://www.natcorp.ox.ac.uk/corpus/index.xml】[)和美国国家语料库(](http://www.natcorp.ox.ac.uk/corpus/index.xml)[【http://www.anc.org/】](http://www.anc.org/))。



# 张贴标签的重要性

对句子进行正确的标注可以提高后续处理任务的质量。如果我们知道 *sue* 是动词而不是名词，那么这可以帮助建立标记之间的正确关系。确定词性、短语、从句以及它们之间的任何关系被称为*解析*。这与标记化相反，在标记化中，我们只对识别*单词*元素感兴趣，而不关心它们的意思。

词性标注用于许多下游过程，例如问题分析和分析文本的情感。一些社交媒体网站经常对评估客户交流的情绪感兴趣。文本索引将频繁使用 POS 数据。语音处理可以使用标签来帮助决定如何发音。



# POS 难的原因是什么？

一门语言的许多方面都会使词性标注变得困难。大多数英语单词都有两个或更多的相关标签。字典并不总是足以确定一个单词的位置。例如，像*比尔*和*力量*这样的词的意思取决于它们的上下文。下面的句子演示了它们如何在同一个句子中作为名词和动词使用。

“比尔用暴力迫使经理将账单撕成两半。”

将 OpenNLP 标记符与此语句一起使用会产生以下输出:

```
    Bill/NNP used/VBD the/DT force/NN to/TO force/VB the/DT manger/NN to/TO tear/VB the/DT bill/NN in/IN two./PRP$
```

textese 是不同文本形式的组合，包括缩写、标签、表情符号和俚语，在 tweets 和 text 等通信媒体中的使用使得标记句子变得更加困难。例如，下面的消息很难标记:

“阿发克她已经死了！顺便说一句，我们在 BBIAM 聚会上玩得很开心。”

它的对等词是:

“据我所知，她讨厌打扫房子！顺便说一句，我在聚会上玩得很开心。一会儿就回来。”

使用 OpenNLP 标记器，我们将获得以下输出:

```
    AFAIK/NNS she/PRP H8/CD cth!/.
    BTW/NNP had/VBD a/DT GR8/CD tym/NN at/IN the/DT party/NN BBIAM./.
```

在本章后面的*使用 MaxentTagger 类标记 textese* 一节中，我们将演示 LingPipe 如何处理 textese。下表给出了常见文本术语的简短列表:

| **短语** | **文字** | **短语** | **文字** |
| 据我所知 | 就我所知 | 顺便问一下 | 顺便说一下 |
| 远离键盘 | 离开键盘的 | 你只能靠自己了 | 游乐儿 |
| 谢谢 | THNX 或 THX | 尽快 | 尽快(As Soon As Possible) |
| 今天 | 2 天 | 你这话是什么意思 | WDYMBT |
| 以前 | B4 | 马上回来 | BBIAM |
| 再见 | C U | 不可以 | （cannot）不能 |
| 哈哈的笑 | 倍硬 | 后来 | l8R |
| 大声笑出来 | 英雄联盟 | 另一方面 | 另一方面 |
| 笑得在地上打滚 | 罗夫尔还是 ROTFL | 我不知道 | 我不知道 |
| 伟大的 | GR8 | 打扫房子 | 三己糖酰基鞘氨醇 |
| 此刻 | 异步传输模式 | 管见所及 | 恕我直言 |

There are several lists of textese; a large list can be found at
[http://www.ukrainecalling.com/textspeak.aspx](http://www.ukrainecalling.com/textspeak.aspx).

标记化是词性标注过程中的一个重要步骤。如果标记没有正确分割，我们可能会得到错误的结果。还有其他几个潜在的问题，包括:

*   如果我们使用小写字母，那么像*山姆*这样的词可能会与奖励管理的人或系统(【www.sam.gov】T2)混淆
*   我们必须考虑到缩写，如*不能*，并认识到撇号可以使用不同的字符
*   虽然像*反之亦然*这样的短语可以被视为一个单位，但它已经被用于英格兰的一个乐队、一部小说的标题和一本杂志的标题
*   我们不能忽视用连字符连接的单词，如*首切*和*首切*，它们的意思不同于它们各自的用法
*   有些单词嵌入了数字，比如 iPhone 5S
*   还需要处理 URL 或电子邮件地址等特殊字符序列

一些单词被发现嵌入在引号或括号中，这可能会使它们的意思混淆。考虑下面的例子:

"“蓝色”是否正确(事实并非如此)是有争议的。"

“蓝色”可以指蓝色，也可以是一个人的昵称。
这个句子的标签输出如下:

```
Whether/IN "Blue"/NNP was/VBD correct/JJ or/CC not/RB (it's/JJ not)/NN is/VBZ debatable/VBG
```



# 使用 NLP APIs

我们将使用 OpenNLP、Stanford API 和 LingPipe 演示词性标注。每个例子都会用到下面的句子。这是儒勒·凡尔纳的《海底两万里》中第五章的第一句话，出自《冒险的 T0》和《T2》。

```
private String[] sentence = {"The", "voyage", "of", "the",  
    "Abraham", "Lincoln", "was", "for", "a", "long", "time", "marked",  
    "by", "no", "special", "incident."};
```

要处理的文本可能不总是以这种方式定义。有时，句子会以单个字符串的形式出现:

```
String theSentence = "The voyage of the Abraham Lincoln was for a "  
    + "long time marked by no special incident.";
```

我们可能需要将一个字符串转换成一个字符串数组。有许多技术可以将这个字符串转换成单词数组。以下`tokenizeSentence`方法执行该操作:

```
public String[] tokenizeSentence(String sentence) { 
    String words[] = sentence.split("S+"); 
    return words; 
}
```

下面的代码演示了此方法的用法:

```
String words[] = tokenizeSentence(theSentence); 
for(String word : words) { 
    System.out.print(word + " ");  
} 
System.out.println(); 
```

输出如下所示:

```
The voyage of the Abraham Lincoln was for a long time marked by no special incident.
```

或者，我们可以使用一个标记器，比如 OpenNLP 的`WhitespaceTokenizer`类，如下所示:

```
String words[] = 
     WhitespaceTokenizer.INSTANCE.tokenize(sentence);
```



# 使用 OpenNLP 位置标签

OpenNLP 提供了几个支持词性标注的类。我们将演示如何使用`POSTaggerME`类来执行基本的标记，以及如何使用`ChunkerME`类来执行分块。组块包括根据相关单词的类型对它们进行分组。这可以提供对句子结构的额外洞察。我们还将研究一个`POSDictionary`实例的创建和使用。



# 为 POS 标记器使用 OpenNLP POSTaggerME 类

OpenNLP `POSTaggerME`类使用最大熵来处理标签。
tagger 根据单词本身和单词的上下文来确定标签的类型。任何给定的单词都可能有多个相关联的标签。标记器使用概率模型来确定要分配的特定标记。

POS 模型是从文件中加载的。`en-pos-maxent.bin`模型经常被使用，它基于 Penn TreeBank 标签集。在[http://opennlp.sourceforge.net/models-1.5/](http://opennlp.sourceforge.net/models-1.5/)可以找到 OpenNLP 的各种预训练 POS 模型。

我们从一个 try-catch 块开始，处理加载模型时可能产生的任何`IOException`,如下所示。

我们将`en-pos-maxent.bin`文件用于模型:

```
try (InputStream modelIn = new FileInputStream( 
    new File(getModelDir(), "en-pos-maxent.bin"));) { 
    ... 
} 
catch (IOException e) { 
    // Handle exceptions 
} 
```

接下来，创建`POSModel`和`POSTaggerME`实例，如下所示:

```
POSModel model = new POSModel(modelIn); 
POSTaggerME tagger = new POSTaggerME(model); 
```

`tag`方法现在可以应用于 tagger，使用要处理的文本作为它的参数:

```
String tags[] = tagger.tag(sentence); 
```

然后显示单词及其标签，如下所示:

```
for (int i = 0; i<sentence.length; i++) { 
    System.out.print(sentence[i] + "/" + tags[i] + " "); 
} 
```

输出如下。每个单词后面都有它的类型:

```
The/DT voyage/NN of/IN the/DT Abraham/NNP Lincoln/NNP was/VBD for/IN a/DT long/JJ time/NN marked/VBN by/IN no/DT special/JJ incident./NN
```

对于任何一个句子，都可能有不止一个可能的标签分配给单词。`topKSequences`方法将根据正确的概率返回一组序列。在下面的代码序列中，使用`sentence`变量执行`topKSequences`方法，然后显示:

```
Sequence topSequences[] = tagger.topKSequences(sentence); 
for (inti = 0; i<topSequences.length; i++) { 
    System.out.println(topSequences[i]); 
}
```

其输出如下，其中第一个数字代表加权分数，括号内的标签是评分的标签序列:

```
    -0.5563571615737618 [DT, NN, IN, DT, NNP, NNP, VBD, IN, DT, JJ, NN, VBN, IN, DT, JJ, NN]
    -2.9886144610050907 [DT, NN, IN, DT, NNP, NNP, VBD, IN, DT, JJ, NN, VBN, IN, DT, JJ, .]
    -3.771930515521527 [DT, NN, IN, DT, NNP, NNP, VBD, IN, DT, JJ, NN, VBN, IN, DT, NN, NN]
```

Ensure that you include the correct `Sequence` class. For this example, use `import opennlp.tools.util.Sequence;`.

`Sequence`类有几个方法，详见下表:

| **方法** | **意为** |
| `getOutcomes` | 返回表示句子标签的字符串列表 |
| `getProbs` | 返回代表序列中每个标签概率的一组`double`变量 |
| `getScore` | 返回序列的加权值 |

在下面的序列中，我们使用其中的几个方法来演示它们的作用。对于每个序列，将显示标记及其概率，用正斜杠分隔:

```
for (int i = 0; i<topSequences.length; i++) { 
    List<String> outcomes = topSequences[i].getOutcomes(); 
    double probabilities[] = topSequences[i].getProbs(); 
    for (int j = 0; j <outcomes.size(); j++) {  
        System.out.printf("%s/%5.3f ",outcomes.get(j), 
        probabilities[j]); 
    } 
    System.out.println(); 
} 
System.out.println();
```

输出如下。每一对行代表一个序列，其中输出已被包装:

```
    DT/0.992 NN/0.990 IN/0.989 DT/0.990 NNP/0.996 NNP/0.991 VBD/0.994 IN/0.996 DT/0.996 JJ/0.991 NN/0.994 VBN/0.860 IN/0.985 DT/0.960 JJ/0.919 NN/0.832 
    DT/0.992 NN/0.990 IN/0.989 DT/0.990 NNP/0.996 NNP/0.991 VBD/0.994 IN/0.996 DT/0.996 JJ/0.991 NN/0.994 VBN/0.860 IN/0.985 DT/0.960 JJ/0.919 ./0.073 
    DT/0.992 NN/0.990 IN/0.989 DT/0.990 NNP/0.996 NNP/0.991 VBD/0.994 IN/0.996 DT/0.996 JJ/0.991 NN/0.994 VBN/0.860 IN/0.985 DT/0.960 NN/0.073 NN/0.419
```



# 使用 opennlp 重庆

组块的过程包括将一个句子分成几个部分或几个组块。然后可以用标签对这些块进行注释。我们将使用`ChunkerME`类来说明这是如何完成的。这个类使用一个加载到`ChunkerModel`实例中的模型。`ChunkerME`类的`chunk`方法执行实际的分块过程。我们还将研究使用`chunkAsSpans`方法来返回关于这些块的跨度的信息。这让我们可以看到一个块有多长，以及什么元素组成了这个块。

我们将使用`en-pos-maxent.bin`文件为`POSTaggerME`实例创建一个模型。我们需要使用这个实例来标记文本，就像我们在本章前面的*为 POS taggers 使用 OpenNLP POSTaggerME 类*一节中所做的那样。我们还将使用`en-chunker.bin`文件创建一个`ChunkerModel`实例，与`ChunkerME`实例一起使用。

这些模型是使用输入流创建的，如下例所示。我们使用 try-with-resources 块来打开和关闭文件，并处理可能引发的任何异常:

```
try ( 
        InputStream posModelStream = new FileInputStream( 
            getModelDir() + "\\en-pos-maxent.bin"); 
        InputStream chunkerStream = new FileInputStream( 
            getModelDir() + "\\en-chunker.bin");) { 
    ... 
} catch (IOException ex) { 
    // Handle exceptions 
}
```

下面的代码序列创建并使用一个标记来查找句子的位置。然后显示句子及其标签:

```
POSModel model = new POSModel(posModelStream); 
POSTaggerME tagger = new POSTaggerME(model); 

String tags[] = tagger.tag(sentence); 
for(int i=0; i<tags.length; i++) { 
    System.out.print(sentence[i] + "/" + tags[i] + " "); 
} 
System.out.println();
```

输出如下。我们已经展示了该输出，以便清楚了解分块器的工作原理:

```
The/DT voyage/NN of/IN the/DT Abraham/NNP Lincoln/NNP was/VBD for/IN a/DT long/JJ time/NN marked/VBN by/IN no/DT special/JJ incident./NN
```

使用输入流创建一个`ChunkerModel`实例。由此创建了`ChunkerME`实例，然后使用`chunk`方法，如下所示。`chunk`方法将使用句子的标记和它的标签来创建一个字符串数组。每个字符串将保存关于令牌及其块的信息:

```
ChunkerModel chunkerModel = new 
     ChunkerModel(chunkerStream); 
ChunkerME chunkerME = new ChunkerME(chunkerModel); 
String result[] = chunkerME.chunk(sentence, tags);
```

显示了`results`数组中的每个标记及其块标签，如下所示:

```
for (int i = 0; i < result.length; i++) { 
    System.out.println("[" + sentence[i] + "] " + result[i]); 
}
```

输出如下。该标记用括号括起来，后面是 chunk 标记。下表解释了这些标签:

| **第一部分** |
| B | 标签开头 |
| 我 | 标签的延续 |
| E | 标签结束(如果标签只有一个单词，则不会出现) |
| **第二部分** |
| 公证人 | 名词块 |
| 动词 | 动词组块 |

多个单词组合在一起，如“`The voyage`”、`the Abraham Lincoln`:

```
    [The] B-NP
    [voyage] I-NP
    [of] B-PP
    [the] B-NP
    [Abraham] I-NP
    [Lincoln] I-NP
    [was] B-VP
    [for] B-PP
    [a] B-NP
    [long] I-NP
    [time] I-NP
    [marked] B-VP
    [by] B-PP
    [no] B-NP
    [special] I-NP
    [incident.] I-NP
```

如果我们想获得更多关于块的详细信息，我们可以使用`ChunkerME`类的`chunkAsSpans`方法。这个方法返回一个`Span`对象的数组。每个对象代表文本中的一个跨度。

还有几个其他的`ChunkerME`类方法可用。这里，我们将举例说明`getType`、`getStart`和`getEnd`方法的使用。`getType`方法返回块标签的第二部分，而`getStart`和`getEnd`方法分别返回原始`sentence`数组中标记的开始和结束索引。`length`方法返回多个标记的跨度长度。

在下面的序列中，使用`sentence`和`tags`数组执行`chunkAsSpans`方法。然后显示`spans`数组。外部的`for`循环一次处理一个`Span`对象，显示基本的跨度信息。
内部`for`循环显示括号内的跨区文本:

```
Span[] spans = chunkerME.chunkAsSpans(sentence, tags); 
for (Span span : spans) { 
    System.out.print("Type: " + span.getType() + " - "  
        + " Begin: " + span.getStart()  
        + " End:" + span.getEnd() 
        + " Length: " + span.length() + "  ["); 
    for (int j = span.getStart(); j < span.getEnd(); j++) { 
        System.out.print(sentence[j] + " "); 
    } 
    System.out.println("]"); 
}
```

下面的输出清楚地显示了跨度类型、它在`sentence`数组中的位置、它的`Length`，以及实际的跨度文本:

```
    Type: NP -  Begin: 0 End:2 Length: 2  [The voyage ]
    Type: PP -  Begin: 2 End:3 Length: 1  [of ]
    Type: NP -  Begin: 3 End:6 Length: 3  [the Abraham Lincoln ]
    Type: VP -  Begin: 6 End:7 Length: 1  [was ]
    Type: PP -  Begin: 7 End:8 Length: 1  [for ]
    Type: NP -  Begin: 8 End:11 Length: 3  [a long time ]
    Type: VP -  Begin: 11 End:12 Length: 1  [marked ]
    Type: PP -  Begin: 12 End:13 Length: 1  [by ]
    Type: NP -  Begin: 13 End:16 Length: 3  [no special incident. ]
```



# 使用 POSDictionary 类

标签字典指定了单词的有效标签。这可以防止标签不适当地应用于单词。此外，一些搜索算法执行得更快，因为它们不必考虑其他不太可能的标签。

在本节中，我们将演示如何:

*   获取标记者的标记字典
*   确定一个单词有哪些标签
*   展示如何改变一个单词的标签
*   向新的标记器工厂添加新的标记字典

与前面的示例一样，我们将使用 try-with-resources 块打开 POS 模型的输入流，然后创建我们的模型和标记器工厂，如下所示:

```
try (InputStream modelIn = new FileInputStream( 
        new File(getModelDir(), "en-pos-maxent.bin"));) { 
    POSModel model = new POSModel(modelIn); 
    POSTaggerFactory posTaggerFactory = model.getFactory(); 
    ... 
} catch (IOException e) { 
    //Handle exceptions 
}
```



# 获取标记者的标记字典

我们使用了`POSModel`类的`getFactory`方法来获得一个`POSTaggerFactory`实例。我们将使用它的`getTagDictionary`方法来获得它的`TagDictionary`实例。这里举例说明了这一点:

```
MutableTagDictionary tagDictionary =  
  (MutableTagDictionary)posTaggerFactory.getTagDictionary(); 
```

`MutableTagDictionary`接口扩展了`TagDictionary`接口。`TagDictionary`接口拥有一个`getTags`方法，`MutableTagDictionary`接口增加了一个`put`方法，允许将标签添加到字典中。这些接口是由`POSDictionary`类实现的。



# 确定单词的标签

要获得给定单词的标签，使用`getTags`方法。这将返回由字符串表示的标记数组。然后会显示标签，如下所示:

```
String tags[] = tagDictionary.getTags("force"); 
for (String tag : tags) { 
    System.out.print("/" + tag); 
} 
System.out.println(); 
```

输出如下所示:

```
/NN/VBP/VB
```

这意味着“力”这个词可以有三种不同的解释。



# 更改单词的标签

接口的方法允许我们给一个单词添加标签。该方法有两个参数:单词及其新标签。方法返回包含前面标记的数组。

在下面的例子中，我们用新标签替换旧标签。然后显示旧标签:

```
String oldTags[] = tagDictionary.put("force", "newTag"); 
for (String tag : oldTags) { 
    System.out.print("/" + tag); 
} 
System.out.println();
```

以下输出列出了单词的旧标签:

```
/NN/VBP/VB
```

这些标签已被新标签替换，如下所示，其中显示了当前标签:

```
tags = tagDictionary.getTags("force"); 
for (String tag : tags) { 
    System.out.print("/" + tag); 
} 
System.out.println();
```

我们得到的是以下内容:

```
 /newTag
```

为了保留旧标签，我们需要创建一个字符串数组来保存旧标签和新标签，然后使用该数组作为`put`方法的第二个参数，如下所示:

```
String newTags[] = new String[tags.length+1]; 
for (int i=0; i<tags.length; i++) { 
    newTags[i] = tags[i]; 
} 
newTags[tags.length] = "newTag"; 
oldTags = tagDictionary.put("force", newTags);
```

如果我们重新显示当前标签，如此处所示，我们可以看到旧标签被保留，新标签被添加:

```
 /NN/VBP/VB/newTag  
```

When adding tags, be careful to assign the tags in the proper order, as it will influence which tag is assigned.<title>Adding a new tag dictionary</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 添加新的标记字典

一个新的标签字典可以被添加到一个`POSTaggerFactory`实例中。我们将通过创建一个新的`POSTaggerFactory`并添加我们之前开发的`tagDictionary`来说明这个过程。首先，我们使用默认构造函数创建一个新工厂，如下面的代码所示。

接下来对新工厂调用`setTagDictionary`方法:

```
POSTaggerFactory newFactory = new POSTaggerFactory(); 
newFactory.setTagDictionary(tagDictionary); 
```

为了确认已经添加了`tag`字典，我们显示了单词`"force"`的标签，如下所示:

```
tags = newFactory.getTagDictionary().getTags("force"); 
for (String tag : tags) { 
    System.out.print("/" + tag); 
} 
System.out.println(); 
```

标签是相同的，如下所示:

```
 /NN/VBP/VB/newTag
```



# 从文件创建字典

如果我们需要创建一个新的字典，那么一种方法是创建一个包含所有单词及其标签的 XML 文件，然后从该文件创建字典。OpenNLP 通过`POSDictionary`类的`create`方法支持这种方法。

XML 文件由根元素`dictionary`和一系列的元素`entry`组成。`entry`元素使用`tags`属性来指定单词的标签。这个单词作为一个`token`元素包含在`entry`元素中。使用存储在`dictionary.txt`文件中的两个单词的简单示例如下:

```
<dictionary case_sensitive="false"> 
    <entry tags="JJ VB"> 
        <token>strong</token> 
    </entry> 
    <entry tags="NN VBP VB"> 
        <token>force</token> 
    </entry> 
</dictionary>
```

为了创建字典，我们使用基于输入流的`create`方法，如下所示:

```
try (InputStream dictionaryIn =  
      new FileInputStream(new File("dictionary.txt"));) { 
    POSDictionary dictionary = 
     POSDictionary.create(dictionaryIn); 
    ... 
} catch (IOException e) { 
    // Handle exceptions 
}
```

`POSDictionary`类有一个返回迭代器对象的`iterator`方法。它的`next`方法为字典中的每个单词返回一个字符串。我们可以使用这些方法来显示字典的内容，如下所示:

```
Iterator<String> iterator = dictionary.iterator(); 
while (iterator.hasNext()) { 
    String entry = iterator.next(); 
    String tags[] = dictionary.getTags(entry); 
    System.out.print(entry + " "); 
    for (String tag : tags) { 
        System.out.print("/" + tag); 
    } 
    System.out.println(); 
}
```

下面的输出显示了我们可以预期的结果:

```
  strong /JJ/VB
  force /NN/VBP/VB
```



# 使用斯坦福 POS 标签

在这一节中，我们将研究斯坦福 API 支持的两种不同的方法来执行标记。第一种技术使用了`MaxentTagger`类。顾名思义，它使用最大熵来寻找位置。我们还将使用这个类来演示一个用于处理 textese 类型文本的模型。第二种方法将对注释器使用管道方法。英语标签使用宾州树库英语 POS 标签集。



# 使用斯坦福 MaxentTagger

`MaxentTagger`类使用一个模型来执行标记任务。API 中捆绑了许多模型，所有模型都带有文件扩展名`.tagger`。它们包括英语、中文、阿拉伯语、法语和德语模型。
这里列出了英国型号。前缀`wsj`，指的是基于华尔街日报的模型。其他术语指的是用于训练模型的技术。这里不涉及这些概念:

*   `wsj-0-18-bidirectional-distsim.tagger`
*   `wsj-0-18-bidirectional-nodistsim.tagger`
*   `wsj-0-18-caseless-left3words-distsim.tagger`
*   `wsj-0-18-left3words-distsim.tagger`
*   `wsj-0-18-left3words-nodistsim.tagger`
*   `english-bidirectional-distsim.tagger`
*   `english-caseless-left3words-distsim.tagger`
*   `english-left3words-distsim.tagger`

该示例从文件中读入一系列句子。然后处理每个句子，并显示访问和显示单词和标签的各种方式。

我们从 try-with-resources 块开始处理 IO 异常，如下所示。`wsj-0-18-bidirectional-distsim.tagger`文件用于创建`MaxentTagger`类的一个实例。

使用`MaxentTagger`类的`tokenizeText`方法创建`HasWord`对象的`List`实例的`List`实例。这些句子是从`sentences.txt`文件中读入的。`HasWord`接口表示单词并包含两个方法:一个`setWord`和一个`word`方法。后一种方法将单词作为字符串返回。每个句子由一个`HasWord`对象的`List`实例表示:

```
try { 
    MaxentTagger tagger = new MaxentTagger(getModelDir() +  
        "//wsj-0-18-bidirectional-distsim.tagger"); 
    List<List<HasWord>> sentences = MaxentTagger.tokenizeText( 
        new BufferedReader(new FileReader("sentences.txt"))); 
    ... 
} catch (FileNotFoundException ex) { 
    // Handle exceptions 
}
```

`sentences.txt`文件包含本书第五章的前四句话*在一次冒险中**海底两万里*:

```
The voyage of the Abraham Lincoln was for a long time marked by no special incident. 
But one circumstance happened which showed the wonderful dexterity of Ned Land, and proved what confidence we might place in him. 
The 30th of June, the frigate spoke some American whalers, from whom we learned that they knew nothing about the narwhal. 
But one of them, the captain of the Monroe, knowing that Ned Land had shipped on board the Abraham Lincoln, begged for his help in chasing a whale they had in sight.
```

添加一个循环来处理`sentences`列表中的每个句子。`tagSentence`方法返回`TaggedWord`对象的`List`实例，如下面的代码所示。`TaggedWord`类实现了`HasWord`接口，并添加了一个`tag`方法，该方法返回与单词相关联的标签。如此处所示，`toString`方法用于显示每个句子:

```
List<TaggedWord> taggedSentence = 
     tagger.tagSentence(sentence); 
for (List<HasWord> sentence : sentences) { 
    List<TaggedWord> taggedSentence= 
         tagger.tagSentence(sentence); 
    System.out.println(taggedSentence); 
}
```

输出如下所示:

```
    [The/DT, voyage/NN, of/IN, the/DT, Abraham/NNP, Lincoln/NNP, was/VBD, for/IN, a/DT, long/JJ, --- time/NN, marked/VBN, by/IN, no/DT, special/JJ, incident/NN, ./.]
     [But/CC, one/CD, circumstance/NN, happened/VBD, which/WDT, showed/VBD, the/DT, wonderful/JJ, dexterity/NN, of/IN, Ned/NNP, Land/NNP, ,/,, and/CC, proved/VBD, what/WP, confidence/NN, we/PRP, might/MD, place/VB, in/IN, him/PRP, ./.]
    [The/DT, 30th/JJ, of/IN, June/NNP, ,/,, the/DT, frigate/NN, spoke/VBD, some/DT, American/JJ, whalers/NNS, ,/,, from/IN, whom/WP, we/PRP, learned/VBD, that/IN, they/PRP, knew/VBD, nothing/NN, about/IN, the/DT, narwhal/NN, ./.]
    [But/CC, one/CD, of/IN, them/PRP, ,/,, the/DT, captain/NN, of/IN, the/DT, Monroe/NNP, ,/,, knowing/VBG, that/IN, Ned/NNP, Land/NNP, had/VBD, shipped/VBN, on/IN, board/NN, the/DT, Abraham/NNP, Lincoln/NNP, ,/,, begged/VBN, for/IN, his/PRP$, help/NN, in/IN, chasing/VBG, a/DT, whale/NN, they/PRP, had/VBD, in/IN, sight/NN, ./.]
```

或者，我们可以使用`Sentence`类的`listToString`方法将标记的句子转换成一个简单的`String`对象。

`HasWord`的`toString`方法使用第二个参数的值`false`来创建结果字符串，如下所示:

```
List<TaggedWord> taggedSentence = 
     tagger.tagSentence(sentence); 
for (List<HasWord> sentence : sentences) { 
    List<TaggedWord> taggedSentence= 
         tagger.tagSentence(sentence); 
    System.out.println(Sentence.listToString(taggedSentence, false)); 
}
```

这产生了更美观的输出:

```
    The/DT voyage/NN of/IN the/DT Abraham/NNP Lincoln/NNP was/VBD for/IN a/DT long/JJ time/NN marked/VBN by/IN no/DT special/JJ incident/NN ./.
    But/CC one/CD circumstance/NN happened/VBD which/WDT showed/VBD the/DT wonderful/JJ dexterity/NN of/IN Ned/NNP Land/NNP ,/, and/CC proved/VBD what/WP confidence/NN we/PRP might/MD place/VB in/IN him/PRP ./.
    The/DT 30th/JJ of/IN June/NNP ,/, the/DT frigate/NN spoke/VBD some/DT American/JJ whalers/NNS ,/, from/IN whom/WP we/PRP learned/VBD that/IN they/PRP knew/VBD nothing/NN about/IN the/DT narwhal/NN ./.
    But/CC one/CD of/IN them/PRP ,/, the/DT captain/NN of/IN the/DT Monroe/NNP ,/, knowing/VBG that/IN Ned/NNP Land/NNP had/VBD shipped/VBN on/IN board/NN the/DT Abraham/NNP Lincoln/NNP ,/, begged/VBN for/IN his/PRP$ help/NN in/IN chasing/VBG a/DT whale/NN they/PRP had/VBD in/IN sight/NN ./.
```

我们可以使用下面的代码序列来产生相同的结果。`word`和`tag`方法提取单词及其标签:

```
List<TaggedWord> taggedSentence = 
     tagger.tagSentence(sentence); 
for (TaggedWord taggedWord : taggedSentence) { 
    System.out.print(taggedWord.word() + "/" + 
         taggedWord.tag() + " "); 
} 
System.out.println();
```

如果我们只对查找给定标签的特定出现感兴趣，我们可以使用如下序列，它将只列出单数名词(`NN`):

```
List<TaggedWord> taggedSentence = 
     tagger.tagSentence(sentence); 
for (TaggedWord taggedWord : taggedSentence) { 
    if (taggedWord.tag().startsWith("NN")) { 
        System.out.print(taggedWord.word() + " "); 
    } 
} 
System.out.println();
```

每个句子都会显示单数名词，如下所示:

```
    NN Tagged: voyage Abraham Lincoln time incident 
    NN Tagged: circumstance dexterity Ned Land confidence 
    NN Tagged: June frigate whalers nothing narwhal 
    NN Tagged: captain Monroe Ned Land board Abraham Lincoln help whale sight
```



# 使用 MaxentTagger 类标记 textese

我们可以使用不同的模型来处理可能包含 textese 的 Twitter 文本。文本工程通用架构 ( **门**)(【https://gate.ac.uk/wiki/twitter-postagger.html】T4)已经为 Twitter 文本开发了一个模型。此处使用模型来处理 textese:

```
MaxentTagger tagger = new MaxentTagger(getModelDir()  
    + "//gate-EN-twitter.model"); 
```

在这里，我们使用来自*的`MaxentTagger`类的`tagString`方法。*本章前面的小节处理 textese:

```
System.out.println(tagger.tagString("AFAIK she H8 cth!")); System.out.println(tagger.tagString( "BTW had a GR8 tym at the party BBIAM.")); 
```

输出如下所示:

```
    AFAIK_NNP she_PRP H8_VBP cth!_NN 
    BTW_UH had_VBD a_DT GR8_NNP tym_NNP at_IN the_DT party_NN BBIAM._NNP
```



# 使用斯坦福管道执行标记

我们在之前的几个例子中使用了斯坦福管道。在这个例子中，我们将使用斯坦福管道来提取 POS 标签。和我们之前的 Stanford 例子一样，我们基于一组注释器创建了一个管道:`tokenize`、`ssplit`、
和`pos`。

这些将标记化，将文本分割成句子，然后找到位置标记:

```
Properties props = new Properties(); 
props.put("annotators", "tokenize, ssplit, pos"); 
StanfordCoreNLP pipeline = new StanfordCoreNLP(props); 
```

为了处理文本，我们将使用`theSentence`变量作为`Annotator`的输入。然后调用管道的`annotate`方法，如下所示:

```
Annotation document = new Annotation(theSentence); 
pipeline.annotate(document);
```

因为管道可以执行不同类型的处理，所以使用一列`CoreMap`对象来访问单词和标签。`Annotation`类的`get`方法返回句子列表，如下所示:

```
List<CoreMap> sentences = 
     document.get(SentencesAnnotation.class);
```

可以使用`get`方法访问`CoreMap`对象的内容。该方法的参数是所需信息的类。如下面的代码示例所示，使用`TextAnnotation`类访问令牌，使用`PartOfSpeechAnnotation`类检索 POS 标记。将显示每个句子的每个单词及其标签:

```
for (CoreMap sentence : sentences) { 
    for (CoreLabel token : sentence.get(TokensAnnotation.class)) { 
        String word = token.get(TextAnnotation.class); 
        String pos = token.get(PartOfSpeechAnnotation.class); 
        System.out.print(word + "/" + pos + " "); 
    } 
    System.out.println(); 
}
```

输出如下所示:

```
The/DT voyage/NN of/IN the/DT Abraham/NNP Lincoln/NNP was/VBD for/IN a/DT long/JJ time/NN marked/VBN by/IN no/DT special/JJ incident/NN ./.
```

管道可以使用附加选项来控制标记器的工作方式。例如，默认情况下，使用`english-left3words-distsim.tagger` tagger 模型。我们可以使用`pos.model`属性指定一个不同的模型，如下所示。还有一个`pos.maxlen`属性来控制最大句子长度:

```
props.put("pos.model", 
"C:/.../Models/english-caseless-left3words-distsim.tagger"); 
```

有时候，有一个 XML 格式的标记文档是很有用的。`StanfordCoreNLP`类的`xmlPrint`方法会写出这样一个文档。该方法的第一个参数是要显示的注释器。它的第二个参数是要写入的`OutputStream`对象。在以下代码序列中，先前的标记结果被写入标准输出。它包含在一个`try...catch`块中，用于处理 IO 异常:

```
try { 
    pipeline.xmlPrint(document, System.out); 
} catch (IOException ex) { 
    // Handle exceptions 
}
```

部分结果列表如下。仅显示前两个单词和最后一个单词。每个 token 标签都包含单词、其位置及其 POS 标签:

```
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet href="CoreNLP-to-HTML.xsl" type="text/xsl"?>
    <root>
    <document>
    <sentences>
    <sentence id="1">
    <tokens>
    <token id="1">
    <word>The</word>
    <CharacterOffsetBegin>0</CharacterOffsetBegin>
    <CharacterOffsetEnd>3</CharacterOffsetEnd>
    <POS>DT</POS>
    </token>
    <token id="2">
    <word>voyage</word>
    <CharacterOffsetBegin>4</CharacterOffsetBegin>
    <CharacterOffsetEnd>10</CharacterOffsetEnd>
    <POS>NN</POS>
    </token>
             ...
    <token id="17">
    <word>.</word>
    <CharacterOffsetBegin>83</CharacterOffsetBegin>
    <CharacterOffsetEnd>84</CharacterOffsetEnd>
    <POS>.</POS>
    </token>
    </tokens>
    </sentence>
    </sentences>
    </document>
    </root>
```

`prettyPrint`方法以类似的方式工作:

```
pipeline.prettyPrint(document, System.out); 
```

然而，输出并不真的那么漂亮，如此处所示。显示原始句子，后面是每个单词、其位置和标签。输出已被格式化，以便于阅读:

```
    The voyage of the Abraham Lincoln was for a long time marked by no special incident.
    [Text=The CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=DT] 
    [Text=voyage CharacterOffsetBegin=4 CharacterOffsetEnd=10 PartOfSpeech=NN] 
    [Text=of CharacterOffsetBegin=11 CharacterOffsetEnd=13 PartOfSpeech=IN] 
    [Text=the CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=DT] 
    [Text=Abraham CharacterOffsetBegin=18 CharacterOffsetEnd=25 PartOfSpeech=NNP]
     [Text=Lincoln CharacterOffsetBegin=26 CharacterOffsetEnd=33 PartOfSpeech=NNP]
     [Text=was CharacterOffsetBegin=34 CharacterOffsetEnd=37 PartOfSpeech=VBD]
     [Text=for CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=IN]
     [Text=a CharacterOffsetBegin=42 CharacterOffsetEnd=43 PartOfSpeech=DT]
     [Text=long CharacterOffsetBegin=44 CharacterOffsetEnd=48 PartOfSpeech=JJ]
     [Text=time CharacterOffsetBegin=49 CharacterOffsetEnd=53 PartOfSpeech=NN]
     [Text=marked CharacterOffsetBegin=54 CharacterOffsetEnd=60 PartOfSpeech=VBN]
     [Text=by CharacterOffsetBegin=61 CharacterOffsetEnd=63 PartOfSpeech=IN] 
    [Text=no CharacterOffsetBegin=64 CharacterOffsetEnd=66 PartOfSpeech=DT]
     [Text=special CharacterOffsetBegin=67 CharacterOffsetEnd=74 PartOfSpeech=JJ]
     [Text=incident CharacterOffsetBegin=75 CharacterOffsetEnd=83 PartOfSpeech=NN]
     [Text=. CharacterOffsetBegin=83 CharacterOffsetEnd=84 PartOfSpeech=.]
```



# 使用 LingPipe POS 标签

LingPipe 使用`Tagger`接口来支持词性标注。这个接口只有一个方法:`tag`。它返回一个`Tagging`对象的`List`实例。这些对象是单词和它们的标签。该接口由`ChainCrf`和`HmmDecoder`类实现。

`ChainCrf`类使用线性链条件随机场解码和估计来确定标签。`HmmDecoder`类使用 HMM 来执行标记。接下来我们将举例说明这个类。

`HmmDecoder`类使用`tag`方法来确定最可能(最好)的标签。它还有一个`tagNBest`方法，对可能的标签进行评分，并返回这个评分标签的迭代器。灵管有三种 POS 型号，可以从[http://alias-i.com/lingpipe/web/models.html](http://alias-i.com/lingpipe/web/models.html)下载。下表列出了这些选项。在我们的演示中，我们将使用 Brown 语料库模型:

| **型号** | **文件** |
| 英语通用文本:布朗语料库 | `pos-en-general-brown.HiddenMarkovModel` |
| 英语生物医学文本:MedPost 语料库 | `pos-en-bio-medpost.HiddenMarkovModel` |
| 英语生物医学文本:GENIA 语料库 | `pos-en-bio-genia.HiddenMarkovModel` |



# 使用带有 Best_First 标记的 HmmDecoder 类

我们从处理异常的 try-with-resources 块和创建`HmmDecoder`实例的代码开始，如下面的代码所示。

从文件中读取模型，然后用作`HmmDecoder`构造函数的参数:

```
try ( 
        FileInputStream inputStream =  
            new FileInputStream(getModelDir() 
            + "//pos-en-general-brown.HiddenMarkovModel"); 
        ObjectInputStream objectStream = 
            new ObjectInputStream(inputStream);) { 
    HiddenMarkovModel hmm = (HiddenMarkovModel) 
        objectStream.readObject(); 
    HmmDecoder decoder = new HmmDecoder(hmm); 
    ... 
} catch (IOException ex) { 
 // Handle exceptions 
} catch (ClassNotFoundException ex) { 
 // Handle exceptions 
};
```

我们将对`theSentence`变量进行标记。首先，它需要被标记化。我们将使用一个`IndoEuropean`标记器，如下所示。`tokenizer`方法要求将文本字符串转换成字符数组。然后，`tokenize`方法以字符串的形式返回一个令牌数组:

```
TokenizerFactory TOKENIZER_FACTORY =  
    IndoEuropeanTokenizerFactory.INSTANCE; 
char[] charArray = theSentence.toCharArray(); 
Tokenizer tokenizer =  
    TOKENIZER_FACTORY.tokenizer( 
      charArray, 0, charArray.length); 
String[] tokens = tokenizer.tokenize();
```

实际的标记是由`HmmDecoder`类的`tag`方法执行的。然而，这个方法需要一个`String`令牌的`List`实例。这个列表是使用`Arrays`类的`asList`方法创建的。`Tagging`类保存一系列标记和标签:

```
List<String> tokenList = Arrays.asList(tokens); 
Tagging<String> tagString = decoder.tag(tokenList);
```

我们现在准备好显示令牌及其标签。下面的循环使用`token`和`tag`方法分别访问`Tagging`对象中的标记和标签。然后显示它们:

```
for (int i = 0; i < tagString.size(); ++i) { 
    System.out.print(tagString.token(i) + "/"  
    + tagString.tag(i) + " "); 
}
```

输出如下所示:

```
The/at voyage/nn of/in the/at Abraham/np Lincoln/np was/bedz for/in a/at long/jj time/nn marked/vbn by/in no/at special/jj incident/nn ./.
```



# 使用带有 NBest 标记的 HmmDecoder 类

标记过程考虑标记的多种组合。`HmmDecoder`类的`tagNBest`方法返回反映不同订单可信度的`ScoredTagging`对象的迭代器。该方法采用一个令牌列表和一个指定所需最大结果数的数字。

前面的句子不够模糊，不足以说明标签的组合。相反，我们将使用下面的句子:

```
String[] sentence = {"Bill", "used", "the", "force", 
     "to", "force", "the", "manager", "to",  
    "tear", "the", "bill","in", "to."}; 
List<String> tokenList = Arrays.asList(sentence); 
```

这里显示了使用此方法的一个示例，从结果数的声明开始:

```
int maxResults = 5;
```

使用上一节中创建的`decoder`对象，我们将`tagNBest`方法应用于它，如下所示:

```
Iterator<ScoredTagging<String>> iterator =  
    decoder.tagNBest(tokenList, maxResults); 
```

迭代器将允许我们访问五个不同的分数。`ScoredTagging`类拥有一个`score`方法，该方法返回一个反映它认为自己执行得有多好的值。在下面的代码序列中，一个`printf`语句显示了这个分数。接下来是一个循环，其中显示了令牌及其标签。

结果是一个分数，后跟带有标签的单词序列:

```
while (iterator.hasNext()) { 
    ScoredTagging<String> scoredTagging = iterator.next(); 
    System.out.printf("Score: %7.3f   Sequence: ", 
        scoredTagging.score()); 
    for (int i = 0; i < tokenList.size(); ++i) { 
        System.out.print(scoredTagging.token(i) + "/"  
            + scoredTagging.tag(i) + " "); 
    } 
    System.out.println(); 
}
```

输出如下。注意，单词`"force"`可以有一个标签`nn`、`jj`或`vb`:

```
    Score: -148.796   Sequence: Bill/np used/vbd the/at force/nn to/to force/vb the/at manager/nn to/to tear/vb the/at bill/nn in/in two./nn 
    Score: -154.434   Sequence: Bill/np used/vbn the/at force/nn to/to force/vb the/at manager/nn to/to tear/vb the/at bill/nn in/in two./nn 
    Score: -154.781   Sequence: Bill/np used/vbd the/at force/nn to/in force/nn the/at manager/nn to/to tear/vb the/at bill/nn in/in two./nn 
    Score: -157.126   Sequence: Bill/np used/vbd the/at force/nn to/to force/vb the/at manager/jj to/to tear/vb the/at bill/nn in/in two./nn 
    Score: -157.340   Sequence: Bill/np used/vbd the/at force/jj to/to force/vb the/at manager/nn to/to tear/vb the/at bill/nn in/in two./nn  
```



# 使用 HmmDecoder 类确定标签可信度

可以使用网格结构来执行统计分析，这对于分析可选的单词排序是有用的。这个结构表示向前/向后得分。`HmmDecoder`类的`tagMarginal`方法返回`TagLattice`类的一个实例，它代表一个网格。

我们可以使用`ConditionalClassification`类的一个实例来检查网格的每个令牌。在下面的例子中，`tagMarginal`方法返回一个`TagLattice`实例。使用一个循环来获取网格中每个标记的`ConditionalClassification`实例。

我们使用的是在上一节中开发的同一个`tokenList`实例:

```
TagLattice<String> lattice = decoder.tagMarginal(tokenList); 
for (int index = 0; index < tokenList.size(); index++) { 
    ConditionalClassification classification =  
        lattice.tokenClassification(index); 
    ... 
}
```

`ConditionalClassification`类有一个`score`和一个`category`方法。`score`方法返回给定类别的相对分数。`category`方法返回这个类别，也就是标签。令牌、其分数及其类别如下所示:

```
System.out.printf("%-8s",tokenList.get(index)); 
for (int i = 0; i < 4; ++i) { 
    double score = classification.score(i); 
    String tag = classification.category(i); 
    System.out.printf("%7.3f/%-3s ",score,tag); 
} 
System.out.println(); 
```

输出如下所示:

```
    Bill      0.974/np    0.018/nn    0.006/rb    0.001/nps 
    used      0.935/vbd   0.065/vbn   0.000/jj    0.000/rb  
    the       1.000/at    0.000/jj    0.000/pps   0.000/pp$$ 
    force     0.977/nn    0.016/jj    0.006/vb    0.001/rb  
    to        0.944/to    0.055/in    0.000/rb    0.000/nn  
    force     0.945/vb    0.053/nn    0.002/rb    0.001/jj  
    the       1.000/at    0.000/jj    0.000/vb    0.000/nn  
    manager   0.982/nn    0.018/jj    0.000/nn$   0.000/vb  
    to        0.988/to    0.012/in    0.000/rb    0.000/nn  
    tear      0.991/vb    0.007/nn    0.001/rb    0.001/jj  
    the       1.000/at    0.000/jj    0.000/vb    0.000/nn  
    bill      0.994/nn    0.003/jj    0.002/rb    0.001/nns 
    in        0.990/in    0.004/rp    0.002/nn    0.001/jj  
    two.      0.960/nn    0.013/np    0.011/nns   0.008/rb
```



# 为 OpenNLP POSModel 定型

训练一个 OpenNLP `POSModel`类似于前面的训练例子。需要一个训练文件，它应该足够大以提供一个好的样本集。培训文件的每句话必须单独在一行上。每一行都由一个标记、下划线字符和标签组成。

下面的训练数据是使用第五章的前五句话创建的，第五章是《海底两万里*中的*在一家企业*。虽然这不是一个大的样本集，但它很容易创建，并且足以用于演示目的。它保存在一个名为`sample.train`的文件中:*

```
    The_DT voyage_NN of_IN the_DT Abraham_NNP Lincoln_NNP was_VBD for_IN a_DT long_JJ time_NN marked_VBN by_IN no_DT special_JJ incident._NN
    But_CC one_CD circumstance_NN happened_VBD which_WDT showed_VBD the_DT wonderful_JJ dexterity_NN of_IN Ned_NNP Land,_NNP and_CC proved_VBD what_WP confidence_NN we_PRP might_MD place_VB in_IN him._PRP$ 
    The_DT 30th_JJ of_IN June,_NNP the_DT frigate_NN spoke_VBD some_DT American_NNP whalers,_, from_IN whom_WP we_PRP learned_VBD that_IN they_PRP knew_VBD nothing_NN about_IN the_DT narwhal._NN 
    But_CC one_CD of_IN them,_PRP$ the_DT captain_NN of_IN the_DT Monroe,_NNP knowing_VBG that_IN Ned_NNP Land_NNP had_VBD shipped_VBN on_IN board_NN the_DT Abraham_NNP Lincoln,_NNP begged_VBD for_IN his_PRP$ help_NN in_IN chasing_VBG a_DT whale_NN they_PRP had_VBD in_IN sight._NN
```

我们将演示使用`POSModel`类的`train`方法创建模型，以及如何将模型保存到文件中。我们从声明`POSModel`实例变量开始:

```
POSModel model = null;
```

try-with-resources 块打开示例文件:

```
try (InputStream dataIn = new FileInputStream("sample.train");) { 
    ... 
} catch (IOException e) { 
    // Handle exceptions 
}
```

创建一个`PlainTextByLineStream`类的实例，并与`WordTagSampleStream`类一起创建一个`ObjectStream<POSSample>`实例。这将样本数据转换成`train`方法要求的格式:

```
ObjectStream<String> lineStream =  
    new PlainTextByLineStream(dataIn, "UTF-8"); 
ObjectStream<POSSample> sampleStream =  
    new WordTagSampleStream(lineStream); 
```

`train`方法使用它的参数来指定语言、样本流、训练参数和任何需要的字典(本例中没有)，如下所示:

```
model = POSTaggerME.train("en", sampleStream, 
    TrainingParameters.defaultParams(), null, null); 
```

这个过程的输出是冗长的。以下输出已被缩短以节省空间:

```
    Indexing events using cutoff of 5

      Computing event counts...  done. 90 events
      Indexing...  done.
    Sorting and merging events... done. Reduced 90 events to 82.
    Done indexing.
    Incorporating indexed data for training...  
    done.
      Number of Event Tokens: 82
          Number of Outcomes: 17
        Number of Predicates: 45
    ...done.
    Computing model parameters ...
    Performing 100 iterations.
      1:  ... loglikelihood=-254.98920096505964  0.14444444444444443
      2:  ... loglikelihood=-201.19283975630537  0.6
      3:  ... loglikelihood=-174.8849213436524  0.6111111111111112
      4:  ... loglikelihood=-157.58164262220754  0.6333333333333333
      5:  ... loglikelihood=-144.69272379986646  0.6555555555555556
    ...
     99:  ... loglikelihood=-33.461128002846024  0.9333333333333333
    100:  ... loglikelihood=-33.29073273669207  0.9333333333333333
```

为了将模型保存到文件中，我们使用了下面的代码。输出流被创建，`POSModel`类的`serialize`方法将模型保存到`en_pos_verne.bin`文件中:

```
try (OutputStream modelOut = new BufferedOutputStream( 
        new FileOutputStream(new File("en_pos_verne.bin")));) { 
    model.serialize(modelOut); 
} catch (IOException e) { 
    // Handle exceptions 
}
```



# 摘要

词性标注是一种识别句子语法部分的强大技术。它为下游任务提供了有用的处理，如问题分析和分析文本的情感。当我们在第 7 章、*信息检索*中讨论解析时，我们将回到这个主题。

由于大多数语言中存在歧义，标记不是一个简单的过程。越来越多的使用 textese 只会让这个过程变得更加困难。幸运的是，有一些模型可以很好地识别这种类型的文本。然而，随着新术语和俚语的引入，这些模型需要不断更新。

我们研究了 OpenNLP、斯坦福 API 和 LingPipe 在支持标记方面的使用。这些库使用几种不同的方法来标记单词，包括基于规则和基于模型的方法。我们看到了如何使用字典来增强标记过程。

我们简要介绍了模型训练过程。预先标记的样本文本被用作该过程的输入，模型作为输出出现。虽然我们没有解决模型的验证问题，但是这可以通过与我们在前面章节中所完成的相似的方式来完成。

各种 POS 标记方法可以根据许多因素进行比较，例如它们的准确性和运行速度。虽然我们没有在这里讨论这些问题，但是有许多可用的网络资源。一个检验他们跑得多快的对比可以在 http://mattwilkens . com/2008/11/08/evaluating-pos-taggers-speed/找到。

在下一章中，[第 6 章](part0155.html#4JQ760-447d219d688d46cb9ed55b88cf17edcf)，*表示具有特征的文本*，我们将研究基于内容对文档进行分类的技术。